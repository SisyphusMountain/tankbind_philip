{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the original TankBind model, and try to find the differences in outputs between the old and the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/fs/gpfs41/lv11/fileset01/pool/pool-marsot'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/fs/gpfs41/lv11/fileset01/pool/pool-marsot/tankbind_philip/TankBind/tankbind\n"
     ]
    }
   ],
   "source": [
    "%cd /fs/gpfs41/lv11/fileset01/pool/pool-marsot/tankbind_philip/TankBind/tankbind/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"/fs/pool/pool-marsot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tankbind_philip.TankBind.tankbind as tankbind_og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tankbind_philip.TankBind.tankbind.model import IaBNet_with_affinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_og = IaBNet_with_affinity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Protein embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace model weights with original model weights\n",
    "for (name1, param1), (name2, param2) in zip(model.protein_embedding.named_parameters(), model_og.conv_protein.named_parameters()):\n",
    "    param1.data = param2.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compound embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GINE(\n",
       "  (gnn_node): GNN_node(\n",
       "    (atom_encoder): AtomEncoder(\n",
       "      (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (convs): ModuleList(\n",
       "      (0-4): 5 x GINConv()\n",
       "    )\n",
       "    (batch_norms): ModuleList(\n",
       "      (0-4): 5 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compound_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteratively modifying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch import nn\n",
    "from torch.nn import Linear\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "from gvp import GVP, GVPConvLayer, LayerNorm, tuple_index\n",
    "from torch.distributions import Categorical\n",
    "from torch_scatter import scatter_mean\n",
    "#from GATv2 import GAT\n",
    "from GINv2 import GIN\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.conv2 = SAGEConv((-1, -1), out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class GVP_embedding(nn.Module):\n",
    "    '''\n",
    "    Modified based on https://github.com/drorlab/gvp-pytorch/blob/main/gvp/models.py\n",
    "    GVP-GNN for Model Quality Assessment as described in manuscript.\n",
    "    \n",
    "    Takes in protein structure graphs of type `torch_geometric.data.Data` \n",
    "    or `torch_geometric.data.Batch` and returns a scalar score for\n",
    "    each graph in the batch in a `torch.Tensor` of shape [n_nodes]\n",
    "    \n",
    "    Should be used with `gvp.data.ProteinGraphDataset`, or with generators\n",
    "    of `torch_geometric.data.Batch` objects with the same attributes.\n",
    "    \n",
    "    :param node_in_dim: node dimensions in input graph, should be\n",
    "                        (6, 3) if using original features\n",
    "    :param node_h_dim: node dimensions to use in GVP-GNN layers\n",
    "    :param node_in_dim: edge dimensions in input graph, should be\n",
    "                        (32, 1) if using original features\n",
    "    :param edge_h_dim: edge dimensions to embed to before use\n",
    "                       in GVP-GNN layers\n",
    "    :seq_in: if `True`, sequences will also be passed in with\n",
    "             the forward pass; otherwise, sequence information\n",
    "             is assumed to be part of input node embeddings\n",
    "    :param num_layers: number of GVP-GNN layers\n",
    "    :param drop_rate: rate to use in all dropout layers\n",
    "    '''\n",
    "    def __init__(self, node_in_dim, node_h_dim, \n",
    "                 edge_in_dim, edge_h_dim,\n",
    "                 seq_in=False, num_layers=3, drop_rate=0.1):\n",
    "\n",
    "        super(GVP_embedding, self).__init__()\n",
    "        \n",
    "        if seq_in:\n",
    "            self.W_s = nn.Embedding(20, 20)\n",
    "            node_in_dim = (node_in_dim[0] + 20, node_in_dim[1])\n",
    "        \n",
    "        self.W_v = nn.Sequential(\n",
    "            LayerNorm(node_in_dim),\n",
    "            GVP(node_in_dim, node_h_dim, activations=(None, None))\n",
    "        )\n",
    "        self.W_e = nn.Sequential(\n",
    "            LayerNorm(edge_in_dim),\n",
    "            GVP(edge_in_dim, edge_h_dim, activations=(None, None))\n",
    "        )\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "                GVPConvLayer(node_h_dim, edge_h_dim, drop_rate=drop_rate) \n",
    "            for _ in range(num_layers))\n",
    "        \n",
    "        ns, _ = node_h_dim\n",
    "        self.W_out = nn.Sequential(\n",
    "            LayerNorm(node_h_dim),\n",
    "            GVP(node_h_dim, (ns, 0)))\n",
    "\n",
    "    def forward(self, h_V, edge_index, h_E, seq):      \n",
    "        '''\n",
    "        :param h_V: tuple (s, V) of node embeddings\n",
    "        :param edge_index: `torch.Tensor` of shape [2, num_edges]\n",
    "        :param h_E: tuple (s, V) of edge embeddings\n",
    "        :param seq: if not `None`, int `torch.Tensor` of shape [num_nodes]\n",
    "                    to be embedded and appended to `h_V`\n",
    "        '''\n",
    "        seq = self.W_s(seq)\n",
    "        h_V = (torch.cat([h_V[0], seq], dim=-1), h_V[1])\n",
    "        h_V = self.W_v(h_V)\n",
    "        h_E = self.W_e(h_E)\n",
    "        for layer in self.layers:\n",
    "            h_V = layer(h_V, edge_index, h_E)\n",
    "        out = self.W_out(h_V)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def get_pair_dis_one_hot(d, bin_size=2, bin_min=-1, bin_max=30):\n",
    "    # without compute_mode='donot_use_mm_for_euclid_dist' could lead to wrong result.\n",
    "    pair_dis = torch.cdist(d, d, compute_mode='donot_use_mm_for_euclid_dist')\n",
    "    pair_dis[pair_dis>bin_max] = bin_max\n",
    "    pair_dis_bin_index = torch.div(pair_dis - bin_min, bin_size, rounding_mode='floor').long()\n",
    "    pair_dis_one_hot = torch.nn.functional.one_hot(pair_dis_bin_index, num_classes=16)\n",
    "    return pair_dis_one_hot\n",
    "\n",
    "class TriangleProteinToCompound(torch.nn.Module):\n",
    "    def __init__(self, embedding_channels=256, c=128, hasgate=True):\n",
    "        super().__init__()\n",
    "        self.layernorm = torch.nn.LayerNorm(embedding_channels)\n",
    "        self.layernorm_c = torch.nn.LayerNorm(c)\n",
    "        self.hasgate = hasgate\n",
    "        if hasgate:\n",
    "            self.gate_linear = Linear(embedding_channels, c)\n",
    "        self.linear = Linear(embedding_channels, c)\n",
    "        self.ending_gate_linear = Linear(embedding_channels, embedding_channels)\n",
    "        self.linear_after_sum = Linear(c, embedding_channels)\n",
    "    def forward(self, z, protein_pair, compound_pair, z_mask):\n",
    "        # z of shape b, i, j, embedding_channels, where i is protein dim, j is compound dim.\n",
    "        # z_mask of shape b, i, j, 1\n",
    "        z = self.layernorm(z)\n",
    "        if self.hasgate:\n",
    "            ab = self.gate_linear(z).sigmoid() * self.linear(z) * z_mask\n",
    "        else:\n",
    "            ab = self.linear(z) * z_mask\n",
    "        g = self.ending_gate_linear(z).sigmoid()\n",
    "        block1 = torch.einsum(\"bikc,bkjc->bijc\", protein_pair, ab)\n",
    "        block2 = torch.einsum(\"bikc,bjkc->bijc\", ab, compound_pair)\n",
    "        z = g * self.linear_after_sum(self.layernorm_c(block1+block2)) * z_mask\n",
    "        return z\n",
    "\n",
    "class TriangleProteinToCompound_v2(torch.nn.Module):\n",
    "    # separate left/right edges (block1/block2).\n",
    "    def __init__(self, embedding_channels=256, c=128):\n",
    "        super().__init__()\n",
    "        self.layernorm = torch.nn.LayerNorm(embedding_channels)\n",
    "        self.layernorm_c = torch.nn.LayerNorm(c)\n",
    "\n",
    "        self.gate_linear1 = Linear(embedding_channels, c)\n",
    "        self.gate_linear2 = Linear(embedding_channels, c)\n",
    "\n",
    "        self.linear1 = Linear(embedding_channels, c)\n",
    "        self.linear2 = Linear(embedding_channels, c)\n",
    "\n",
    "        self.ending_gate_linear = Linear(embedding_channels, embedding_channels)\n",
    "        self.linear_after_sum = Linear(c, embedding_channels)\n",
    "    def forward(self, z, protein_pair, compound_pair, z_mask):\n",
    "        # z of shape b, i, j, embedding_channels, where i is protein dim, j is compound dim.\n",
    "        z = self.layernorm(z)\n",
    "        protein_pair = self.layernorm(protein_pair)\n",
    "        compound_pair = self.layernorm(compound_pair)\n",
    " \n",
    "        ab1 = self.gate_linear1(z).sigmoid() * self.linear1(z) * z_mask\n",
    "        ab2 = self.gate_linear2(z).sigmoid() * self.linear2(z) * z_mask\n",
    "        protein_pair = self.gate_linear2(protein_pair).sigmoid() * self.linear2(protein_pair)\n",
    "        compound_pair = self.gate_linear1(compound_pair).sigmoid() * self.linear1(compound_pair)\n",
    "\n",
    "        g = self.ending_gate_linear(z).sigmoid()\n",
    "        block1 = torch.einsum(\"bikc,bkjc->bijc\", protein_pair, ab1)\n",
    "        block2 = torch.einsum(\"bikc,bjkc->bijc\", ab2, compound_pair)\n",
    "        # print(g.shape, block1.shape, block2.shape)\n",
    "        z = g * self.linear_after_sum(self.layernorm_c(block1+block2)) * z_mask\n",
    "        return z\n",
    "\n",
    "class Self_Attention(nn.Module):\n",
    "    def __init__(self, hidden_size,num_attention_heads=8,drop_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.attention_head_size = int(hidden_size / num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        self.dp = nn.Dropout(drop_rate)\n",
    "        self.ln = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self,q,k,v,attention_mask=None,attention_weight=None):\n",
    "        q = self.transpose_for_scores(q)\n",
    "        k = self.transpose_for_scores(k)\n",
    "        v = self.transpose_for_scores(v)\n",
    "        attention_scores = torch.matmul(q, k.transpose(-1, -2))\n",
    "\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "        # attention_probs = self.dp(attention_probs)\n",
    "        if attention_weight is not None:\n",
    "            attention_weight_sorted_sorted = torch.argsort(torch.argsort(-attention_weight,axis=-1),axis=-1)\n",
    "            # if self.training:\n",
    "            #     top_mask = (attention_weight_sorted_sorted<np.random.randint(28,45))\n",
    "            # else:\n",
    "            top_mask = (attention_weight_sorted_sorted<32)\n",
    "            attention_probs = attention_probs * top_mask\n",
    "            # attention_probs = attention_probs * attention_weight\n",
    "            attention_probs = attention_probs / (torch.sum(attention_probs,dim=-1,keepdim=True) + 1e-5)\n",
    "        # print(attention_probs.shape,v.shape)\n",
    "        # attention_probs = self.dp(attention_probs)\n",
    "        outputs = torch.matmul(attention_probs, v)\n",
    "\n",
    "        outputs = outputs.permute(0, 2, 1, 3).contiguous()\n",
    "        new_output_shape = outputs.size()[:-2] + (self.all_head_size,)\n",
    "        outputs = outputs.view(*new_output_shape)\n",
    "        outputs = self.ln(outputs)\n",
    "        return outputs\n",
    "\n",
    "class TriangleSelfAttentionRowWise(torch.nn.Module):\n",
    "    # use the protein-compound matrix only.\n",
    "    def __init__(self, embedding_channels=128, c=32, num_attention_heads=4):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.attention_head_size = c\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        # self.dp = nn.Dropout(drop_rate)\n",
    "        # self.ln = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        self.layernorm = torch.nn.LayerNorm(embedding_channels)\n",
    "        # self.layernorm_c = torch.nn.LayerNorm(c)\n",
    "\n",
    "        self.linear_q = Linear(embedding_channels, self.all_head_size, bias=False)\n",
    "        self.linear_k = Linear(embedding_channels, self.all_head_size, bias=False)\n",
    "        self.linear_v = Linear(embedding_channels, self.all_head_size, bias=False)\n",
    "        # self.b = Linear(embedding_channels, h, bias=False)\n",
    "        self.g = Linear(embedding_channels, self.all_head_size)\n",
    "        self.final_linear = Linear(self.all_head_size, embedding_channels)\n",
    "\n",
    "    def reshape_last_dim(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x\n",
    "\n",
    "    def forward(self, z, z_mask):\n",
    "        # z of shape b, i, j, embedding_channels, where i is protein dim, j is compound dim.\n",
    "        # z_mask of shape b, i, j\n",
    "        z = self.layernorm(z)\n",
    "        p_length = z.shape[1]\n",
    "        batch_n = z.shape[0]\n",
    "        # new_z = torch.zeros(z.shape, device=z.device)\n",
    "        z_i = z\n",
    "        z_mask_i = z_mask.view((batch_n, p_length, 1, 1, -1))\n",
    "        attention_mask_i = (1e9 * (z_mask_i.float() - 1.))\n",
    "        # q, k, v of shape b, j, h, c\n",
    "        q = self.reshape_last_dim(self.linear_q(z_i)) #  * (self.attention_head_size**(-0.5))\n",
    "        k = self.reshape_last_dim(self.linear_k(z_i))\n",
    "        v = self.reshape_last_dim(self.linear_v(z_i))\n",
    "        logits = torch.einsum('biqhc,bikhc->bihqk', q, k) + attention_mask_i\n",
    "        weights = nn.Softmax(dim=-1)(logits)\n",
    "        # weights of shape b, h, j, j\n",
    "        # attention_probs = self.dp(attention_probs)\n",
    "        weighted_avg = torch.einsum('bihqk,bikhc->biqhc', weights, v)\n",
    "        g = self.reshape_last_dim(self.g(z_i)).sigmoid()\n",
    "        output = g * weighted_avg\n",
    "        new_output_shape = output.size()[:-2] + (self.all_head_size,)\n",
    "        output = output.view(*new_output_shape)\n",
    "        # output of shape b, j, embedding.\n",
    "        # z[:, i] = output\n",
    "        z = output\n",
    "        # print(g.shape, block1.shape, block2.shape)\n",
    "        z = self.final_linear(z) * z_mask.unsqueeze(-1)\n",
    "        return z\n",
    "\n",
    "\n",
    "class Transition(torch.nn.Module):\n",
    "    # separate left/right edges (block1/block2).\n",
    "    def __init__(self, embedding_channels=256, n=4):\n",
    "        super().__init__()\n",
    "        self.layernorm = torch.nn.LayerNorm(embedding_channels)\n",
    "        self.linear1 = Linear(embedding_channels, n*embedding_channels)\n",
    "        self.linear2 = Linear(n*embedding_channels, embedding_channels)\n",
    "    def forward(self, z):\n",
    "        # z of shape b, i, j, embedding_channels, where i is protein dim, j is compound dim.\n",
    "        z = self.layernorm(z)\n",
    "        z = self.linear2((self.linear1(z)).relu())\n",
    "        return z\n",
    "\n",
    "\n",
    "\n",
    "class IaBNet_with_affinity_attention(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels=128, embedding_channels=128, c=128, mode=0, protein_embed_mode=1, compound_embed_mode=1, n_trigonometry_module_stack=5, protein_bin_max=30, readout_mode=2):\n",
    "        super().__init__()\n",
    "        self.layernorm = torch.nn.LayerNorm(embedding_channels)\n",
    "        self.protein_bin_max = protein_bin_max\n",
    "        self.mode = mode\n",
    "        self.protein_embed_mode = protein_embed_mode\n",
    "        self.compound_embed_mode = compound_embed_mode\n",
    "        self.n_trigonometry_module_stack = n_trigonometry_module_stack\n",
    "        self.readout_mode = readout_mode\n",
    "\n",
    "        if protein_embed_mode == 0:\n",
    "            self.conv_protein = GNN(hidden_channels, embedding_channels)\n",
    "            self.conv_compound = GNN(hidden_channels, embedding_channels)\n",
    "        if protein_embed_mode == 1:\n",
    "            self.conv_protein = GVP_embedding((6, 3), (embedding_channels, 16), \n",
    "                                              (32, 1), (32, 1), seq_in=True)\n",
    "            \n",
    "\n",
    "        if compound_embed_mode == 0:\n",
    "            self.conv_compound = GNN(hidden_channels, embedding_channels)\n",
    "        elif compound_embed_mode == 1:\n",
    "            self.conv_compound = GIN(input_dim = 56, hidden_dims = [128,56,embedding_channels], edge_input_dim = 19, concat_hidden = False)\n",
    "\n",
    "        if mode == 0:\n",
    "            self.protein_pair_embedding = Linear(16, c)\n",
    "            self.compound_pair_embedding = Linear(16, c)\n",
    "            self.protein_to_compound_list = []\n",
    "            self.protein_to_compound_list = nn.ModuleList([TriangleProteinToCompound_v2(embedding_channels=embedding_channels, c=c) for _ in range(n_trigonometry_module_stack)])\n",
    "            self.triangle_self_attention_list = nn.ModuleList([TriangleSelfAttentionRowWise(embedding_channels=embedding_channels) for _ in range(n_trigonometry_module_stack)])\n",
    "            self.tranistion = Transition(embedding_channels=embedding_channels, n=4)\n",
    "\n",
    "        self.linear = Linear(embedding_channels, 1)\n",
    "        self.linear_energy = Linear(embedding_channels, 1)\n",
    "        if readout_mode == 2:\n",
    "            self.gate_linear = Linear(embedding_channels, 1)\n",
    "        # self.gate_linear = Linear(embedding_channels, 1)\n",
    "        self.bias = torch.nn.Parameter(torch.ones(1))\n",
    "        self.leaky = torch.nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout2d(p=0.25)\n",
    "    def forward(self, data):\n",
    "        if self.protein_embed_mode == 0:\n",
    "            x = data['protein'].x.float()\n",
    "            edge_index = data[(\"protein\", \"p2p\", \"protein\")].edge_index\n",
    "            protein_batch = data['protein'].batch\n",
    "            protein_out = self.conv_protein(x, edge_index)\n",
    "        if self.protein_embed_mode == 1:\n",
    "            nodes = (data['protein']['node_s'], data['protein']['node_v'])\n",
    "            edges = (data[(\"protein\", \"p2p\", \"protein\")][\"edge_s\"], data[(\"protein\", \"p2p\", \"protein\")][\"edge_v\"])\n",
    "            protein_batch = data['protein'].batch\n",
    "            protein_out = self.conv_protein(nodes, data[(\"protein\", \"p2p\", \"protein\")][\"edge_index\"], edges, data.seq)\n",
    "\n",
    "        if self.compound_embed_mode == 0:\n",
    "            compound_x = data['compound'].x.float()\n",
    "            compound_edge_index = data[(\"compound\", \"c2c\", \"compound\")].edge_index\n",
    "            compound_batch = data['compound'].batch\n",
    "            compound_out = self.conv_compound(compound_x, compound_edge_index)\n",
    "        elif self.compound_embed_mode == 1:\n",
    "            compound_x = data['compound'].x.float()\n",
    "            compound_edge_index = data[(\"compound\", \"c2c\", \"compound\")].edge_index.T\n",
    "            # compound_edge_index = data[(\"compound\", \"c2c\", \"compound\")].edge_index\n",
    "            compound_edge_feature = data[(\"compound\", \"c2c\", \"compound\")].edge_attr\n",
    "            edge_weight = data[(\"compound\", \"c2c\", \"compound\")].edge_weight\n",
    "            compound_batch = data['compound'].batch\n",
    "            # Enzo : print dimensions\n",
    "            #print(f\"{compound_edge_index.shape=}, {edge_weight.shape=}, {compound_edge_feature.shape=}, {compound_x.shape=}\")\n",
    "            compound_out = self.conv_compound(compound_edge_index,edge_weight,compound_edge_feature,compound_x.shape[0],compound_x)['node_feature']\n",
    "    \n",
    "        # protein_batch version could further process b matrix. better than for loop.\n",
    "        # protein_out_batched of shape b, n, c\n",
    "        protein_out_batched, protein_out_mask = to_dense_batch(protein_out, protein_batch)\n",
    "        compound_out_batched, compound_out_mask = to_dense_batch(compound_out, compound_batch)\n",
    "\n",
    "        node_xyz = data.node_xyz\n",
    "\n",
    "        p_coords_batched, p_coords_mask = to_dense_batch(node_xyz, protein_batch)\n",
    "        # c_coords_batched, c_coords_mask = to_dense_batch(coords, compound_batch)\n",
    "\n",
    "        protein_pair = get_pair_dis_one_hot(p_coords_batched, bin_size=2, bin_min=-1, bin_max=self.protein_bin_max)\n",
    "        # compound_pair = get_pair_dis_one_hot(c_coords_batched, bin_size=1, bin_min=-0.5, bin_max=15)\n",
    "        compound_pair_batched, compound_pair_batched_mask = to_dense_batch(data.compound_pair, data.compound_pair_batch)\n",
    "        batch_n = compound_pair_batched.shape[0]\n",
    "        max_compound_size_square = compound_pair_batched.shape[1]\n",
    "        max_compound_size = int(max_compound_size_square**0.5)\n",
    "        assert (max_compound_size**2 - max_compound_size_square)**2 < 1e-4\n",
    "        compound_pair = torch.zeros((batch_n, max_compound_size, max_compound_size, 16)).to(data.compound_pair.device)\n",
    "        for i in range(batch_n):\n",
    "            one = compound_pair_batched[i]\n",
    "            compound_size_square = (data.compound_pair_batch == i).sum()\n",
    "            compound_size = int(compound_size_square**0.5)\n",
    "            compound_pair[i,:compound_size, :compound_size] = one[:compound_size_square].reshape(\n",
    "                                                                (compound_size, compound_size, -1))\n",
    "        protein_pair = self.protein_pair_embedding(protein_pair.float())\n",
    "        compound_pair = self.compound_pair_embedding(compound_pair.float())\n",
    "        # b = torch.einsum(\"bik,bjk->bij\", protein_out_batched, compound_out_batched).flatten()\n",
    "\n",
    "        protein_out_batched = self.layernorm(protein_out_batched)\n",
    "        compound_out_batched = self.layernorm(compound_out_batched)\n",
    "        # z of shape, b, protein_length, compound_length, channels.\n",
    "        z = torch.einsum(\"bik,bjk->bijk\", protein_out_batched, compound_out_batched)\n",
    "        z_mask = torch.einsum(\"bi,bj->bij\", protein_out_mask, compound_out_mask)\n",
    "        # z = z * z_mask.unsqueeze(-1)\n",
    "        # print(protein_pair.shape, compound_pair.shape, b.shape)\n",
    "        if self.mode == 0:\n",
    "            for _ in range(1):\n",
    "                for i_module in range(self.n_trigonometry_module_stack):\n",
    "                    z = z + self.dropout(self.protein_to_compound_list[i_module](z, protein_pair, compound_pair, z_mask.unsqueeze(-1)))\n",
    "                    z = z + self.dropout(self.triangle_self_attention_list[i_module](z, z_mask))\n",
    "                    z = self.tranistion(z)\n",
    "        # batch_dim = z.shape[0]\n",
    "\n",
    "        b = self.linear(z).squeeze(-1)\n",
    "        y_pred = b[z_mask]\n",
    "        y_pred = y_pred.sigmoid() * 10   # normalize to 0 to 10.\n",
    "        if self.readout_mode == 0:\n",
    "            pair_energy = self.linear_energy(z).squeeze(-1) * z_mask\n",
    "            affinity_pred = self.leaky(self.bias + ((pair_energy).sum(axis=(-1, -2))))\n",
    "        if self.readout_mode == 1:\n",
    "            # valid_interaction_z = (z * z_mask.unsqueeze(-1)).mean(axis=(1, 2))\n",
    "            valid_interaction_z = (z * z_mask.unsqueeze(-1)).sum(axis=(1, 2)) / z_mask.sum(axis=(1, 2)).unsqueeze(-1)\n",
    "            affinity_pred = self.linear_energy(valid_interaction_z).squeeze(-1)\n",
    "            # print(\"z shape\", z.shape, \"z_mask shape\", z_mask.shape,   \"valid_interaction_z shape\", valid_interaction_z.shape, \"affinity_pred shape\", affinity_pred.shape)\n",
    "        if self.readout_mode == 2:\n",
    "            pair_energy = (self.gate_linear(z).sigmoid() * self.linear_energy(z)).squeeze(-1) * z_mask\n",
    "            affinity_pred = self.leaky(self.bias + ((pair_energy).sum(axis=(-1, -2))))\n",
    "        return y_pred, affinity_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TriangleSelfAttentionRowWise(\n",
       "  (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "  (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "  (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "  (g): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (final_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_og.triangle_self_attention_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bindbind.models.models.tankbind_layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tankbind_philip.TankBind.tankbind.data import TankBindDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/fs/pool/pool-marsot/pdbbind/pdbbind2020/dataset/processed/data.pt', '/fs/pool/pool-marsot/pdbbind/pdbbind2020/dataset/processed/protein.pt', '/fs/pool/pool-marsot/pdbbind/pdbbind2020/dataset/processed/compound.pt']\n",
      "['/fs/pool/pool-marsot/tankbind_philip/TankBind/dataset/test_dataset/processed/data.pt', '/fs/pool/pool-marsot/tankbind_philip/TankBind/dataset/test_dataset/processed/protein.pt', '/fs/pool/pool-marsot/tankbind_philip/TankBind/dataset/test_dataset/processed/compound.pt']\n"
     ]
    }
   ],
   "source": [
    "add_noise_to_com = 0.0\n",
    "# compoundMode = 1 is for GIN model.\n",
    "#new_dataset = TankBindDataSet(f\"{pre}/apr22_pdbbind_gvp_pocket_radius20\", add_noise_to_com=add_noise_to_com)'\n",
    "new_dataset = TankBindDataSet(\"/fs/pool/pool-marsot/pdbbind/pdbbind2020/dataset\", add_noise_to_com=add_noise_to_com)\n",
    "# modified by Enzo\n",
    "# load compound features extracted using torchdrug.\n",
    "# new_dataset.compound_dict = torch.load(f\"{pre}/compound_dict.pt\")\n",
    "new_dataset.data = new_dataset.data.query(\"c_length < 100 and native_num_contact > 5\").reset_index(drop=True)\n",
    "d = new_dataset.data\n",
    "only_native_train_index = d.query(\"use_compound_com and group =='train'\").index.values\n",
    "train = new_dataset[only_native_train_index]\n",
    "train_index = d.query(\"group =='train'\").index.values\n",
    "train_after_warm_up = new_dataset[train_index]\n",
    "# train = torch.utils.data.ConcatDataset([train1, train2])\n",
    "valid_index = d.query(\"use_compound_com and group =='valid'\").index.values\n",
    "valid = new_dataset[valid_index]\n",
    "test_index = d.query(\"use_compound_com and group =='test'\").index.values\n",
    "test = new_dataset[test_index]\n",
    "\n",
    "#all_pocket_test_fileName = f\"{pre}/apr23_testset_pdbbind_gvp_pocket_radius20/\"\n",
    "# added by Enzo\n",
    "all_pocket_test_fileName = \"/fs/pool/pool-marsot/tankbind_philip/TankBind/dataset/test_dataset\"\n",
    "all_pocket_test = TankBindDataSet(all_pocket_test_fileName)\n",
    "#all_pocket_test.compound_dict = torch.load(f\"{pre}/compound_dict.pt\")\n",
    "# added by Enzo\n",
    "all_pocket_test.compound_dict = \"/fs/pool/pool-marsot/pdbbind/pdbbind2020/dataset/processed/compound.pt\"\n",
    "# info is used to evaluate the test set. \n",
    "info = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import RandomSampler\n",
    "from torch_geometric.loader import DataLoader\n",
    "sampler = RandomSampler(train, replacement=True, num_samples=20000)\n",
    "train_loader = DataLoader(train, batch_size=4, follow_batch=['x', 'compound_pair'], sampler=sampler, pin_memory=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroDataBatch(\n",
       "  dis_map=[13207],\n",
       "  node_xyz=[539, 3],\n",
       "  coords=[108, 3],\n",
       "  y=[13207],\n",
       "  seq=[539],\n",
       "  affinity=[4],\n",
       "  compound_pair=[4010, 16],\n",
       "  compound_pair_batch=[4010],\n",
       "  compound_pair_ptr=[5],\n",
       "  pdb=[4],\n",
       "  group=[4],\n",
       "  real_affinity_mask=[4],\n",
       "  real_y_mask=[13207],\n",
       "  is_equivalent_native_pocket=[4],\n",
       "  equivalent_native_y_mask=[13207],\n",
       "  protein={\n",
       "    node_s=[539, 6],\n",
       "    node_v=[539, 3, 3],\n",
       "    batch=[539],\n",
       "    ptr=[5],\n",
       "  },\n",
       "  compound={\n",
       "    x=[108, 56],\n",
       "    x_batch=[108],\n",
       "    x_ptr=[5],\n",
       "    batch=[108],\n",
       "    ptr=[5],\n",
       "  },\n",
       "  (protein, p2p, protein)={\n",
       "    edge_index=[2, 12801],\n",
       "    edge_s=[12801, 32],\n",
       "    edge_v=[12801, 1, 3],\n",
       "  },\n",
       "  (compound, c2c, compound)={\n",
       "    edge_index=[2, 218],\n",
       "    edge_weight=[218],\n",
       "    edge_attr=[218, 19],\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IaBNet_with_affinity(\n",
       "  (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (conv_protein): GVP_embedding(\n",
       "    (W_s): Embedding(20, 20)\n",
       "    (W_v): Sequential(\n",
       "      (0): LayerNorm(\n",
       "        (scalar_norm): LayerNorm((26,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): GVP(\n",
       "        (wh): Linear(in_features=3, out_features=16, bias=False)\n",
       "        (ws): Linear(in_features=42, out_features=128, bias=True)\n",
       "        (wv): Linear(in_features=16, out_features=16, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (W_e): Sequential(\n",
       "      (0): LayerNorm(\n",
       "        (scalar_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): GVP(\n",
       "        (wh): Linear(in_features=1, out_features=1, bias=False)\n",
       "        (ws): Linear(in_features=33, out_features=32, bias=True)\n",
       "        (wv): Linear(in_features=1, out_features=1, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x GVPConvLayer(\n",
       "        (conv): GVPConv()\n",
       "        (norm): ModuleList(\n",
       "          (0-1): 2 x LayerNorm(\n",
       "            (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (dropout): ModuleList(\n",
       "          (0-1): 2 x Dropout(\n",
       "            (sdropout): Dropout(p=0.1, inplace=False)\n",
       "            (vdropout): _VDropout()\n",
       "          )\n",
       "        )\n",
       "        (ff_func): Sequential(\n",
       "          (0): GVP(\n",
       "            (wh): Linear(in_features=16, out_features=32, bias=False)\n",
       "            (ws): Linear(in_features=160, out_features=512, bias=True)\n",
       "            (wv): Linear(in_features=32, out_features=32, bias=False)\n",
       "          )\n",
       "          (1): GVP(\n",
       "            (wh): Linear(in_features=32, out_features=32, bias=False)\n",
       "            (ws): Linear(in_features=544, out_features=128, bias=True)\n",
       "            (wv): Linear(in_features=32, out_features=16, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (W_out): Sequential(\n",
       "      (0): LayerNorm(\n",
       "        (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): GVP(\n",
       "        (wh): Linear(in_features=16, out_features=16, bias=False)\n",
       "        (ws): Linear(in_features=144, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_compound): GIN(\n",
       "    (layers): ModuleList(\n",
       "      (0): GraphIsomorphismConv(\n",
       "        (mlp): MultiLayerPerceptron(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=56, out_features=128, bias=True)\n",
       "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (edge_linear): Linear(in_features=19, out_features=56, bias=True)\n",
       "      )\n",
       "      (1): GraphIsomorphismConv(\n",
       "        (mlp): MultiLayerPerceptron(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=128, out_features=56, bias=True)\n",
       "            (1): Linear(in_features=56, out_features=56, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (edge_linear): Linear(in_features=19, out_features=128, bias=True)\n",
       "      )\n",
       "      (2): GraphIsomorphismConv(\n",
       "        (mlp): MultiLayerPerceptron(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=56, out_features=128, bias=True)\n",
       "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (edge_linear): Linear(in_features=19, out_features=56, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (readout): SumReadout()\n",
       "  )\n",
       "  (protein_pair_embedding): Linear(in_features=16, out_features=128, bias=True)\n",
       "  (compound_pair_embedding): Linear(in_features=16, out_features=128, bias=True)\n",
       "  (protein_to_compound_list): ModuleList(\n",
       "    (0-4): 5 x TriangleProteinToCompound_v2(\n",
       "      (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (layernorm_c): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (gate_linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (gate_linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (ending_gate_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (linear_after_sum): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (triangle_self_attention_list): ModuleList(\n",
       "    (0-4): 5 x TriangleSelfAttentionRowWise(\n",
       "      (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "      (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "      (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "      (g): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (final_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (tranistion): Transition(\n",
       "    (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "  )\n",
       "  (linear): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (linear_energy): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (gate_linear): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (leaky): LeakyReLU(negative_slope=0.01)\n",
       "  (dropout): Dropout2d(p=0.25, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_dense_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_dis_one_hot(d, bin_size=2, bin_min=-1, bin_max=30):\n",
    "    # without compute_mode='donot_use_mm_for_euclid_dist' could lead to wrong result.\n",
    "    pair_dis = torch.cdist(d, d, compute_mode='donot_use_mm_for_euclid_dist')\n",
    "    pair_dis[pair_dis>bin_max] = bin_max\n",
    "    pair_dis_bin_index = torch.div(pair_dis - bin_min, bin_size, rounding_mode='floor').long()\n",
    "    pair_dis_one_hot = torch.nn.functional.one_hot(pair_dis_bin_index, num_classes=16)\n",
    "    return pair_dis_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IaBNet_with_affinity(\n",
       "  (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (conv_protein): GVP_embedding(\n",
       "    (W_s): Embedding(20, 20)\n",
       "    (W_v): Sequential(\n",
       "      (0): LayerNorm(\n",
       "        (scalar_norm): LayerNorm((26,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): GVP(\n",
       "        (wh): Linear(in_features=3, out_features=16, bias=False)\n",
       "        (ws): Linear(in_features=42, out_features=128, bias=True)\n",
       "        (wv): Linear(in_features=16, out_features=16, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (W_e): Sequential(\n",
       "      (0): LayerNorm(\n",
       "        (scalar_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): GVP(\n",
       "        (wh): Linear(in_features=1, out_features=1, bias=False)\n",
       "        (ws): Linear(in_features=33, out_features=32, bias=True)\n",
       "        (wv): Linear(in_features=1, out_features=1, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x GVPConvLayer(\n",
       "        (conv): GVPConv()\n",
       "        (norm): ModuleList(\n",
       "          (0-1): 2 x LayerNorm(\n",
       "            (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (dropout): ModuleList(\n",
       "          (0-1): 2 x Dropout(\n",
       "            (sdropout): Dropout(p=0.1, inplace=False)\n",
       "            (vdropout): _VDropout()\n",
       "          )\n",
       "        )\n",
       "        (ff_func): Sequential(\n",
       "          (0): GVP(\n",
       "            (wh): Linear(in_features=16, out_features=32, bias=False)\n",
       "            (ws): Linear(in_features=160, out_features=512, bias=True)\n",
       "            (wv): Linear(in_features=32, out_features=32, bias=False)\n",
       "          )\n",
       "          (1): GVP(\n",
       "            (wh): Linear(in_features=32, out_features=32, bias=False)\n",
       "            (ws): Linear(in_features=544, out_features=128, bias=True)\n",
       "            (wv): Linear(in_features=32, out_features=16, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (W_out): Sequential(\n",
       "      (0): LayerNorm(\n",
       "        (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): GVP(\n",
       "        (wh): Linear(in_features=16, out_features=16, bias=False)\n",
       "        (ws): Linear(in_features=144, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_compound): GIN(\n",
       "    (layers): ModuleList(\n",
       "      (0): GraphIsomorphismConv(\n",
       "        (mlp): MultiLayerPerceptron(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=56, out_features=128, bias=True)\n",
       "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (edge_linear): Linear(in_features=19, out_features=56, bias=True)\n",
       "      )\n",
       "      (1): GraphIsomorphismConv(\n",
       "        (mlp): MultiLayerPerceptron(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=128, out_features=56, bias=True)\n",
       "            (1): Linear(in_features=56, out_features=56, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (edge_linear): Linear(in_features=19, out_features=128, bias=True)\n",
       "      )\n",
       "      (2): GraphIsomorphismConv(\n",
       "        (mlp): MultiLayerPerceptron(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=56, out_features=128, bias=True)\n",
       "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (edge_linear): Linear(in_features=19, out_features=56, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (readout): SumReadout()\n",
       "  )\n",
       "  (protein_pair_embedding): Linear(in_features=16, out_features=128, bias=True)\n",
       "  (compound_pair_embedding): Linear(in_features=16, out_features=128, bias=True)\n",
       "  (protein_to_compound_list): ModuleList(\n",
       "    (0-4): 5 x TriangleProteinToCompound_v2(\n",
       "      (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (layernorm_c): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (gate_linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (gate_linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (ending_gate_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (linear_after_sum): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (triangle_self_attention_list): ModuleList(\n",
       "    (0-4): 5 x TriangleSelfAttentionRowWise(\n",
       "      (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "      (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "      (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "      (g): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (final_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (tranistion): Transition(\n",
       "    (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "  )\n",
       "  (linear): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (linear_energy): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (gate_linear): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (leaky): LeakyReLU(negative_slope=0.01)\n",
       "  (dropout): Dropout2d(p=0.25, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_og.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nodes = (data['protein']['node_s'], data['protein']['node_v'])\n",
    "edges = (data[(\"protein\", \"p2p\", \"protein\")][\"edge_s\"], data[(\"protein\", \"p2p\", \"protein\")][\"edge_v\"])\n",
    "protein_batch = data['protein'].batch\n",
    "protein_out = model_og.conv_protein(nodes, data[(\"protein\", \"p2p\", \"protein\")][\"edge_index\"], edges, data.seq)\n",
    "\n",
    "\n",
    "compound_x = data['compound'].x.float()\n",
    "compound_edge_index = data[(\"compound\", \"c2c\", \"compound\")].edge_index.T\n",
    "compound_edge_feature = data[(\"compound\", \"c2c\", \"compound\")].edge_attr\n",
    "edge_weight = data[(\"compound\", \"c2c\", \"compound\")].edge_weight\n",
    "compound_batch = data['compound'].batch\n",
    "# Enzo : print dimensions\n",
    "#print(f\"{compound_edge_index.shape=}, {edge_weight.shape=}, {compound_edge_feature.shape=}, {compound_x.shape=}\")\n",
    "compound_out = model_og.conv_compound(compound_edge_index,edge_weight,compound_edge_feature,compound_x.shape[0],compound_x)['node_feature']\n",
    "\n",
    "protein_out_batched, protein_out_mask = to_dense_batch(protein_out, protein_batch)\n",
    "compound_out_batched, compound_out_mask = to_dense_batch(compound_out, compound_batch)\n",
    "\n",
    "node_xyz = data.node_xyz\n",
    "\n",
    "p_coords_batched, p_coords_mask = to_dense_batch(node_xyz, protein_batch)\n",
    "\n",
    "protein_pair = get_pair_dis_one_hot(p_coords_batched, bin_size=2, bin_min=-1, bin_max=model_og.protein_bin_max)\n",
    "compound_pair_batched, compound_pair_batched_mask = to_dense_batch(data.compound_pair, data.compound_pair_batch)\n",
    "batch_n = compound_pair_batched.shape[0]\n",
    "max_compound_size_square = compound_pair_batched.shape[1]\n",
    "max_compound_size = int(max_compound_size_square**0.5)\n",
    "assert (max_compound_size**2 - max_compound_size_square)**2 < 1e-4\n",
    "compound_pair = torch.zeros((batch_n, max_compound_size, max_compound_size, 16)).to(data.compound_pair.device)\n",
    "for i in range(batch_n):\n",
    "    one = compound_pair_batched[i]\n",
    "    compound_size_square = (data.compound_pair_batch == i).sum()\n",
    "    compound_size = int(compound_size_square**0.5)\n",
    "    compound_pair[i,:compound_size, :compound_size] = one[:compound_size_square].reshape(\n",
    "                                                        (compound_size, compound_size, -1))\n",
    "protein_pair = model_og.protein_pair_embedding(protein_pair.float())\n",
    "compound_pair = model_og.compound_pair_embedding(compound_pair.float())\n",
    "\n",
    "protein_out_batched = model_og.layernorm(protein_out_batched)\n",
    "compound_out_batched = model_og.layernorm(compound_out_batched)\n",
    "# z of shape, b, protein_length, compound_length, channels.\n",
    "z = torch.einsum(\"bik,bjk->bijk\", protein_out_batched, compound_out_batched)\n",
    "z_mask = torch.einsum(\"bi,bj->bij\", protein_out_mask, compound_out_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i_module in range(self.n_trigonometry_module_stack):\n",
    "    z = z + self.dropout(self.protein_to_compound_list[i_module](z, protein_pair, compound_pair, z_mask.unsqueeze(-1)))\n",
    "    z = z + self.dropout(self.triangle_self_attention_list[i_module](z, z_mask))\n",
    "    z = self.tranistion(z)\n",
    "\n",
    "\n",
    "b = self.linear(z).squeeze(-1)\n",
    "y_pred = b[z_mask]\n",
    "y_pred = y_pred.sigmoid() * 10   # normalize to 0 to 10.\n",
    "if self.readout_mode == 0:\n",
    "    pair_energy = self.linear_energy(z).squeeze(-1) * z_mask\n",
    "    affinity_pred = self.leaky(self.bias + ((pair_energy).sum(axis=(-1, -2))))\n",
    "if self.readout_mode == 1:\n",
    "    # valid_interaction_z = (z * z_mask.unsqueeze(-1)).mean(axis=(1, 2))\n",
    "    valid_interaction_z = (z * z_mask.unsqueeze(-1)).sum(axis=(1, 2)) / z_mask.sum(axis=(1, 2)).unsqueeze(-1)\n",
    "    affinity_pred = self.linear_energy(valid_interaction_z).squeeze(-1)\n",
    "    # print(\"z shape\", z.shape, \"z_mask shape\", z_mask.shape,   \"valid_interaction_z shape\", valid_interaction_z.shape, \"affinity_pred shape\", affinity_pred.shape)\n",
    "if self.readout_mode == 2:\n",
    "    pair_energy = (self.gate_linear(z).sigmoid() * self.linear_energy(z)).squeeze(-1) * z_mask\n",
    "    affinity_pred = self.leaky(self.bias + ((pair_energy).sum(axis=(-1, -2))))\n",
    "return y_pred, affinity_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model_og.triangle_self_attention_list[0](z, z_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 177, 55, 128])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 177, 55])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "att0 = model_og.triangle_self_attention_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = att0.layernorm(z)\n",
    "p_length = y.shape[1]\n",
    "batch_n = y.shape[0]\n",
    "z_i = y\n",
    "z_mask_i = z_mask.view((batch_n, p_length, 1, 1, -1))\n",
    "attention_mask_i = (1e9 * (z_mask_i.float() - 1.))\n",
    "q = att0.reshape_last_dim(att0.linear_q(z_i))\n",
    "k = att0.reshape_last_dim(att0.linear_k(z_i))\n",
    "v = att0.reshape_last_dim(att0.linear_v(z_i))\n",
    "logits = torch.einsum('biqhc,bikhc->bihqk', q, k) + attention_mask_i\n",
    "weights = nn.Softmax(dim=-1)(logits)\n",
    "weighted_avg = torch.einsum('bihqk,bikhc->biqhc', weights, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 177, 55, 4, 32])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etape 1: padding et batching: verifier qu'on obtient le meme resultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader.dataloader import Collater\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "import torch\n",
    "\n",
    "\n",
    "class TankBindDataLoader(torch.utils.data.DataLoader):\n",
    "    \"\"\"Subclass of the torch DataLoader, in order to apply the collate function TankBindCollater.\"\"\"\n",
    "    def __init__(self,\n",
    "                 dataset,\n",
    "                 batch_size=1,\n",
    "                 shuffle=False,\n",
    "                 follow_batch=None,\n",
    "                 exclude_keys=None,\n",
    "                 make_divisible_by_8=True,\n",
    "                 **kwargs):\n",
    "        self.follow_batch = follow_batch\n",
    "        self.exclude_keys = exclude_keys\n",
    "        self.make_divisible_by_8=make_divisible_by_8\n",
    "        super().__init__(dataset,\n",
    "                         batch_size,\n",
    "                         shuffle,\n",
    "                         collate_fn=TankBindCollater(dataset, follow_batch, exclude_keys, make_divisible_by_8=self.make_divisible_by_8),\n",
    "                         **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "class TankBindCollater(Collater):\n",
    "    \"\"\"Applies batching operations and computations of masks in place of the model, in order to avoid having to recompute it in the\n",
    "    forward pass on GPU.\"\"\"\n",
    "    def __init__(self, dataset,\n",
    "                 follow_batch=None,\n",
    "                 exclude_keys=None,\n",
    "                 make_divisible_by_8=True):\n",
    "        super().__init__(dataset, follow_batch, exclude_keys)\n",
    "        self.make_divisible_by_8 = make_divisible_by_8\n",
    "    def __call__(self, batch):\n",
    "        data = super().__call__(batch)\n",
    "        if self.make_divisible_by_8:\n",
    "            max_dim_divisible_by_8_protein = 8 * (torch.diff(data[\"protein\"].ptr).max() // 8 + 1)\n",
    "            max_dim_divisible_by_8_compound = 8 * (torch.diff(data[\"compound\"].ptr).max() // 8 + 1)\n",
    "        else:\n",
    "            max_dim_divisible_by_8_protein = torch.diff(data[\"protein\"].ptr).max()\n",
    "            max_dim_divisible_by_8_compound = torch.diff(data[\"compound\"].ptr).max()\n",
    "        protein_coordinates_batched, _ = to_dense_batch(\n",
    "            data.node_xyz, data[\"protein\"].batch,\n",
    "            max_num_nodes=max_dim_divisible_by_8_protein,\n",
    "            )\n",
    "        protein_pairwise_representation = get_pair_dis_index(\n",
    "            protein_coordinates_batched,\n",
    "            bin_size=2,\n",
    "            bin_min=-1,\n",
    "            bin_max=protein_bin_max,\n",
    "            ) # shape [batch_n, max_protein_size, max_protein_size, 16]\n",
    "        _compound_lengths = (data[\"compound\"].ptr[1:] - data[\"compound\"].ptr[:-1]) ** 2\n",
    "        _total = torch.cumsum(_compound_lengths, 0)\n",
    "        compound_pairwise_distance_batch = torch.zeros(\n",
    "                _total[-1], dtype=torch.long\n",
    "            )\n",
    "        for i in range(len(_total) - 1):\n",
    "            compound_pairwise_distance_batch[_total[i] : _total[i + 1]] = i + 1\n",
    "        compound_pair_batched, compound_pair_batched_mask = to_dense_batch(\n",
    "            data.compound_pair,\n",
    "            data.compound_pair_batch,\n",
    "            )\n",
    "        compound_pairwise_representation = torch.zeros(\n",
    "            (len(batch), max_dim_divisible_by_8_compound, max_dim_divisible_by_8_compound, 16),\n",
    "            dtype=torch.float32,\n",
    "            )\n",
    "        for i in range(len(batch)):\n",
    "            one = compound_pair_batched[i]\n",
    "            compound_size_square = (compound_pairwise_distance_batch == i).sum()\n",
    "            compound_size = int(compound_size_square**0.5)\n",
    "            compound_pairwise_representation[i, :compound_size, :compound_size] = one[\n",
    "                :compound_size_square\n",
    "                ].reshape((compound_size, compound_size, -1))\n",
    "        data.batch_n = len(batch)\n",
    "        data.max_dim_divisible_by_8_protein = max_dim_divisible_by_8_protein\n",
    "        data.max_dim_divisible_by_8_compound = max_dim_divisible_by_8_compound\n",
    "        data[\"protein\", \"p2p\", \"protein\"].pairwise_representation = protein_pairwise_representation\n",
    "        data[\"compound\", \"p2p\", \"compound\"].pairwise_representation = compound_pairwise_representation\n",
    "        data[\"compound\", \"p2p\", \"compound\"].pairwise_representation_mask = compound_pair_batched_mask\n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_pair_dis_index(d, bin_size=2, bin_min=-1, bin_max=30):\n",
    "    \"\"\"\n",
    "    Computing pairwise distances and binning.\n",
    "    \"\"\"\n",
    "    pair_dis = torch.cdist(d, d, compute_mode='donot_use_mm_for_euclid_dist')\n",
    "    pair_dis[pair_dis>bin_max] = bin_max\n",
    "    pair_dis_bin_index = torch.div(pair_dis - bin_min, bin_size, rounding_mode='floor').long()\n",
    "    return pair_dis_bin_index\n",
    "\n",
    "protein_bin_max = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m(train, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, follow_batch\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompound_pair\u001b[39m\u001b[38;5;124m'\u001b[39m], pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train, batch_size=4, follow_batch=['x', 'compound_pair'], pin_memory=False, num_workers=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bindbind.torch_datasets.tankbind_dataloader import TankBindDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_2 = TankBindDataLoader(train, batch_size=4, follow_batch=['x', 'compound_pair'], make_divisible_by_8=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_3 = TankBindDataLoader(train, batch_size=4, follow_batch=['x', 'compound_pair'], make_divisible_by_8=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_2 = next(iter(train_loader_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_3 = next(iter(train_loader_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing whether masking has an influence on the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_og.eval()\n",
    "model_og.to('cpu')\n",
    "result_2 = model_og(batch_2.to('cpu'))\n",
    "result_3 = model_og(batch_3.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4.5944, 4.6123, 4.6162,  ..., 4.5521, 4.5225, 4.6108],\n",
       "        grad_fn=<MulBackward0>),\n",
       " tensor([-0.2154, -0.8206, -0.2637, -0.1369], grad_fn=<LeakyReluBackward0>))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4.5944, 4.6123, 4.6162,  ..., 4.5521, 4.5225, 4.6108],\n",
       "        grad_fn=<MulBackward0>),\n",
       " tensor([-0.2154, -0.8206, -0.2637, -0.1369], grad_fn=<LeakyReluBackward0>))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True,  ..., True, True, True])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_2[0] == result_3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.linalg.norm(result_2[0] - result_3[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.linalg.norm(result_2[1] - result_3[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_1[0].eq(result_2[0]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_1[1].eq(result_2[1]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroDataBatch(\n",
       "  dis_map=[9498],\n",
       "  node_xyz=[675, 3],\n",
       "  coords=[55, 3],\n",
       "  y=[9498],\n",
       "  seq=[675],\n",
       "  affinity=[4],\n",
       "  compound_pair=[853, 16],\n",
       "  compound_pair_batch=[853],\n",
       "  compound_pair_ptr=[5],\n",
       "  pdb=[4],\n",
       "  group=[4],\n",
       "  real_affinity_mask=[4],\n",
       "  real_y_mask=[9498],\n",
       "  is_equivalent_native_pocket=[4],\n",
       "  equivalent_native_y_mask=[9498],\n",
       "  protein={\n",
       "    node_s=[675, 6],\n",
       "    node_v=[675, 3, 3],\n",
       "    batch=[675],\n",
       "    ptr=[5],\n",
       "  },\n",
       "  compound={\n",
       "    x=[55, 56],\n",
       "    x_batch=[55],\n",
       "    x_ptr=[5],\n",
       "    batch=[55],\n",
       "    ptr=[5],\n",
       "  },\n",
       "  (protein, p2p, protein)={\n",
       "    edge_index=[2, 16185],\n",
       "    edge_s=[16185, 32],\n",
       "    edge_v=[16185, 1, 3],\n",
       "  },\n",
       "  (compound, c2c, compound)={\n",
       "    edge_index=[2, 110],\n",
       "    edge_weight=[110],\n",
       "    edge_attr=[110, 19],\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_2 = next(iter(train_loader_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroDataBatch(\n",
       "  dis_map=[9498],\n",
       "  node_xyz=[675, 3],\n",
       "  coords=[55, 3],\n",
       "  y=[9498],\n",
       "  seq=[675],\n",
       "  affinity=[4],\n",
       "  compound_pair=[853, 16],\n",
       "  compound_pair_batch=[853],\n",
       "  compound_pair_ptr=[5],\n",
       "  pdb=[4],\n",
       "  group=[4],\n",
       "  real_affinity_mask=[4],\n",
       "  real_y_mask=[9498],\n",
       "  is_equivalent_native_pocket=[4],\n",
       "  equivalent_native_y_mask=[9498],\n",
       "  batch_n=4,\n",
       "  max_dim_divisible_by_8_protein=228,\n",
       "  max_dim_divisible_by_8_compound=22,\n",
       "  protein={\n",
       "    node_s=[675, 6],\n",
       "    node_v=[675, 3, 3],\n",
       "    batch=[675],\n",
       "    ptr=[5],\n",
       "  },\n",
       "  compound={\n",
       "    x=[55, 56],\n",
       "    x_batch=[55],\n",
       "    x_ptr=[5],\n",
       "    batch=[55],\n",
       "    ptr=[5],\n",
       "  },\n",
       "  (protein, p2p, protein)={\n",
       "    edge_index=[2, 16185],\n",
       "    edge_s=[16185, 32],\n",
       "    edge_v=[16185, 1, 3],\n",
       "    pairwise_representation=[4, 228, 228],\n",
       "  },\n",
       "  (compound, c2c, compound)={\n",
       "    edge_index=[2, 110],\n",
       "    edge_weight=[110],\n",
       "    edge_attr=[110, 19],\n",
       "  },\n",
       "  (compound, p2p, compound)={\n",
       "    pairwise_representation=[4, 22, 22, 16],\n",
       "    pairwise_representation_mask=[4, 484],\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroDataBatch(\n",
       "  dis_map=[9498],\n",
       "  node_xyz=[675, 3],\n",
       "  coords=[55, 3],\n",
       "  y=[9498],\n",
       "  seq=[675],\n",
       "  affinity=[4],\n",
       "  compound_pair=[853, 16],\n",
       "  compound_pair_batch=[853],\n",
       "  compound_pair_ptr=[5],\n",
       "  pdb=[4],\n",
       "  group=[4],\n",
       "  real_affinity_mask=[4],\n",
       "  real_y_mask=[9498],\n",
       "  is_equivalent_native_pocket=[4],\n",
       "  equivalent_native_y_mask=[9498],\n",
       "  batch_n=4,\n",
       "  max_dim_divisible_by_8_protein=228,\n",
       "  max_dim_divisible_by_8_compound=22,\n",
       "  protein={\n",
       "    node_s=[675, 6],\n",
       "    node_v=[675, 3, 3],\n",
       "    batch=[675],\n",
       "    ptr=[5],\n",
       "  },\n",
       "  compound={\n",
       "    x=[55, 56],\n",
       "    x_batch=[55],\n",
       "    x_ptr=[5],\n",
       "    batch=[55],\n",
       "    ptr=[5],\n",
       "  },\n",
       "  (protein, p2p, protein)={\n",
       "    edge_index=[2, 16185],\n",
       "    edge_s=[16185, 32],\n",
       "    edge_v=[16185, 1, 3],\n",
       "    pairwise_representation=[4, 228, 228],\n",
       "  },\n",
       "  (compound, c2c, compound)={\n",
       "    edge_index=[2, 110],\n",
       "    edge_weight=[110],\n",
       "    edge_attr=[110, 19],\n",
       "  },\n",
       "  (compound, p2p, compound)={\n",
       "    pairwise_representation=[4, 22, 22, 16],\n",
       "    pairwise_representation_mask=[4, 484],\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = batch_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dim_divisible_by_8_protein = data.max_dim_divisible_by_8_protein\n",
    "max_dim_divisible_by_8_compound = data.max_dim_divisible_by_8_compound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein = model_og.conv_protein(\n",
    "    h_V=(\n",
    "        data[\"protein\"][\"node_s\"],\n",
    "        data[\"protein\"][\"node_v\"],\n",
    "    ),\n",
    "    edge_index=data[(\"protein\", \"p2p\", \"protein\")][\"edge_index\"],\n",
    "    h_E=(\n",
    "        data[(\"protein\", \"p2p\", \"protein\")][\"edge_s\"],\n",
    "        data[(\"protein\", \"p2p\", \"protein\")][\"edge_v\"],\n",
    "    ),\n",
    "    seq=data.seq,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_node_features = data[\"compound\"].x.float()\n",
    "compound_edge_features = data[\"compound\", \"c2c\", \"compound\"].edge_attr\n",
    "compound_embedding_tensor = model_og.conv_compound(\n",
    "    data[\"compound\", \"c2c\", \"compound\"].edge_index.T,\n",
    "    torch.ones(data[\"compound\", \"c2c\", \"compound\"].edge_index.shape[1], dtype=torch.float32),\n",
    "    compound_edge_features,\n",
    "    compound_node_features.shape[0],\n",
    "    compound_node_features,\n",
    ")['node_feature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_batched, protein_mask = to_dense_batch(protein, data[\"protein\"].batch, max_num_nodes=max_dim_divisible_by_8_protein)\n",
    "compound_batched, compound_mask = to_dense_batch(\n",
    "    compound_embedding_tensor, data[\"compound\"].batch,\n",
    "    max_num_nodes=max_dim_divisible_by_8_compound,\n",
    ")\n",
    "protein_batched = model_og.layernorm(protein_batched)\n",
    "compound_batched = model_og.layernorm(compound_batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_pairwise_representation = data[\"protein\", \"p2p\", \"protein\"].pairwise_representation # shape [batch_n, max_protein_size, max_protein_size, 16]\n",
    "compound_pairwise_representation = data[\"compound\", \"p2p\", \"compound\"].pairwise_representation # shape [batch_n, max_compound_size, max_compound_size, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_layer = torch.nn.Embedding(16, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_pairwise_representation_one_hot = torch.nn.functional.one_hot(protein_pairwise_representation, num_classes=16).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_layer.weight = torch.nn.Parameter(model_og.protein_pair_embedding.weight.T+model_og.protein_pair_embedding.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_pair_og = model_og.protein_pair_embedding(protein_pairwise_representation_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_pair = new_layer(protein_pairwise_representation)\n",
    "compound_pair = model_og.compound_pair_embedding(compound_pairwise_representation.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 232, 232, 128])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protein_pair.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## MASKING ########\n",
    "batch_n = data.batch_n\n",
    "z_mask = torch.einsum(\"bi,bj->bij\", protein_mask, compound_mask)\n",
    "# BUG: hardcoded 4 heads...\n",
    "z_mask_attention = torch.einsum(\"bik, bq-> biqk\", z_mask, compound_mask).reshape(batch_n*protein_batched.shape[1], max_dim_divisible_by_8_compound, max_dim_divisible_by_8_compound).unsqueeze(1).expand(-1, 4, -1, -1).contiguous()\n",
    "z_mask_attention = torch.where(z_mask_attention, 0.0, -10.0**6)\n",
    "z_mask_flat = torch.arange(\n",
    "    start=0, end=z_mask.numel(), device=model_og.device\n",
    ").view(z_mask.shape)[z_mask]\n",
    "protein_square_mask = torch.einsum(\"bi,bj->bij\", protein_mask, protein_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.einsum(\"bik,bjk->bijk\", protein_batched, compound_batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 232, 24, 128])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "class TriangleSelfAttentionRowWise(torch.nn.Module):\n",
    "    # use the protein-compound matrix only.\n",
    "    def __init__(self, embedding_channels=128, c=32, num_attention_heads=4):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.attention_head_size = c\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        # self.dp = nn.Dropout(drop_rate)\n",
    "        # self.ln = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        self.layernorm = torch.nn.LayerNorm(embedding_channels)\n",
    "        # self.layernorm_c = torch.nn.LayerNorm(c)\n",
    "\n",
    "        self.linear_q = Linear(embedding_channels, self.all_head_size, bias=False)\n",
    "        self.linear_k = Linear(embedding_channels, self.all_head_size, bias=False)\n",
    "        self.linear_v = Linear(embedding_channels, self.all_head_size, bias=False)\n",
    "        # self.b = Linear(embedding_channels, h, bias=False)\n",
    "        self.g = Linear(embedding_channels, self.all_head_size)\n",
    "        self.final_linear = Linear(self.all_head_size, embedding_channels)\n",
    "\n",
    "    def reshape_last_dim(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x\n",
    "\n",
    "    def forward(self, z, z_mask):\n",
    "        # z of shape b, i, j, embedding_channels, where i is protein dim, j is compound dim.\n",
    "        # z_mask of shape b, i, j\n",
    "        z = self.layernorm(z)\n",
    "        p_length = z.shape[1]\n",
    "        batch_n = z.shape[0]\n",
    "        # new_z = torch.zeros(z.shape, device=z.device)\n",
    "        z_i = z\n",
    "        z_mask_i = z_mask.view((batch_n, p_length, 1, 1, -1))\n",
    "        attention_mask_i = (1e9 * (z_mask_i.float() - 1.))\n",
    "        # q, k, v of shape b, j, h, c\n",
    "        q = self.reshape_last_dim(self.linear_q(z_i))  * (self.attention_head_size**(-0.5))\n",
    "        k = self.reshape_last_dim(self.linear_k(z_i))\n",
    "        v = self.reshape_last_dim(self.linear_v(z_i))\n",
    "        logits = torch.einsum('biqhc,bikhc->bihqk', q, k) + attention_mask_i\n",
    "        weights = nn.Softmax(dim=-1)(logits)\n",
    "        # weights of shape b, h, j, j\n",
    "        # attention_probs = self.dp(attention_probs)\n",
    "        weighted_avg = torch.einsum('bihqk,bikhc->biqhc', weights, v)\n",
    "        g = self.reshape_last_dim(self.g(z_i)).sigmoid()\n",
    "        output = g * weighted_avg\n",
    "        new_output_shape = output.size()[:-2] + (self.all_head_size,)\n",
    "        output = output.view(*new_output_shape)\n",
    "        # output of shape b, j, embedding.\n",
    "        # z[:, i] = output\n",
    "        z = output\n",
    "        # print(g.shape, block1.shape, block2.shape)\n",
    "        z = self.final_linear(z) * z_mask.unsqueeze(-1)\n",
    "        return z\n",
    "    \n",
    "class FastTriangleSelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_channels, num_attention_heads):\n",
    "        super().__init__()\n",
    "        self.layernorm = nn.LayerNorm(embedding_channels, bias=False)\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.attention_head_size = embedding_channels // num_attention_heads\n",
    "        self.linear_qkv = nn.Linear(embedding_channels, 3*embedding_channels, bias=False)\n",
    "        self.output_linear = nn.Linear(embedding_channels, embedding_channels)\n",
    "        self.g = nn.Linear(embedding_channels, embedding_channels)\n",
    "    def forward(self, z, z_mask_attention_float, z_mask):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        z: torch.Tensor of shape [batch, n_protein, n_compound, embedding_channels]\n",
    "        z_mask: torch.Tensor of shape [batch*n_protein*num_attention_heads, n_compound, n_compound] saying which coefficients\n",
    "            correspond to actual data. (we take this weird shape because scaled_dot_product_attention\n",
    "            requires it). We take it to be float(\"-inf\") where we want to mask.\n",
    "        Returns\n",
    "        -------\n",
    "        \"\"\"\n",
    "        z = self.layernorm(z)\n",
    "        batch_size, n_protein, n_compound, embedding_channels = z.shape\n",
    "        z = z.reshape(batch_size*n_protein, n_compound, embedding_channels)\n",
    "        q, k, v = self.linear_qkv(z).chunk(3, dim=-1)\n",
    "        q = q.view(batch_size*n_protein, n_compound, self.num_attention_heads, self.attention_head_size).contiguous()\n",
    "        k = k.view(batch_size*n_protein, n_compound, self.num_attention_heads, self.attention_head_size).contiguous()\n",
    "        v = v.view(batch_size*n_protein, n_compound, self.num_attention_heads, self.attention_head_size).contiguous()\n",
    "        attention_coefficients = xops.memory_efficient_attention(query=q,\n",
    "                                                key=k,\n",
    "                                                value=v,\n",
    "                                                attn_bias=z_mask_attention_float.to(\"cuda:0\")) # shape [batch*protein_nodes, compound_nodes, n_heads, embedding//n_heads]        \n",
    "\n",
    "        attention_output = attention_coefficients.view(batch_size, n_protein, n_compound, embedding_channels)\n",
    "        g = self.g(z).sigmoid()\n",
    "        output = g * attention_output.view(batch_size*n_protein, n_compound, embedding_channels)\n",
    "\n",
    "        output = self.output_linear(output.view(batch_size, n_protein, n_compound, embedding_channels))*z_mask.unsqueeze(-1).to('cuda:0')\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FastTriangleSelfAttention(\n",
       "  (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (linear_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "  (output_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (g): Linear(in_features=128, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_attention = FastTriangleSelfAttention(embedding_channels=128, num_attention_heads=4)\n",
    "attention = TriangleSelfAttentionRowWise(embedding_channels=128, c=32, num_attention_heads=4)\n",
    "fast_attention.layernorm.weight = attention.layernorm.weight\n",
    "fast_attention.layernorm.bias = attention.layernorm.bias\n",
    "fast_attention.linear_qkv.weight = torch.nn.Parameter(torch.cat([attention.linear_q.weight, attention.linear_k.weight, attention.linear_v.weight], dim=0))\n",
    "fast_attention.output_linear.weight = attention.final_linear.weight\n",
    "fast_attention.output_linear.bias = attention.final_linear.bias\n",
    "fast_attention.g.weight = attention.g.weight\n",
    "fast_attention.g.bias = attention.g.bias\n",
    "attention.to('cuda:0')\n",
    "fast_attention.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = z_2.clone()\n",
    "z = z.to(\"cuda:0\")\n",
    "z = attention.layernorm(z)\n",
    "p_length = z.shape[1]\n",
    "batch_n = z.shape[0]\n",
    "# new_z = torch.zeros(z.shape, device=z.device)\n",
    "z_i = z\n",
    "z_mask_i = z_mask.view((batch_n, p_length, 1, 1, -1))\n",
    "attention_mask_i = (1e9 * (z_mask_i.float() - 1.))\n",
    "# q, k, v of shape b, j, h, c\n",
    "q = attention.reshape_last_dim(attention.linear_q(z_i))  * (attention.attention_head_size**(-0.5))\n",
    "k = attention.reshape_last_dim(attention.linear_k(z_i))\n",
    "v = attention.reshape_last_dim(attention.linear_v(z_i))\n",
    "logits = torch.einsum('biqhc,bikhc->bihqk', q, k) + attention_mask_i.to('cuda:0')\n",
    "weights = nn.Softmax(dim=-1)(logits)\n",
    "# weights of shape b, h, j, j\n",
    "# attention_probs = self.dp(attention_probs)\n",
    "weighted_avg = torch.einsum('bihqk,bikhc->biqhc', weights, v)\n",
    "g_1 = attention.reshape_last_dim(attention.g(z_i)).sigmoid()\n",
    "output_1 = g_1 * weighted_avg\n",
    "new_output_shape = output_1.size()[:-2] + (attention.all_head_size,)\n",
    "output_1 = output_1.view(*new_output_shape)\n",
    "# output of shape b, j, embedding.\n",
    "# z[:, i] = output\n",
    "z = output_1\n",
    "# print(g.shape, block1.shape, block2.shape)\n",
    "z_out = attention.final_linear(z) * z_mask.unsqueeze(-1).to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        ...,\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True]], device='cuda:0')"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_attention.g.weight == attention.g.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = z_2.clone().to(\"cuda:0\")\n",
    "z = fast_attention.layernorm(z)\n",
    "batch_size, n_protein, n_compound, embedding_channels = z.shape\n",
    "z = z.reshape(batch_size*n_protein, n_compound, embedding_channels)\n",
    "q, k, v = fast_attention.linear_qkv(z).chunk(3, dim=-1)\n",
    "q = q.view(batch_size*n_protein, n_compound, fast_attention.num_attention_heads, fast_attention.attention_head_size).contiguous()\n",
    "k = k.view(batch_size*n_protein, n_compound, fast_attention.num_attention_heads, fast_attention.attention_head_size).contiguous()\n",
    "v = v.view(batch_size*n_protein, n_compound, fast_attention.num_attention_heads, fast_attention.attention_head_size).contiguous()\n",
    "attention_coefficients = xops.memory_efficient_attention(query=q,\n",
    "                                        key=k,\n",
    "                                        value=v,\n",
    "                                        attn_bias=z_mask_attention_float.to(\"cuda:0\")) # shape [batch*protein_nodes, compound_nodes, n_heads, embedding//n_heads]        \n",
    "\n",
    "attention_output = attention_coefficients.view(batch_size, n_protein, n_compound, embedding_channels)\n",
    "g = fast_attention.g(z).sigmoid()\n",
    "output = g * attention_output.view(batch_size*n_protein, n_compound, embedding_channels)\n",
    "\n",
    "output = fast_attention.output_linear(output.view(batch_size, n_protein, n_compound, embedding_channels))*z_mask.unsqueeze(-1).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 232, 24])"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 232, 24, 128])"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.1219,  0.0946, -0.1399,  ...,  0.0886,  0.1805,  0.2867],\n",
       "          [ 0.1213,  0.0935, -0.1294,  ...,  0.0775,  0.2057,  0.2919],\n",
       "          [ 0.1147,  0.0873, -0.1234,  ...,  0.0750,  0.1941,  0.3020],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0202,  0.1372,  0.1826,  ..., -0.2824, -0.0042,  0.0900],\n",
       "          [ 0.0239,  0.1361,  0.1826,  ..., -0.2821, -0.0052,  0.1046],\n",
       "          [ 0.0249,  0.1438,  0.1881,  ..., -0.2812, -0.0038,  0.0968],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.1441,  0.1901,  0.0691,  ..., -0.2150,  0.0659,  0.2961],\n",
       "          [ 0.1295,  0.1872,  0.0991,  ..., -0.2252,  0.0514,  0.3245],\n",
       "          [ 0.1374,  0.1762,  0.0858,  ..., -0.2319,  0.0573,  0.3163],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0579,  0.3863,  0.0120,  ..., -0.2456,  0.1124,  0.2127],\n",
       "          [ 0.0458,  0.3874,  0.0144,  ..., -0.2418,  0.1237,  0.2206],\n",
       "          [ 0.0397,  0.3864,  0.0144,  ..., -0.2437,  0.1302,  0.2151],\n",
       "          ...,\n",
       "          [ 0.0548,  0.3775,  0.0285,  ..., -0.2385,  0.1457,  0.2165],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0970, -0.0641,  0.1602,  ..., -0.4456,  0.0884,  0.0098],\n",
       "          [ 0.1006, -0.0736,  0.1599,  ..., -0.4358,  0.0992,  0.0036],\n",
       "          [ 0.1008, -0.0799,  0.1633,  ..., -0.4250,  0.1074, -0.0099],\n",
       "          ...,\n",
       "          [ 0.0917, -0.1038,  0.1665,  ..., -0.3880,  0.1401,  0.0059],\n",
       "          [ 0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "          [ 0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000]],\n",
       "\n",
       "         [[ 0.1480,  0.3316, -0.0275,  ..., -0.2920,  0.0186,  0.2331],\n",
       "          [ 0.1514,  0.3330, -0.0287,  ..., -0.2822,  0.0238,  0.2293],\n",
       "          [ 0.1493,  0.3264, -0.0274,  ..., -0.2773,  0.0231,  0.2321],\n",
       "          ...,\n",
       "          [ 0.1666,  0.3068, -0.0497,  ..., -0.2798,  0.0178,  0.2343],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[-0.0746, -0.1967,  0.0162,  ..., -0.1968,  0.4505,  0.2298],\n",
       "          [-0.0646, -0.1932, -0.0059,  ..., -0.2223,  0.4658,  0.2402],\n",
       "          [-0.0615, -0.2026,  0.0037,  ..., -0.2195,  0.4763,  0.2346],\n",
       "          ...,\n",
       "          [-0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "          [-0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0389,  0.0796, -0.1508,  ..., -0.1452,  0.0717, -0.0121],\n",
       "          [ 0.0281,  0.0752, -0.1481,  ..., -0.1459,  0.0601, -0.0098],\n",
       "          [ 0.0299,  0.0737, -0.1603,  ..., -0.1186,  0.0664, -0.0135],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0914,  0.1795, -0.0567,  ...,  0.0015,  0.2513,  0.2738],\n",
       "          [ 0.0894,  0.1818, -0.0469,  ..., -0.0117,  0.2745,  0.2761],\n",
       "          [ 0.0866,  0.1770, -0.0469,  ..., -0.0074,  0.2616,  0.2789],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0258, -0.0634,  0.2089,  ..., -0.1261,  0.2342,  0.1758],\n",
       "          [ 0.0362, -0.0425,  0.2322,  ..., -0.1330,  0.2609,  0.1815],\n",
       "          [ 0.0466, -0.0330,  0.2436,  ..., -0.1403,  0.2590,  0.1781],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.1041,  0.1160,  0.2641,  ..., -0.1973,  0.0619, -0.0022],\n",
       "          [ 0.1017,  0.1264,  0.2738,  ..., -0.1951,  0.0638,  0.0111],\n",
       "          [ 0.1002,  0.1303,  0.2797,  ..., -0.1928,  0.0695,  0.0152],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[-0.0395, -0.2458,  0.0783,  ..., -0.2252,  0.3441,  0.1479],\n",
       "          [-0.0339, -0.2414,  0.0723,  ..., -0.2281,  0.3471,  0.1582],\n",
       "          [-0.0363, -0.2408,  0.0798,  ..., -0.2191,  0.3497,  0.1607],\n",
       "          ...,\n",
       "          [ 0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],\n",
       "       device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6822e-07, device='cuda:0', grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(z_out - output).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.6849e-05, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(weighted_avg - attention_output.view(4, 232, 24, 4, 32)).norm()/attention_output.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 232, 24, 128])"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-4.1723e-01,  1.7940e-01, -3.4570e-01,  ..., -1.7574e-01,\n",
       "            3.7142e-01, -2.3193e-02],\n",
       "          [-4.1757e-01,  1.8025e-01, -3.4542e-01,  ..., -1.7599e-01,\n",
       "            3.7168e-01, -2.3248e-02],\n",
       "          [-4.1740e-01,  1.7964e-01, -3.4540e-01,  ..., -1.7634e-01,\n",
       "            3.7122e-01, -2.3353e-02],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         [[-9.5555e-02,  1.9026e-01,  6.7507e-02,  ..., -3.1940e-01,\n",
       "            3.8911e-02, -2.0688e-02],\n",
       "          [-9.4462e-02,  1.9031e-01,  6.7647e-02,  ..., -3.1976e-01,\n",
       "            3.8055e-02, -2.1225e-02],\n",
       "          [-9.5457e-02,  1.9091e-01,  6.7285e-02,  ..., -3.1949e-01,\n",
       "            3.7964e-02, -2.0498e-02],\n",
       "          ...,\n",
       "          [-0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         [[-1.2435e-01, -3.1340e-01,  3.8553e-01,  ..., -9.6576e-01,\n",
       "           -5.7030e-01,  5.8297e-01],\n",
       "          [-1.2578e-01, -3.1332e-01,  3.8568e-01,  ..., -9.6505e-01,\n",
       "           -5.6990e-01,  5.8214e-01],\n",
       "          [-1.2405e-01, -3.1357e-01,  3.8612e-01,  ..., -9.6583e-01,\n",
       "           -5.6986e-01,  5.8226e-01],\n",
       "          ...,\n",
       "          [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[-3.3897e-01,  3.1317e-01,  7.0224e-02,  ..., -1.4162e-01,\n",
       "            4.3227e-01,  2.4534e-01],\n",
       "          [-3.4139e-01,  3.1569e-01,  7.8128e-02,  ..., -1.4321e-01,\n",
       "            4.2966e-01,  2.4286e-01],\n",
       "          [-3.3784e-01,  3.0511e-01,  6.5826e-02,  ..., -1.3778e-01,\n",
       "            4.3878e-01,  2.5285e-01],\n",
       "          ...,\n",
       "          [ 2.3892e-01,  3.7207e-01, -6.9381e-02,  ...,  4.9107e-03,\n",
       "           -2.1128e-01, -3.0360e-01],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         [[-3.8597e-01,  2.4572e-01,  3.0386e-01,  ..., -6.7982e-01,\n",
       "           -4.6803e-01,  2.7898e-01],\n",
       "          [-3.8572e-01,  2.4266e-01,  3.0941e-01,  ..., -6.7805e-01,\n",
       "           -4.6945e-01,  2.7759e-01],\n",
       "          [-3.8146e-01,  2.4303e-01,  3.1053e-01,  ..., -6.7329e-01,\n",
       "           -4.6016e-01,  2.7934e-01],\n",
       "          ...,\n",
       "          [-2.1096e-01,  2.8790e-01, -2.0351e-01,  ..., -2.3289e-01,\n",
       "            1.6007e-01, -8.7851e-02],\n",
       "          [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         [[-3.2740e-01, -1.4240e-03,  1.2069e-01,  ..., -5.6345e-01,\n",
       "           -2.3311e-01,  1.6475e-01],\n",
       "          [-3.2816e-01,  7.4526e-04,  1.2037e-01,  ..., -5.6310e-01,\n",
       "           -2.3402e-01,  1.6381e-01],\n",
       "          [-3.2715e-01,  2.0571e-03,  1.2017e-01,  ..., -5.6543e-01,\n",
       "           -2.3501e-01,  1.6459e-01],\n",
       "          ...,\n",
       "          [ 1.8694e-01,  3.2438e-01, -1.4075e-02,  ...,  2.6587e-02,\n",
       "           -5.0678e-01,  1.7348e-02],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
       "           -0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  0.0000e+00,\n",
       "           -0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[-4.2965e-01,  6.7996e-02,  3.1258e-01,  ..., -2.0575e-01,\n",
       "           -2.5262e-01, -4.5816e-01],\n",
       "          [-4.2950e-01,  6.7346e-02,  3.1287e-01,  ..., -2.0668e-01,\n",
       "           -2.5202e-01, -4.5806e-01],\n",
       "          [-1.4458e-01,  1.5674e-01, -2.3354e-02,  ..., -1.2833e-01,\n",
       "           -1.7203e-01, -2.3394e-01],\n",
       "          ...,\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         [[-4.6505e-01, -1.7947e-01,  5.0049e-01,  ..., -2.4182e-01,\n",
       "            8.7363e-03, -2.3848e-01],\n",
       "          [-4.6687e-01, -1.7997e-01,  4.9925e-01,  ..., -2.4145e-01,\n",
       "            8.9612e-03, -2.3843e-01],\n",
       "          [-1.8287e-01, -1.7652e-01,  2.1379e-01,  ..., -2.3170e-02,\n",
       "            1.2340e-01, -8.6765e-02],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         [[-5.7936e-01,  9.7395e-02, -3.6683e-01,  ..., -1.1096e-01,\n",
       "            3.9739e-01, -3.2882e-02],\n",
       "          [-5.7792e-01,  9.8320e-02, -3.6821e-01,  ..., -1.1045e-01,\n",
       "            3.9706e-01, -3.2428e-02],\n",
       "          [-4.5922e-01,  1.4959e-01, -2.7664e-01,  ..., -4.0960e-02,\n",
       "            3.5908e-01, -3.6697e-02],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[-7.9597e-01,  1.0450e-01,  2.8465e-01,  ..., -3.8185e-01,\n",
       "            4.4020e-01,  7.6678e-02],\n",
       "          [-7.9472e-01,  1.0755e-01,  2.8624e-01,  ..., -3.8219e-01,\n",
       "            4.4228e-01,  7.8247e-02],\n",
       "          [-5.8658e-01,  1.3080e-01,  2.4005e-04,  ..., -8.1558e-02,\n",
       "            4.3269e-01,  1.4781e-01],\n",
       "          ...,\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         [[-1.8491e-01,  4.3350e-01,  1.2315e-01,  ..., -2.8777e-01,\n",
       "            3.6362e-01,  2.8658e-02],\n",
       "          [-1.8514e-01,  4.3944e-01,  1.2722e-01,  ..., -2.9033e-01,\n",
       "            3.6795e-01,  3.2948e-02],\n",
       "          [-1.6285e-01,  3.3232e-01,  2.2990e-02,  ..., -8.7628e-02,\n",
       "            3.6654e-01, -2.2863e-02],\n",
       "          ...,\n",
       "          [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "           -0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         [[-4.7542e-01,  1.0147e-02,  2.7784e-01,  ..., -3.8840e-01,\n",
       "           -1.6149e-01, -3.0386e-01],\n",
       "          [-4.7523e-01,  1.2035e-02,  2.7677e-01,  ..., -3.8990e-01,\n",
       "           -1.6244e-01, -3.0161e-01],\n",
       "          [-2.5653e-01,  7.5977e-02,  4.0630e-02,  ..., -2.3460e-01,\n",
       "           -9.4682e-02, -2.0680e-01],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]]]], device='cuda:0',\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = z_2.clone()\n",
    "fast_attention.to(\"cuda:0\")\n",
    "fast_attention(z.to(\"cuda:0\"), z_mask_attention.to(\"cuda:0\"), z_mask.to(\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-8.0932e-02,  4.2761e-01,  7.3466e-02,  ...,  1.0252e-01,\n",
       "            3.5413e-01, -3.0100e-01],\n",
       "          [-7.9785e-02,  4.2358e-01,  7.5949e-02,  ...,  7.9677e-02,\n",
       "            3.4278e-01, -2.9386e-01],\n",
       "          [-8.1421e-02,  4.4335e-01,  7.5089e-02,  ...,  8.1080e-02,\n",
       "            3.4571e-01, -3.0940e-01],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         [[-1.3750e-01,  1.9741e-01,  1.9737e-01,  ...,  8.0988e-02,\n",
       "            3.8124e-01, -3.2311e-01],\n",
       "          [-1.1666e-01,  2.1328e-01,  1.8591e-01,  ...,  8.9159e-02,\n",
       "            3.5562e-01, -3.1623e-01],\n",
       "          [-1.2569e-01,  2.1042e-01,  1.9157e-01,  ...,  9.5573e-02,\n",
       "            3.6962e-01, -3.2571e-01],\n",
       "          ...,\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         [[-2.0786e-01,  1.6818e-01,  2.0850e-01,  ..., -2.1700e-01,\n",
       "            2.0729e-01,  1.2318e-02],\n",
       "          [-2.3721e-01,  1.6496e-01,  2.2093e-01,  ..., -2.3630e-01,\n",
       "            1.8822e-01,  3.5982e-03],\n",
       "          [-2.2292e-01,  1.7626e-01,  2.3204e-01,  ..., -2.4587e-01,\n",
       "            1.7019e-01,  6.1125e-04],\n",
       "          ...,\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[-7.5428e-02,  4.8336e-02,  2.0673e-01,  ...,  1.8337e-01,\n",
       "            4.3735e-01, -6.0206e-02],\n",
       "          [-6.7676e-02,  5.1547e-02,  2.0586e-01,  ...,  1.8081e-01,\n",
       "            4.2527e-01, -5.3842e-02],\n",
       "          [-6.1657e-02,  4.9734e-02,  2.0792e-01,  ...,  1.7849e-01,\n",
       "            4.2109e-01, -5.0314e-02],\n",
       "          ...,\n",
       "          [-2.7505e-02,  6.3796e-02,  1.9163e-01,  ...,  1.5941e-01,\n",
       "            4.3331e-01, -6.6308e-02],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         [[-2.4949e-01,  1.0875e-01,  1.7461e-01,  ..., -7.9756e-02,\n",
       "            6.0214e-01, -2.2902e-01],\n",
       "          [-2.4279e-01,  1.0701e-01,  1.7813e-01,  ..., -8.5833e-02,\n",
       "            5.8578e-01, -2.2959e-01],\n",
       "          [-2.3962e-01,  1.1475e-01,  1.7922e-01,  ..., -9.1041e-02,\n",
       "            5.6887e-01, -2.2999e-01],\n",
       "          ...,\n",
       "          [-2.4280e-01,  1.2441e-01,  1.8536e-01,  ..., -1.0218e-01,\n",
       "            5.4365e-01, -2.4733e-01],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         [[-2.3723e-02, -2.9694e-02,  2.1701e-01,  ...,  1.1422e-01,\n",
       "            2.8009e-01, -1.1353e-01],\n",
       "          [-2.0113e-02, -3.4277e-02,  2.1839e-01,  ...,  1.0749e-01,\n",
       "            2.7202e-01, -1.2034e-01],\n",
       "          [-1.6808e-02, -3.7135e-02,  2.2079e-01,  ...,  1.0656e-01,\n",
       "            2.6905e-01, -1.2132e-01],\n",
       "          ...,\n",
       "          [-1.6447e-02, -1.9257e-02,  2.1246e-01,  ...,  1.0878e-01,\n",
       "            3.0500e-01, -1.0747e-01],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[-1.2752e-01,  2.4918e-01,  2.6432e-01,  ...,  6.7006e-03,\n",
       "            4.8054e-01, -3.2787e-01],\n",
       "          [-1.0580e-01,  2.5508e-01,  2.7755e-01,  ..., -2.6851e-03,\n",
       "            4.6796e-01, -3.3417e-01],\n",
       "          [-1.0291e-01,  2.6751e-01,  2.7363e-01,  ...,  7.3930e-03,\n",
       "            4.7116e-01, -3.4911e-01],\n",
       "          ...,\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         [[ 7.0170e-02,  1.8169e-02,  1.5492e-01,  ...,  1.0998e-01,\n",
       "            3.6084e-01, -1.7816e-01],\n",
       "          [ 9.4502e-02,  2.5727e-02,  1.6346e-01,  ...,  1.2480e-01,\n",
       "            3.8104e-01, -1.6898e-01],\n",
       "          [ 6.8534e-02, -2.6583e-03,  1.7173e-01,  ...,  1.0519e-01,\n",
       "            3.3554e-01, -1.8378e-01],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         [[-1.2622e-01,  3.2678e-01,  8.7382e-02,  ...,  1.8029e-01,\n",
       "            4.6486e-01, -2.9833e-01],\n",
       "          [-1.2392e-01,  3.2637e-01,  8.2022e-02,  ...,  1.6701e-01,\n",
       "            4.6508e-01, -2.9417e-01],\n",
       "          [-1.2824e-01,  3.4292e-01,  7.9501e-02,  ...,  1.6706e-01,\n",
       "            4.6684e-01, -2.9792e-01],\n",
       "          ...,\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]]],\n",
       "\n",
       "\n",
       "        [[[-2.0540e-01,  1.6481e-01,  6.3586e-02,  ...,  1.0707e-01,\n",
       "            6.2221e-01, -2.8388e-01],\n",
       "          [-2.2377e-01,  1.7139e-01,  6.4074e-02,  ...,  1.3548e-01,\n",
       "            6.2346e-01, -2.5906e-01],\n",
       "          [-2.3306e-01,  1.6497e-01,  7.3118e-02,  ...,  1.4669e-01,\n",
       "            6.1862e-01, -2.5113e-01],\n",
       "          ...,\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         [[-1.9067e-01,  1.7982e-01,  8.1678e-02,  ...,  6.3423e-02,\n",
       "            1.8434e-01, -2.8235e-01],\n",
       "          [-1.7418e-01,  1.8977e-01,  7.8759e-02,  ...,  8.0897e-02,\n",
       "            1.9385e-01, -2.6628e-01],\n",
       "          [-1.8535e-01,  1.7391e-01,  8.9210e-02,  ...,  8.7767e-02,\n",
       "            2.0475e-01, -2.6006e-01],\n",
       "          ...,\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [-0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         [[-4.4378e-02,  9.9075e-02,  3.6329e-01,  ..., -3.5698e-02,\n",
       "            5.5385e-01, -3.2987e-01],\n",
       "          [-1.5271e-02,  9.8093e-02,  3.7902e-01,  ..., -3.0120e-02,\n",
       "            5.4529e-01, -3.3440e-01],\n",
       "          [-2.4831e-02,  9.9724e-02,  3.8022e-01,  ..., -1.9817e-02,\n",
       "            5.4705e-01, -3.3289e-01],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]],\n",
       "\n",
       "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00, -0.0000e+00]]]], device='cuda:0',\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = z_2.clone()\n",
    "attention.to(\"cuda:0\")\n",
    "attention(z.to(\"cuda:0\"), z_mask.to(\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = model_og.triangle_self_attention_list[0]\n",
    "y = z.clone()\n",
    "y = attention.layernorm(y)\n",
    "p_length = y.shape[1]\n",
    "batch_n = y.shape[0]\n",
    "z_i = y\n",
    "q = attention.reshape_last_dim(attention.linear_q(z_i))\n",
    "k = attention.reshape_last_dim(attention.linear_k(z_i))\n",
    "v = attention.reshape_last_dim(attention.linear_v(z_i))\n",
    "logits = torch.einsum('biqhc,bikhc->bihqk', q, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FastTriangleSelfAttention(\n",
       "  (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (linear_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "  (output_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (g): Linear(in_features=128, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_attention.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = z_2.clone().to('cuda:0')\n",
    "z_mask_attention = z_mask_attention.to('cuda:0')\n",
    "z = fast_attention.layernorm(z)\n",
    "batch_size, n_protein, n_compound, embedding_channels = z.shape\n",
    "z = z.reshape(batch_size*n_protein, n_compound, embedding_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(z[~z_mask.view(928, 24)]==0).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, k, v = fast_attention.linear_qkv(z).chunk(3, dim=-1)\n",
    "q = q.view(batch_size*n_protein, n_compound, fast_attention.num_attention_heads, fast_attention.attention_head_size).contiguous()\n",
    "k = k.view(batch_size*n_protein, n_compound, fast_attention.num_attention_heads, fast_attention.attention_head_size).contiguous()\n",
    "v = v.view(batch_size*n_protein, n_compound, fast_attention.num_attention_heads, fast_attention.attention_head_size).contiguous()\n",
    "attention_coefficients = xops.memory_efficient_attention(query=q,\n",
    "                                        key=k,\n",
    "                                        value=v,\n",
    "                                        attn_bias=z_mask_attention) # shape [batch*protein_nodes, compound_nodes, n_heads, embedding//n_heads]        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_output = attention_coefficients.permute(0, 2, 1, 3).contiguous().view(batch_size, n_protein, n_compound, embedding_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = fast_attention.g(z).sigmoid()\n",
    "output = g * attention_output.view(batch_size*n_protein, n_compound, embedding_channels)\n",
    "\n",
    "output = fast_attention.output_linear(attention_output)*z_mask.unsqueeze(-1).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(output[~z_mask]==0).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(z[~z_mask].norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_2 = z.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = attention.layernorm(z)\n",
    "p_length = z.shape[1]\n",
    "batch_n = z.shape[0]\n",
    "# new_z = torch.zeros(z.shape, device=z.device)\n",
    "z_i = z\n",
    "z_mask_i = z_mask.view((batch_n, p_length, 1, 1, -1))\n",
    "attention_mask_i = (1e9 * (z_mask_i.float() - 1.))\n",
    "# q, k, v of shape b, j, h, c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention.layernorm.bias = torch.nn.Parameter(10*torch.ones(128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1278.6993, grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(z[~z_mask].norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = attention.reshape_last_dim(attention.linear_q(z_i)) #  * (self.attention_head_size**(-0.5))\n",
    "k = attention.reshape_last_dim(attention.linear_k(z_i))\n",
    "v = attention.reshape_last_dim(attention.linear_v(z_i))\n",
    "logits = torch.einsum('biqhc,bikhc->bihqk', q, k) + attention_mask_i\n",
    "weights = nn.Softmax(dim=-1)(logits) + attention_mask_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "_attention_mask_i = attention_mask_i.expand(4, 232, 4, 24, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(logits[_attention_mask_i < 0]>-10).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([928, 4, 24, 24])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.view(4*232, 4, 24, 24).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(62.8368, grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(weights.view(4*232, 4, 24, 24)[(z_mask_attention>-1)].relu().norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# weights of shape b, h, j, j\n",
    "# attention_probs = self.dp(attention_probs)\n",
    "weighted_avg = torch.einsum('bihqk,bikhc->biqhc', weights, v)\n",
    "g = attention.reshape_last_dim(attention.g(z_i)).sigmoid()\n",
    "output = g * weighted_avg\n",
    "new_output_shape = output.size()[:-2] + (attention.all_head_size,)\n",
    "output = output.view(*new_output_shape)\n",
    "# output of shape b, j, embedding.\n",
    "# z[:, i] = output\n",
    "z = output\n",
    "# print(g.shape, block1.shape, block2.shape)\n",
    "z = attention.final_linear(z) * z_mask.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_cuda = z.to('cuda:0')\n",
    "z_mask_cuda = z_mask.to('cuda:0')\n",
    "z_mask_attention_float = z_mask_attention.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.1252, -0.1378, -0.4370,  ...,  0.3072, -0.4999,  0.3409],\n",
       "          [-0.1248, -0.1379, -0.4369,  ...,  0.3072, -0.4995,  0.3411],\n",
       "          [-0.1254, -0.1377, -0.4361,  ...,  0.3080, -0.4998,  0.3407],\n",
       "          ...,\n",
       "          [-0.2171, -0.3801,  0.2941,  ..., -0.1717,  0.0874, -0.1296],\n",
       "          [-0.1613, -0.3047,  0.2157,  ..., -0.1519,  0.1093, -0.1257],\n",
       "          [-0.1613, -0.3047,  0.2157,  ..., -0.1519,  0.1093, -0.1257]],\n",
       "\n",
       "         [[-0.2119, -0.3509, -0.4920,  ...,  0.1195,  0.3097,  0.1126],\n",
       "          [-0.2116, -0.3504, -0.4922,  ...,  0.1198,  0.3095,  0.1128],\n",
       "          [-0.2119, -0.3506, -0.4921,  ...,  0.1192,  0.3093,  0.1127],\n",
       "          ...,\n",
       "          [-0.0260,  0.2772,  0.1034,  ...,  0.0422, -0.0248, -0.1400],\n",
       "          [-0.0063,  0.3141,  0.0646,  ...,  0.0381, -0.0311, -0.0800],\n",
       "          [-0.0063,  0.3141,  0.0646,  ...,  0.0381, -0.0311, -0.0800]],\n",
       "\n",
       "         [[-0.0747,  0.0261, -0.4693,  ...,  0.2404, -0.2333,  0.0390],\n",
       "          [-0.0748,  0.0263, -0.4693,  ...,  0.2406, -0.2336,  0.0391],\n",
       "          [-0.0748,  0.0265, -0.4690,  ...,  0.2408, -0.2327,  0.0387],\n",
       "          ...,\n",
       "          [-0.0961, -0.1033,  0.2658,  ..., -0.0807,  0.0386, -0.3387],\n",
       "          [-0.0947,  0.0096,  0.2586,  ..., -0.0822,  0.0564, -0.1986],\n",
       "          [-0.0947,  0.0096,  0.2586,  ..., -0.0822,  0.0564, -0.1986]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          ...,\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054]],\n",
       "\n",
       "         [[ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          ...,\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054]],\n",
       "\n",
       "         [[ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          ...,\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1525, -0.2510, -0.4892,  ..., -0.0040, -0.2841,  0.5829],\n",
       "          [ 0.1438, -0.2569, -0.4864,  ..., -0.0024, -0.2865,  0.5831],\n",
       "          [ 0.1668, -0.2491, -0.4937,  ..., -0.0058, -0.2950,  0.5897],\n",
       "          ...,\n",
       "          [ 0.1266,  0.1371,  0.2536,  ..., -0.1887, -0.2233,  0.2660],\n",
       "          [ 0.1251,  0.1352,  0.2514,  ..., -0.1888, -0.2234,  0.2681],\n",
       "          [ 0.1225,  0.1416,  0.2532,  ..., -0.1714, -0.1908,  0.2618]],\n",
       "\n",
       "         [[-0.2849,  0.2453, -0.5657,  ..., -0.1239, -0.0341, -0.0647],\n",
       "          [-0.2840,  0.2458, -0.5644,  ..., -0.1242, -0.0342, -0.0641],\n",
       "          [-0.2839,  0.2460, -0.5646,  ..., -0.1245, -0.0329, -0.0651],\n",
       "          ...,\n",
       "          [-0.2601, -0.6179,  0.2750,  ...,  0.2808,  0.4203, -0.4464],\n",
       "          [-0.2586, -0.6172,  0.2747,  ...,  0.2808,  0.4202, -0.4467],\n",
       "          [-0.2240, -0.5826,  0.2595,  ...,  0.2599,  0.3637, -0.4379]],\n",
       "\n",
       "         [[-0.0625,  0.1724, -0.6230,  ...,  0.0647, -0.6489,  0.2970],\n",
       "          [-0.0635,  0.1701, -0.6248,  ...,  0.0680, -0.6491,  0.2978],\n",
       "          [-0.0556,  0.1766, -0.6264,  ...,  0.0663, -0.6519,  0.3028],\n",
       "          ...,\n",
       "          [-0.0938, -0.0531,  0.5243,  ..., -0.1023,  0.0667, -0.2244],\n",
       "          [-0.0906, -0.0558,  0.5265,  ..., -0.0999,  0.0690, -0.2287],\n",
       "          [-0.0767, -0.0520,  0.5079,  ..., -0.0885,  0.0730, -0.2375]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          ...,\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054]],\n",
       "\n",
       "         [[ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          ...,\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054]],\n",
       "\n",
       "         [[ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          ...,\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054]]],\n",
       "\n",
       "\n",
       "        [[[-0.3535,  0.0439, -0.6273,  ...,  0.2654, -0.1613, -0.1631],\n",
       "          [-0.3529,  0.0440, -0.6269,  ...,  0.2653, -0.1611, -0.1628],\n",
       "          [-0.2288, -0.0161, -0.4350,  ...,  0.2357, -0.2386, -0.0184],\n",
       "          ...,\n",
       "          [-0.1832, -0.1472, -0.0207,  ..., -0.0182,  0.0198, -0.1105],\n",
       "          [-0.1832, -0.1472, -0.0207,  ..., -0.0182,  0.0198, -0.1105],\n",
       "          [-0.1832, -0.1472, -0.0207,  ..., -0.0182,  0.0198, -0.1105]],\n",
       "\n",
       "         [[ 0.1319,  0.3120, -0.2081,  ...,  0.5012, -0.3741,  0.2467],\n",
       "          [ 0.1330,  0.3078, -0.2083,  ...,  0.5031, -0.3709,  0.2473],\n",
       "          [ 0.0506,  0.2001, -0.2006,  ...,  0.5193, -0.2813,  0.1932],\n",
       "          ...,\n",
       "          [-0.2149, -0.0861, -0.0301,  ..., -0.0457,  0.0316, -0.1250],\n",
       "          [-0.2149, -0.0861, -0.0301,  ..., -0.0457,  0.0316, -0.1250],\n",
       "          [-0.2149, -0.0861, -0.0301,  ..., -0.0457,  0.0316, -0.1250]],\n",
       "\n",
       "         [[ 0.0101, -0.1293, -0.5967,  ...,  0.0969, -0.5291,  0.4813],\n",
       "          [ 0.0116, -0.1291, -0.5959,  ...,  0.0972, -0.5291,  0.4821],\n",
       "          [ 0.1474, -0.0874, -0.4957,  ...,  0.0486, -0.6123,  0.5495],\n",
       "          ...,\n",
       "          [-0.0908, -0.0594,  0.1827,  ..., -0.1203,  0.0173, -0.0110],\n",
       "          [-0.0908, -0.0594,  0.1827,  ..., -0.1203,  0.0173, -0.0110],\n",
       "          [-0.0908, -0.0594,  0.1827,  ..., -0.1203,  0.0173, -0.0110]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          ...,\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054]],\n",
       "\n",
       "         [[ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          ...,\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054]],\n",
       "\n",
       "         [[ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          ...,\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054]]],\n",
       "\n",
       "\n",
       "        [[[-0.3267, -0.0118, -0.5721,  ...,  0.1812,  0.0197,  0.1413],\n",
       "          [-0.3263, -0.0110, -0.5709,  ...,  0.1803,  0.0186,  0.1403],\n",
       "          [-0.1605, -0.0671, -0.3803,  ...,  0.0895, -0.0673,  0.2473],\n",
       "          ...,\n",
       "          [-0.0917, -0.1357,  0.1099,  ..., -0.0032, -0.1207, -0.0444],\n",
       "          [-0.0917, -0.1357,  0.1099,  ..., -0.0032, -0.1207, -0.0444],\n",
       "          [-0.0917, -0.1357,  0.1099,  ..., -0.0032, -0.1207, -0.0444]],\n",
       "\n",
       "         [[-0.2518, -0.4365, -0.4836,  ...,  0.1748,  0.2980,  0.1328],\n",
       "          [-0.2507, -0.4365, -0.4823,  ...,  0.1762,  0.2993,  0.1331],\n",
       "          [-0.1590, -0.2305, -0.3594,  ...,  0.2171,  0.2821,  0.2418],\n",
       "          ...,\n",
       "          [-0.0121,  0.1124,  0.0999,  ...,  0.0331, -0.0636, -0.0727],\n",
       "          [-0.0121,  0.1124,  0.0999,  ...,  0.0331, -0.0636, -0.0727],\n",
       "          [-0.0121,  0.1124,  0.0999,  ...,  0.0331, -0.0636, -0.0727]],\n",
       "\n",
       "         [[-0.4507, -0.0562, -0.8002,  ...,  0.1543, -0.1804, -0.1197],\n",
       "          [-0.4511, -0.0557, -0.7999,  ...,  0.1544, -0.1792, -0.1207],\n",
       "          [-0.2639, -0.0801, -0.5857,  ...,  0.1494, -0.2877,  0.0422],\n",
       "          ...,\n",
       "          [-0.1590, -0.2273,  0.1001,  ...,  0.0016,  0.0854, -0.1634],\n",
       "          [-0.1590, -0.2273,  0.1001,  ...,  0.0016,  0.0854, -0.1634],\n",
       "          [-0.1590, -0.2273,  0.1001,  ...,  0.0016,  0.0854, -0.1634]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          ...,\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054]],\n",
       "\n",
       "         [[ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          ...,\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054]],\n",
       "\n",
       "         [[ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          ...,\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054],\n",
       "          [ 0.0026,  0.0167,  0.0714,  ..., -0.0239, -0.0420,  0.0054]]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_attention(z_cuda, z_mask_attention_float, z_mask_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "No operator found for `memory_efficient_attention_forward` with inputs:\n     query       : shape=(928, 24, 4, 32) (torch.float32)\n     key         : shape=(928, 24, 4, 32) (torch.float32)\n     value       : shape=(928, 24, 4, 32) (torch.float32)\n     attn_bias   : <class 'torch.Tensor'>\n     p           : 0.0\n`flshattF@v2.5.6` is not supported because:\n    device=cpu (supported: {'cuda'})\n    dtype=torch.float32 (supported: {torch.float16, torch.bfloat16})\n    attn_bias type is <class 'torch.Tensor'>\n`cutlassF` is not supported because:\n    device=cpu (supported: {'cuda'})\n`smallkF` is not supported because:\n    device=cpu (supported: {'cuda'})\n    bias with non-zero stride not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfast_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_mask_attention_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz_mask_attention\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs/gpfs41/lv11/fileset01/pool/pool-marsot/.conda/envs/tankbind_py38_h100/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs/gpfs41/lv11/fileset01/pool/pool-marsot/.conda/envs/tankbind_py38_h100/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[72], line 80\u001b[0m, in \u001b[0;36mFastTriangleSelfAttention.forward\u001b[0;34m(self, z, z_mask_attention_float, z_mask)\u001b[0m\n\u001b[1;32m     78\u001b[0m k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mview(batch_size\u001b[38;5;241m*\u001b[39mn_protein, n_compound, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_attention_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_head_size)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m     79\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mview(batch_size\u001b[38;5;241m*\u001b[39mn_protein, n_compound, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_attention_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_head_size)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m---> 80\u001b[0m attention_coefficients \u001b[38;5;241m=\u001b[39m \u001b[43mxops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory_efficient_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mattn_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz_mask_attention_float\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# shape [batch*protein_nodes, compound_nodes, n_heads, embedding//n_heads]        \u001b[39;00m\n\u001b[1;32m     85\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m attention_coefficients\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(batch_size, n_protein, n_compound, embedding_channels)\n\u001b[1;32m     86\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_linear(attention_output)\n",
      "File \u001b[0;32m/fs/gpfs41/lv11/fileset01/pool/pool-marsot/.conda/envs/tankbind_py38_h100/lib/python3.8/site-packages/xformers/ops/fmha/__init__.py:268\u001b[0m, in \u001b[0;36mmemory_efficient_attention\u001b[0;34m(query, key, value, attn_bias, p, scale, op, output_dtype)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmemory_efficient_attention\u001b[39m(\n\u001b[1;32m    157\u001b[0m     query: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    158\u001b[0m     key: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m     output_dtype: Optional[torch\u001b[38;5;241m.\u001b[39mdtype] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    166\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    167\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implements the memory-efficient attention mechanism following\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m    `\"Self-Attention Does Not Need O(n^2) Memory\" <http://arxiv.org/abs/2112.05682>`_.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;03m    :return: multi-head attention Tensor with shape ``[B, Mq, H, Kv]``\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_memory_efficient_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mInputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m            \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m            \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattn_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs/gpfs41/lv11/fileset01/pool/pool-marsot/.conda/envs/tankbind_py38_h100/lib/python3.8/site-packages/xformers/ops/fmha/__init__.py:392\u001b[0m, in \u001b[0;36m_memory_efficient_attention\u001b[0;34m(inp, op)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _memory_efficient_attention_forward(\n\u001b[1;32m    388\u001b[0m         inp, op\u001b[38;5;241m=\u001b[39mop[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     )\n\u001b[1;32m    391\u001b[0m output_shape \u001b[38;5;241m=\u001b[39m inp\u001b[38;5;241m.\u001b[39mnormalize_bmhk()\n\u001b[0;32m--> 392\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_fMHA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(output_shape)\n",
      "File \u001b[0;32m/fs/gpfs41/lv11/fileset01/pool/pool-marsot/.conda/envs/tankbind_py38_h100/lib/python3.8/site-packages/torch/autograd/function.py:598\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    606\u001b[0m     )\n",
      "File \u001b[0;32m/fs/gpfs41/lv11/fileset01/pool/pool-marsot/.conda/envs/tankbind_py38_h100/lib/python3.8/site-packages/xformers/ops/fmha/__init__.py:67\u001b[0m, in \u001b[0;36m_fMHA.forward\u001b[0;34m(ctx, op, *args)\u001b[0m\n\u001b[1;32m     64\u001b[0m op_fw \u001b[38;5;241m=\u001b[39m op[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     65\u001b[0m op_bw \u001b[38;5;241m=\u001b[39m op[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m out, op_ctx \u001b[38;5;241m=\u001b[39m \u001b[43m_memory_efficient_attention_forward_requires_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop_fw\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Saving attn_bias is a bit complicated, as the\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# torch part should go in `save_for_backward`\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inp\u001b[38;5;241m.\u001b[39mattn_bias, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m/fs/gpfs41/lv11/fileset01/pool/pool-marsot/.conda/envs/tankbind_py38_h100/lib/python3.8/site-packages/xformers/ops/fmha/__init__.py:417\u001b[0m, in \u001b[0;36m_memory_efficient_attention_forward_requires_grad\u001b[0;34m(inp, op)\u001b[0m\n\u001b[1;32m    415\u001b[0m output_shape \u001b[38;5;241m=\u001b[39m inp\u001b[38;5;241m.\u001b[39mnormalize_bmhk()\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 417\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_dispatch_fw\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    419\u001b[0m     _ensure_op_supports_or_raise(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_efficient_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m, op, inp)\n",
      "File \u001b[0;32m/fs/gpfs41/lv11/fileset01/pool/pool-marsot/.conda/envs/tankbind_py38_h100/lib/python3.8/site-packages/xformers/ops/fmha/dispatch.py:125\u001b[0m, in \u001b[0;36m_dispatch_fw\u001b[0;34m(inp, needs_gradient)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_dispatch_fw\u001b[39m(inp: Inputs, needs_gradient: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Type[AttentionFwOpBase]:\n\u001b[1;32m    117\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Computes the best operator for forward\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m    Raises:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m        AttentionOp: The best operator for the configuration\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_run_priority_list\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_efficient_attention_forward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_dispatch_fw_priority_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneeds_gradient\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs/gpfs41/lv11/fileset01/pool/pool-marsot/.conda/envs/tankbind_py38_h100/lib/python3.8/site-packages/xformers/ops/fmha/dispatch.py:65\u001b[0m, in \u001b[0;36m_run_priority_list\u001b[0;34m(name, priority_list, inp)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m op, not_supported \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(priority_list, not_supported_reasons):\n\u001b[1;32m     64\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m _format_not_supported_reasons(op, not_supported)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: No operator found for `memory_efficient_attention_forward` with inputs:\n     query       : shape=(928, 24, 4, 32) (torch.float32)\n     key         : shape=(928, 24, 4, 32) (torch.float32)\n     value       : shape=(928, 24, 4, 32) (torch.float32)\n     attn_bias   : <class 'torch.Tensor'>\n     p           : 0.0\n`flshattF@v2.5.6` is not supported because:\n    device=cpu (supported: {'cuda'})\n    dtype=torch.float32 (supported: {torch.float16, torch.bfloat16})\n    attn_bias type is <class 'torch.Tensor'>\n`cutlassF` is not supported because:\n    device=cpu (supported: {'cuda'})\n`smallkF` is not supported because:\n    device=cpu (supported: {'cuda'})\n    bias with non-zero stride not supported"
     ]
    }
   ],
   "source": [
    "fast_attention(z, z_mask_attention_float=z_mask_attention, z_mask=z_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 232, 24, 128])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 232, 24, 4, 32])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = z.clone()\n",
    "for i_module in range(model_og.n_trigonometry_module_stack):\n",
    "    y = y + model_og.dropout(model_og.protein_to_compound_list[i_module](y, protein_pair, compound_pair, z_mask.unsqueeze(-1)))\n",
    "    y = y + model_og.dropout(model_og.triangle_self_attention_list[i_module](y, z_mask))\n",
    "    y = model_og.tranistion(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = model_og.linear(y).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = b[z_mask]\n",
    "y_pred = y_pred.sigmoid() * 10   # normalize to 0 to 10.\n",
    "pair_energy = (model_og.gate_linear(y).sigmoid() * model_og.linear_energy(y)).squeeze(-1) * z_mask\n",
    "affinity_pred = model_og.leaky(model_og.bias + ((pair_energy).sum(axis=(-1, -2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0011, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(y_pred_2-y_pred).sum() # montre qu'on doit faire attention aux biais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0981e-05, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(affinity_pred_2-affinity_pred).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_og.to(\"cpu\")\n",
    "y_pred_2, affinity_pred_2 = model_og(batch.to(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 40.00 MiB. GPU \u0001 has a total capacity of 15.74 GiB of which 27.44 MiB is free. Process 23209 has 1.10 GiB memory in use. Including non-PyTorch memory, this process has 14.61 GiB memory in use. Of the allocated memory 14.34 GiB is allocated by PyTorch, and 129.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[228], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_og\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m y_pred_2, affinity_pred_2 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_og\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m model_og\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/fs/gpfs41/lv11/fileset01/pool/pool-marsot/.conda/envs/tankbind_py38_h100/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs/gpfs41/lv11/fileset01/pool/pool-marsot/.conda/envs/tankbind_py38_h100/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/fs/pool/pool-marsot/tankbind_philip/TankBind/tankbind/model.py:388\u001b[0m, in \u001b[0;36mIaBNet_with_affinity.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    386\u001b[0m             z \u001b[38;5;241m=\u001b[39m z \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprotein_to_compound_list[i_module](z, protein_pair, compound_pair, z_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n\u001b[1;32m    387\u001b[0m             z \u001b[38;5;241m=\u001b[39m z \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtriangle_self_attention_list[i_module](z, z_mask))\n\u001b[0;32m--> 388\u001b[0m             z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranistion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;66;03m# batch_dim = z.shape[0]\u001b[39;00m\n\u001b[1;32m    391\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(z)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/fs/gpfs41/lv11/fileset01/pool/pool-marsot/.conda/envs/tankbind_py38_h100/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs/gpfs41/lv11/fileset01/pool/pool-marsot/.conda/envs/tankbind_py38_h100/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/fs/pool/pool-marsot/tankbind_philip/TankBind/tankbind/model.py:273\u001b[0m, in \u001b[0;36mTransition.forward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, z):\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;66;03m# z of shape b, i, j, embedding_channels, where i is protein dim, j is compound dim.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm(z)\n\u001b[0;32m--> 273\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2((\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mrelu())\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m z\n",
      "File \u001b[0;32m/fs/gpfs41/lv11/fileset01/pool/pool-marsot/.conda/envs/tankbind_py38_h100/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs/gpfs41/lv11/fileset01/pool/pool-marsot/.conda/envs/tankbind_py38_h100/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/fs/gpfs41/lv11/fileset01/pool/pool-marsot/.conda/envs/tankbind_py38_h100/lib/python3.8/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 40.00 MiB. GPU \u0001 has a total capacity of 15.74 GiB of which 27.44 MiB is free. Process 23209 has 1.10 GiB memory in use. Including non-PyTorch memory, this process has 14.61 GiB memory in use. Of the allocated memory 14.34 GiB is allocated by PyTorch, and 129.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "model_og.to(\"cuda:1\")\n",
    "y_pred_2, affinity_pred_2 = model_og(batch.to(\"cuda:1\"))\n",
    "model_og.to(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original TankBind forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_2 = (data['protein']['node_s'], data['protein']['node_v'])\n",
    "edges_2 = (data[(\"protein\", \"p2p\", \"protein\")][\"edge_s\"], data[(\"protein\", \"p2p\", \"protein\")][\"edge_v\"])\n",
    "protein_batch_2 = data['protein'].batch\n",
    "protein_out_2 = model_og.conv_protein(nodes_2, data[(\"protein\", \"p2p\", \"protein\")][\"edge_index\"], edges_2, data.seq)\n",
    "\n",
    "compound_x_2 = data['compound'].x.float()\n",
    "compound_edge_index_2 = data[(\"compound\", \"c2c\", \"compound\")].edge_index.T\n",
    "compound_edge_feature_2 = data[(\"compound\", \"c2c\", \"compound\")].edge_attr\n",
    "edge_weight_2 = data[(\"compound\", \"c2c\", \"compound\")].edge_weight\n",
    "compound_batch_2 = data['compound'].batch\n",
    "compound_out_2 = model_og.conv_compound(compound_edge_index_2, edge_weight_2, compound_edge_feature_2, compound_x_2.shape[0], compound_x_2)['node_feature']\n",
    "\n",
    "protein_out_batched_2, protein_out_mask_2 = to_dense_batch(protein_out_2, protein_batch_2)\n",
    "compound_out_batched_2, compound_out_mask_2 = to_dense_batch(compound_out_2, compound_batch_2)\n",
    "\n",
    "node_xyz_2 = data.node_xyz\n",
    "\n",
    "p_coords_batched_2, p_coords_mask_2 = to_dense_batch(node_xyz_2, protein_batch_2)\n",
    "\n",
    "protein_pair_2 = get_pair_dis_one_hot(p_coords_batched_2, bin_size=2, bin_min=-1, bin_max=model_og.protein_bin_max)\n",
    "compound_pair_batched_2, compound_pair_batched_mask_2 = to_dense_batch(data.compound_pair, data.compound_pair_batch)\n",
    "batch_n_2 = compound_pair_batched_2.shape[0]\n",
    "max_compound_size_square_2 = compound_pair_batched_2.shape[1]\n",
    "max_compound_size_2 = int(max_compound_size_square_2**0.5)\n",
    "assert (max_compound_size_2**2 - max_compound_size_square_2)**2 < 1e-4\n",
    "compound_pair_2 = torch.zeros((batch_n_2, max_compound_size_2, max_compound_size_2, 16)).to(data.compound_pair.device)\n",
    "for i in range(batch_n_2):\n",
    "    one_2 = compound_pair_batched_2[i]\n",
    "    compound_size_square_2 = (data.compound_pair_batch == i).sum()\n",
    "    compound_size_2 = int(compound_size_square_2**0.5)\n",
    "    compound_pair_2[i, :compound_size_2, :compound_size_2] = one_2[:compound_size_square_2].reshape(\n",
    "                                                            (compound_size_2, compound_size_2, -1))\n",
    "\n",
    "protein_pair_2 = model_og.protein_pair_embedding(protein_pair_2.float())\n",
    "compound_pair_2 = model_og.compound_pair_embedding(compound_pair_2.float())\n",
    "\n",
    "protein_out_batched_2 = model_og.layernorm(protein_out_batched_2)\n",
    "compound_out_batched_2 = model_og.layernorm(compound_out_batched_2)\n",
    "\n",
    "z_2 = torch.einsum(\"bik,bjk->bijk\", protein_out_batched_2, compound_out_batched_2)\n",
    "z_mask_2 = torch.einsum(\"bi,bj->bij\", protein_out_mask_2, compound_out_mask_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(compound_pair-compound_pair_2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(protein_pair-protein_pair_2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(protein_batched-protein_out_batched_2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(compound_batched-compound_out_batched).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(z_2-z).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_mask.eq(z_mask_2).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(z-z_2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_module_2 in range(model_og.n_trigonometry_module_stack):\n",
    "    z_2 = z_2 + model_og.dropout(model_og.protein_to_compound_list[i_module_2](z_2, protein_pair_2, compound_pair_2, z_mask_2.unsqueeze(-1)))\n",
    "    z_2 = z_2 + model_og.dropout(model_og.triangle_self_attention_list[i_module_2](z_2, z_mask_2))\n",
    "    z_2 = model_og.tranistion(z_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y and z_2 are equal.\n",
      "protein_pair and protein_pair_2 are NOT equal.\n",
      "compound_pair and compound_pair_2 are equal.\n",
      "z_mask and z_mask_2 are equal.\n",
      "Output y and Output z_2 are NOT equal.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming the relevant variables are already defined and initialized:\n",
    "# y, protein_pair, compound_pair, z_mask, z_2, protein_pair_2, compound_pair_2, z_mask_2\n",
    "\n",
    "# Define a function to check if two variables are equal\n",
    "def check_equality(var1, var2, var_name):\n",
    "    if torch.equal(var1, var2):\n",
    "        print(f\"{var_name} are equal.\")\n",
    "    else:\n",
    "        print(f\"{var_name} are NOT equal.\")\n",
    "\n",
    "# Check the input variables\n",
    "y = z.clone()\n",
    "check_equality(y, z_2, \"y and z_2\")\n",
    "check_equality(protein_pair, protein_pair_2, \"protein_pair and protein_pair_2\")\n",
    "check_equality(compound_pair, compound_pair_2, \"compound_pair and compound_pair_2\")\n",
    "check_equality(z_mask, z_mask_2, \"z_mask and z_mask_2\")\n",
    "\n",
    "# Run the first code block\n",
    "for i_module in range(model_og.n_trigonometry_module_stack):\n",
    "    y = y + model_og.dropout(model_og.protein_to_compound_list[i_module](y, protein_pair, compound_pair, z_mask.unsqueeze(-1)))\n",
    "    y = y + model_og.dropout(model_og.triangle_self_attention_list[i_module](y, z_mask))\n",
    "    y = model_og.tranistion(y)\n",
    "\n",
    "# Save the result of the first code block\n",
    "output_y = y.clone()\n",
    "\n",
    "# Run the second code block\n",
    "for i_module_2 in range(model_og.n_trigonometry_module_stack):\n",
    "    z_2 = z_2 + model_og.dropout(model_og.protein_to_compound_list[i_module_2](z_2, protein_pair_2, compound_pair_2, z_mask_2.unsqueeze(-1)))\n",
    "    z_2 = z_2 + model_og.dropout(model_og.triangle_self_attention_list[i_module_2](z_2, z_mask_2))\n",
    "    z_2 = model_og.tranistion(z_2)\n",
    "\n",
    "# Save the result of the second code block\n",
    "output_z_2 = z_2.clone()\n",
    "\n",
    "# Check the outputs\n",
    "check_equality(output_y, output_z_2, \"Output y and Output z_2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(protein_pair-protein_pair_2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(231392.3594, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(z_2-y).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "b_2 = model_og.linear(z_2).squeeze(-1)\n",
    "y_pred_2 = b_2[z_mask_2]\n",
    "y_pred_2 = y_pred_2.sigmoid() * 10   # normalize to 0 to 10.\n",
    "\n",
    "pair_energy_2 = (model_og.gate_linear(z_2).sigmoid() * model_og.linear_energy(z_2)).squeeze(-1) * z_mask_2\n",
    "affinity_pred_2 = model_og.leaky(model_og.bias + ((pair_energy_2).sum(axis=(-1, -2))))\n",
    "return y_pred_2, affinity_pred_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 July\n",
    "- Test fast attention net\n",
    "- Modify lightning training\n",
    "- Test original model with 3000 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/fs/gpfs41/lv11/fileset01/pool/pool-marsot/tankbind_philip\n"
     ]
    }
   ],
   "source": [
    "%cd /fs/pool/pool-marsot/tankbind_philip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import IaBNet_with_affinity\n",
    "from torch_geometric.loader.dataloader import Collater\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "import torch\n",
    "\n",
    "\n",
    "class TankBindDataLoader(torch.utils.data.DataLoader):\n",
    "    \"\"\"Subclass of the torch DataLoader, in order to apply the collate function TankBindCollater.\"\"\"\n",
    "    def __init__(self,\n",
    "                 dataset,\n",
    "                 batch_size=1,\n",
    "                 shuffle=False,\n",
    "                 follow_batch=None,\n",
    "                 exclude_keys=None,\n",
    "                 make_divisible_by_8=True,\n",
    "                 **kwargs):\n",
    "        self.follow_batch = follow_batch\n",
    "        self.exclude_keys = exclude_keys\n",
    "        self.make_divisible_by_8=make_divisible_by_8\n",
    "        super().__init__(dataset,\n",
    "                         batch_size,\n",
    "                         shuffle,\n",
    "                         collate_fn=TankBindCollater(dataset, follow_batch, exclude_keys, make_divisible_by_8=self.make_divisible_by_8),\n",
    "                         **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "class TankBindCollater(Collater):\n",
    "    \"\"\"Applies batching operations and computations of masks in place of the model, in order to avoid having to recompute it in the\n",
    "    forward pass on GPU.\"\"\"\n",
    "    def __init__(self, dataset,\n",
    "                 follow_batch=None,\n",
    "                 exclude_keys=None,\n",
    "                 make_divisible_by_8=True):\n",
    "        super().__init__(dataset, follow_batch, exclude_keys)\n",
    "        self.make_divisible_by_8 = make_divisible_by_8\n",
    "    def __call__(self, batch):\n",
    "        data = super().__call__(batch)\n",
    "        if self.make_divisible_by_8:\n",
    "            max_dim_divisible_by_8_protein = 8 * (torch.diff(data[\"protein\"].ptr).max() // 8 + 1)\n",
    "            max_dim_divisible_by_8_compound = 8 * (torch.diff(data[\"compound\"].ptr).max() // 8 + 1)\n",
    "        else:\n",
    "            max_dim_divisible_by_8_protein = torch.diff(data[\"protein\"].ptr).max()\n",
    "            max_dim_divisible_by_8_compound = torch.diff(data[\"compound\"].ptr).max()\n",
    "        protein_coordinates_batched, _ = to_dense_batch(\n",
    "            data.node_xyz, data[\"protein\"].batch,\n",
    "            max_num_nodes=max_dim_divisible_by_8_protein,\n",
    "            )\n",
    "        protein_pairwise_representation = get_pair_dis_index(\n",
    "            protein_coordinates_batched,\n",
    "            bin_size=2,\n",
    "            bin_min=-1,\n",
    "            bin_max=protein_bin_max,\n",
    "            ) # shape [batch_n, max_protein_size, max_protein_size, 16]\n",
    "        _compound_lengths = (data[\"compound\"].ptr[1:] - data[\"compound\"].ptr[:-1]) ** 2\n",
    "        _total = torch.cumsum(_compound_lengths, 0)\n",
    "        compound_pairwise_distance_batch = torch.zeros(\n",
    "                _total[-1], dtype=torch.long\n",
    "            )\n",
    "        for i in range(len(_total) - 1):\n",
    "            compound_pairwise_distance_batch[_total[i] : _total[i + 1]] = i + 1\n",
    "        compound_pair_batched, compound_pair_batched_mask = to_dense_batch(\n",
    "            data.compound_pair,\n",
    "            data.compound_pair_batch,\n",
    "            )\n",
    "        compound_pairwise_representation = torch.zeros(\n",
    "            (len(batch), max_dim_divisible_by_8_compound, max_dim_divisible_by_8_compound, 16),\n",
    "            dtype=torch.float32,\n",
    "            )\n",
    "        for i in range(len(batch)):\n",
    "            one = compound_pair_batched[i]\n",
    "            compound_size_square = (compound_pairwise_distance_batch == i).sum()\n",
    "            compound_size = int(compound_size_square**0.5)\n",
    "            compound_pairwise_representation[i, :compound_size, :compound_size] = one[\n",
    "                :compound_size_square\n",
    "                ].reshape((compound_size, compound_size, -1))\n",
    "        data.batch_n = len(batch)\n",
    "        data.max_dim_divisible_by_8_protein = max_dim_divisible_by_8_protein\n",
    "        data.max_dim_divisible_by_8_compound = max_dim_divisible_by_8_compound\n",
    "        data[\"protein\", \"p2p\", \"protein\"].pairwise_representation = protein_pairwise_representation\n",
    "        data[\"compound\", \"p2p\", \"compound\"].pairwise_representation = compound_pairwise_representation\n",
    "        data[\"compound\", \"p2p\", \"compound\"].pairwise_representation_mask = compound_pair_batched_mask\n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_pair_dis_index(d, bin_size=2, bin_min=-1, bin_max=30):\n",
    "    \"\"\"\n",
    "    Computing pairwise distances and binning.\n",
    "    \"\"\"\n",
    "    pair_dis = torch.cdist(d, d, compute_mode='donot_use_mm_for_euclid_dist')\n",
    "    pair_dis[pair_dis>bin_max] = bin_max\n",
    "    pair_dis_bin_index = torch.div(pair_dis - bin_min, bin_size, rounding_mode='floor').long()\n",
    "    return pair_dis_bin_index\n",
    "\n",
    "protein_bin_max = 30\n",
    "\n",
    "train_loader_3 = TankBindDataLoader(train, batch_size=4, follow_batch=['x', 'compound_pair'], make_divisible_by_8=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch import nn\n",
    "from torch.nn import Linear\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "from gvp import GVP, GVPConvLayer, LayerNorm, tuple_index\n",
    "from torch.distributions import Categorical\n",
    "from torch_scatter import scatter_mean\n",
    "#from GATv2 import GAT\n",
    "from GINv2 import GIN\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.conv2 = SAGEConv((-1, -1), out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class GVP_embedding(nn.Module):\n",
    "    '''\n",
    "    Modified based on https://github.com/drorlab/gvp-pytorch/blob/main/gvp/models.py\n",
    "    GVP-GNN for Model Quality Assessment as described in manuscript.\n",
    "    \n",
    "    Takes in protein structure graphs of type `torch_geometric.data.Data` \n",
    "    or `torch_geometric.data.Batch` and returns a scalar score for\n",
    "    each graph in the batch in a `torch.Tensor` of shape [n_nodes]\n",
    "    \n",
    "    Should be used with `gvp.data.ProteinGraphDataset`, or with generators\n",
    "    of `torch_geometric.data.Batch` objects with the same attributes.\n",
    "    \n",
    "    :param node_in_dim: node dimensions in input graph, should be\n",
    "                        (6, 3) if using original features\n",
    "    :param node_h_dim: node dimensions to use in GVP-GNN layers\n",
    "    :param node_in_dim: edge dimensions in input graph, should be\n",
    "                        (32, 1) if using original features\n",
    "    :param edge_h_dim: edge dimensions to embed to before use\n",
    "                       in GVP-GNN layers\n",
    "    :seq_in: if `True`, sequences will also be passed in with\n",
    "             the forward pass; otherwise, sequence information\n",
    "             is assumed to be part of input node embeddings\n",
    "    :param num_layers: number of GVP-GNN layers\n",
    "    :param drop_rate: rate to use in all dropout layers\n",
    "    '''\n",
    "    def __init__(self, node_in_dim, node_h_dim, \n",
    "                 edge_in_dim, edge_h_dim,\n",
    "                 seq_in=False, num_layers=3, drop_rate=0.1):\n",
    "\n",
    "        super(GVP_embedding, self).__init__()\n",
    "        \n",
    "        if seq_in:\n",
    "            self.W_s = nn.Embedding(20, 20)\n",
    "            node_in_dim = (node_in_dim[0] + 20, node_in_dim[1])\n",
    "        \n",
    "        self.W_v = nn.Sequential(\n",
    "            LayerNorm(node_in_dim),\n",
    "            GVP(node_in_dim, node_h_dim, activations=(None, None))\n",
    "        )\n",
    "        self.W_e = nn.Sequential(\n",
    "            LayerNorm(edge_in_dim),\n",
    "            GVP(edge_in_dim, edge_h_dim, activations=(None, None))\n",
    "        )\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "                GVPConvLayer(node_h_dim, edge_h_dim, drop_rate=drop_rate) \n",
    "            for _ in range(num_layers))\n",
    "        \n",
    "        ns, _ = node_h_dim\n",
    "        self.W_out = nn.Sequential(\n",
    "            LayerNorm(node_h_dim),\n",
    "            GVP(node_h_dim, (ns, 0)))\n",
    "\n",
    "    def forward(self, h_V, edge_index, h_E, seq):      \n",
    "        '''\n",
    "        :param h_V: tuple (s, V) of node embeddings\n",
    "        :param edge_index: `torch.Tensor` of shape [2, num_edges]\n",
    "        :param h_E: tuple (s, V) of edge embeddings\n",
    "        :param seq: if not `None`, int `torch.Tensor` of shape [num_nodes]\n",
    "                    to be embedded and appended to `h_V`\n",
    "        '''\n",
    "        seq = self.W_s(seq)\n",
    "        h_V = (torch.cat([h_V[0], seq], dim=-1), h_V[1])\n",
    "        h_V = self.W_v(h_V)\n",
    "        h_E = self.W_e(h_E)\n",
    "        for layer in self.layers:\n",
    "            h_V = layer(h_V, edge_index, h_E)\n",
    "        out = self.W_out(h_V)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def get_pair_dis_one_hot(d, bin_size=2, bin_min=-1, bin_max=30):\n",
    "    # without compute_mode='donot_use_mm_for_euclid_dist' could lead to wrong result.\n",
    "    pair_dis = torch.cdist(d, d, compute_mode='donot_use_mm_for_euclid_dist')\n",
    "    pair_dis[pair_dis>bin_max] = bin_max\n",
    "    pair_dis_bin_index = torch.div(pair_dis - bin_min, bin_size, rounding_mode='floor').long()\n",
    "    pair_dis_one_hot = torch.nn.functional.one_hot(pair_dis_bin_index, num_classes=16)\n",
    "    return pair_dis_one_hot\n",
    "\n",
    "class TriangleProteinToCompound(torch.nn.Module):\n",
    "    def __init__(self, embedding_channels=256, c=128, hasgate=True):\n",
    "        super().__init__()\n",
    "        self.layernorm = torch.nn.LayerNorm(embedding_channels)\n",
    "        self.layernorm_c = torch.nn.LayerNorm(c)\n",
    "        self.hasgate = hasgate\n",
    "        if hasgate:\n",
    "            self.gate_linear = Linear(embedding_channels, c)\n",
    "        self.linear = Linear(embedding_channels, c)\n",
    "        self.ending_gate_linear = Linear(embedding_channels, embedding_channels)\n",
    "        self.linear_after_sum = Linear(c, embedding_channels)\n",
    "    def forward(self, z, protein_pair, compound_pair, z_mask):\n",
    "        # z of shape b, i, j, embedding_channels, where i is protein dim, j is compound dim.\n",
    "        # z_mask of shape b, i, j, 1\n",
    "        z = self.layernorm(z)\n",
    "        if self.hasgate:\n",
    "            ab = self.gate_linear(z).sigmoid() * self.linear(z) * z_mask\n",
    "        else:\n",
    "            ab = self.linear(z) * z_mask\n",
    "        g = self.ending_gate_linear(z).sigmoid()\n",
    "        block1 = torch.einsum(\"bikc,bkjc->bijc\", protein_pair, ab)\n",
    "        block2 = torch.einsum(\"bikc,bjkc->bijc\", ab, compound_pair)\n",
    "        z = g * self.linear_after_sum(self.layernorm_c(block1+block2)) * z_mask\n",
    "        return z\n",
    "\n",
    "class TriangleProteinToCompound_v2(torch.nn.Module):\n",
    "    # separate left/right edges (block1/block2).\n",
    "    def __init__(self, embedding_channels=256, c=128):\n",
    "        super().__init__()\n",
    "        self.layernorm = torch.nn.LayerNorm(embedding_channels, bias=False)\n",
    "        self.layernorm_c = torch.nn.LayerNorm(c, bias=False)\n",
    "\n",
    "        # self.gate_linear1 = Linear(embedding_channels, c)\n",
    "        # self.gate_linear2 = Linear(embedding_channels, c)\n",
    "        # modification by Enzo to remove biases. (hypothesis: biases make the outputs dependent on padding)\n",
    "        self.gate_linear1 = Linear(embedding_channels, c, bias=False)\n",
    "        self.gate_linear2 = Linear(embedding_channels, c, bias=False)\n",
    "\n",
    "        self.linear1 = Linear(embedding_channels, c)\n",
    "        self.linear2 = Linear(embedding_channels, c)\n",
    "\n",
    "        self.ending_gate_linear = Linear(embedding_channels, embedding_channels)\n",
    "        self.linear_after_sum = Linear(c, embedding_channels)\n",
    "    def forward(self, z, protein_pair, compound_pair, z_mask):\n",
    "        # z of shape b, i, j, embedding_channels, where i is protein dim, j is compound dim.\n",
    "        z = self.layernorm(z)\n",
    "        protein_pair = self.layernorm(protein_pair)\n",
    "        compound_pair = self.layernorm(compound_pair)\n",
    " \n",
    "        ab1 = self.gate_linear1(z).sigmoid() * self.linear1(z) * z_mask\n",
    "        ab2 = self.gate_linear2(z).sigmoid() * self.linear2(z) * z_mask\n",
    "        protein_pair = self.gate_linear2(protein_pair).sigmoid() * self.linear2(protein_pair)\n",
    "        compound_pair = self.gate_linear1(compound_pair).sigmoid() * self.linear1(compound_pair)\n",
    "\n",
    "        g = self.ending_gate_linear(z).sigmoid()\n",
    "        block1 = torch.einsum(\"bikc,bkjc->bijc\", protein_pair, ab1)\n",
    "        block2 = torch.einsum(\"bikc,bjkc->bijc\", ab2, compound_pair)\n",
    "        # print(g.shape, block1.shape, block2.shape)\n",
    "        z = g * self.linear_after_sum(self.layernorm_c(block1+block2)) * z_mask\n",
    "        return z\n",
    "\n",
    "# class Self_Attention(nn.Module):\n",
    "#     def __init__(self, hidden_size,num_attention_heads=8,drop_rate=0.5):\n",
    "#         super().__init__()\n",
    "#         self.num_attention_heads = num_attention_heads\n",
    "#         self.attention_head_size = int(hidden_size / num_attention_heads)\n",
    "#         self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "#         self.dp = nn.Dropout(drop_rate)\n",
    "#         self.ln = nn.LayerNorm(hidden_size)\n",
    "\n",
    "#     def transpose_for_scores(self, x):\n",
    "#         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "#         x = x.view(*new_x_shape)\n",
    "#         return x.permute(0, 2, 1, 3)\n",
    "\n",
    "#     def forward(self,q,k,v,attention_mask=None,attention_weight=None):\n",
    "#         q = self.transpose_for_scores(q)\n",
    "#         k = self.transpose_for_scores(k)\n",
    "#         v = self.transpose_for_scores(v)\n",
    "#         attention_scores = torch.matmul(q, k.transpose(-1, -2))\n",
    "\n",
    "#         attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "#         # attention_probs = self.dp(attention_probs)\n",
    "#         if attention_weight is not None:\n",
    "#             attention_weight_sorted_sorted = torch.argsort(torch.argsort(-attention_weight,axis=-1),axis=-1)\n",
    "#             # if self.training:\n",
    "#             #     top_mask = (attention_weight_sorted_sorted<np.random.randint(28,45))\n",
    "#             # else:\n",
    "#             top_mask = (attention_weight_sorted_sorted<32)\n",
    "#             attention_probs = attention_probs * top_mask\n",
    "#             # attention_probs = attention_probs * attention_weight\n",
    "#             attention_probs = attention_probs / (torch.sum(attention_probs,dim=-1,keepdim=True) + 1e-5)\n",
    "#         # print(attention_probs.shape,v.shape)\n",
    "#         # attention_probs = self.dp(attention_probs)\n",
    "#         outputs = torch.matmul(attention_probs, v)\n",
    "\n",
    "#         outputs = outputs.permute(0, 2, 1, 3).contiguous()\n",
    "#         new_output_shape = outputs.size()[:-2] + (self.all_head_size,)\n",
    "#         outputs = outputs.view(*new_output_shape)\n",
    "#         outputs = self.ln(outputs)\n",
    "#         return outputs\n",
    "\n",
    "\n",
    "class FastTriangleSelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_channels, num_attention_heads):\n",
    "        super().__init__()\n",
    "        self.layernorm = nn.LayerNorm(embedding_channels, bias=False)\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.attention_head_size = embedding_channels // num_attention_heads\n",
    "        self.linear_qkv = nn.Linear(embedding_channels, 3*embedding_channels, bias=False)\n",
    "        self.output_linear = nn.Linear(embedding_channels, embedding_channels)\n",
    "        self.g = nn.Linear(embedding_channels, embedding_channels)\n",
    "    def forward(self, z, z_mask_attention_float, z_mask):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        z: torch.Tensor of shape [batch, n_protein, n_compound, embedding_channels]\n",
    "        z_mask: torch.Tensor of shape [batch*n_protein*num_attention_heads, n_compound, n_compound] saying which coefficients\n",
    "            correspond to actual data. (we take this weird shape because scaled_dot_product_attention\n",
    "            requires it). We take it to be float(\"-inf\") where we want to mask.\n",
    "        Returns\n",
    "        -------\n",
    "        \"\"\"\n",
    "        z = self.layernorm(z)\n",
    "        batch_size, n_protein, n_compound, embedding_channels = z.shape\n",
    "        z = z.reshape(batch_size*n_protein, n_compound, embedding_channels)\n",
    "        q, k, v = self.linear_qkv(z).chunk(3, dim=-1)\n",
    "        q = q.view(batch_size*n_protein, n_compound, self.num_attention_heads, self.attention_head_size).contiguous()\n",
    "        k = k.view(batch_size*n_protein, n_compound, self.num_attention_heads, self.attention_head_size).contiguous()\n",
    "        v = v.view(batch_size*n_protein, n_compound, self.num_attention_heads, self.attention_head_size).contiguous()\n",
    "        attention_coefficients = xops.memory_efficient_attention(query=q,\n",
    "                                                key=k,\n",
    "                                                value=v,\n",
    "                                                attn_bias=z_mask_attention_float.to(\"cuda:0\")) # shape [batch*protein_nodes, compound_nodes, n_heads, embedding//n_heads]        \n",
    "\n",
    "        attention_output = attention_coefficients.view(batch_size, n_protein, n_compound, embedding_channels)\n",
    "        g = self.g(z).sigmoid()\n",
    "        output = g * attention_output.view(batch_size*n_protein, n_compound, embedding_channels)\n",
    "\n",
    "        output = self.output_linear(output.view(batch_size, n_protein, n_compound, embedding_channels))*z_mask.unsqueeze(-1).to('cuda:0')\n",
    "        return output\n",
    "\n",
    "class TriangleSelfAttentionRowWise(torch.nn.Module):\n",
    "    # use the protein-compound matrix only.\n",
    "    def __init__(self, embedding_channels=128, c=32, num_attention_heads=4):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.attention_head_size = c\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        # self.dp = nn.Dropout(drop_rate)\n",
    "        # self.ln = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        self.layernorm = torch.nn.LayerNorm(embedding_channels)\n",
    "        # self.layernorm_c = torch.nn.LayerNorm(c)\n",
    "\n",
    "        self.linear_q = Linear(embedding_channels, self.all_head_size, bias=False)\n",
    "        self.linear_k = Linear(embedding_channels, self.all_head_size, bias=False)\n",
    "        self.linear_v = Linear(embedding_channels, self.all_head_size, bias=False)\n",
    "        # self.b = Linear(embedding_channels, h, bias=False)\n",
    "        self.g = Linear(embedding_channels, self.all_head_size)\n",
    "        self.final_linear = Linear(self.all_head_size, embedding_channels)\n",
    "\n",
    "    def reshape_last_dim(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x\n",
    "\n",
    "    def forward(self, z, z_mask):\n",
    "        # z of shape b, i, j, embedding_channels, where i is protein dim, j is compound dim.\n",
    "        # z_mask of shape b, i, j\n",
    "        z = self.layernorm(z)\n",
    "        p_length = z.shape[1]\n",
    "        batch_n = z.shape[0]\n",
    "        # new_z = torch.zeros(z.shape, device=z.device)\n",
    "        z_i = z\n",
    "        z_mask_i = z_mask.view((batch_n, p_length, 1, 1, -1))\n",
    "        attention_mask_i = (1e9 * (z_mask_i.float() - 1.))\n",
    "        # q, k, v of shape b, j, h, c\n",
    "        q = self.reshape_last_dim(self.linear_q(z_i)) #  * (self.attention_head_size**(-0.5))\n",
    "        k = self.reshape_last_dim(self.linear_k(z_i))\n",
    "        v = self.reshape_last_dim(self.linear_v(z_i))\n",
    "        logits = torch.einsum('biqhc,bikhc->bihqk', q, k) + attention_mask_i\n",
    "        weights = nn.Softmax(dim=-1)(logits)\n",
    "        # weights of shape b, h, j, j\n",
    "        # attention_probs = self.dp(attention_probs)\n",
    "        weighted_avg = torch.einsum('bihqk,bikhc->biqhc', weights, v)\n",
    "        g = self.reshape_last_dim(self.g(z_i)).sigmoid()\n",
    "        output = g * weighted_avg\n",
    "        new_output_shape = output.size()[:-2] + (self.all_head_size,)\n",
    "        output = output.view(*new_output_shape)\n",
    "        # output of shape b, j, embedding.\n",
    "        # z[:, i] = output\n",
    "        z = output\n",
    "        # print(g.shape, block1.shape, block2.shape)\n",
    "        z = self.final_linear(z) * z_mask.unsqueeze(-1)\n",
    "        return z\n",
    "\n",
    "\n",
    "class Transition(torch.nn.Module):\n",
    "    # separate left/right edges (block1/block2).\n",
    "    def __init__(self, embedding_channels=256, n=4):\n",
    "        super().__init__()\n",
    "        self.layernorm = torch.nn.LayerNorm(embedding_channels)\n",
    "        self.linear1 = Linear(embedding_channels, n*embedding_channels)\n",
    "        self.linear2 = Linear(n*embedding_channels, embedding_channels)\n",
    "    def forward(self, z):\n",
    "        # z of shape b, i, j, embedding_channels, where i is protein dim, j is compound dim.\n",
    "        z = self.layernorm(z)\n",
    "        z = self.linear2((self.linear1(z)).relu())\n",
    "        return z\n",
    "\n",
    "\n",
    "\n",
    "class IaBNet_with_affinity(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels=128, embedding_channels=128, c=128, fast_attention=True, mode=0, protein_embed_mode=1, compound_embed_mode=1, n_trigonometry_module_stack=5, protein_bin_max=30, readout_mode=2):\n",
    "        super().__init__()\n",
    "        self.layernorm = torch.nn.LayerNorm(embedding_channels)\n",
    "        self.protein_bin_max = protein_bin_max\n",
    "        self.mode = mode\n",
    "        self.protein_embed_mode = protein_embed_mode\n",
    "        self.compound_embed_mode = compound_embed_mode\n",
    "        self.n_trigonometry_module_stack = n_trigonometry_module_stack\n",
    "        self.readout_mode = readout_mode\n",
    "        # Added by Enzo\n",
    "        self.fast_attention = fast_attention\n",
    "        if protein_embed_mode == 0:\n",
    "            self.conv_protein = GNN(hidden_channels, embedding_channels)\n",
    "            self.conv_compound = GNN(hidden_channels, embedding_channels)\n",
    "            # self.conv_protein = SAGEConv((-1, -1), embedding_channels)\n",
    "            # self.conv_compound = SAGEConv((-1, -1), embedding_channels)\n",
    "        if protein_embed_mode == 1:\n",
    "            self.conv_protein = GVP_embedding((6, 3), (embedding_channels, 16), \n",
    "                                              (32, 1), (32, 1), seq_in=True)\n",
    "            \n",
    "\n",
    "        if compound_embed_mode == 0:\n",
    "            self.conv_compound = GNN(hidden_channels, embedding_channels)\n",
    "        elif compound_embed_mode == 1:\n",
    "            self.conv_compound = GIN(input_dim = 56, hidden_dims = [128,56,embedding_channels], edge_input_dim = 19, concat_hidden = False)\n",
    "\n",
    "        if mode == 0:\n",
    "            self.protein_pair_embedding = nn.Embedding(16, c)\n",
    "            self.compound_pair_embedding = Linear(16, c)\n",
    "            self.protein_to_compound_list = []\n",
    "            self.protein_to_compound_list = nn.ModuleList([TriangleProteinToCompound_v2(embedding_channels=embedding_channels, c=c) for _ in range(n_trigonometry_module_stack)])\n",
    "            if fast_attention:\n",
    "                self.triangle_self_attention_list = nn.ModuleList([FastTriangleSelfAttention(embedding_channels=embedding_channels, num_attention_heads=4) for _ in range(n_trigonometry_module_stack)])\n",
    "            else:\n",
    "                self.triangle_self_attention_list = nn.ModuleList([TriangleSelfAttentionRowWise(embedding_channels=embedding_channels) for _ in range(n_trigonometry_module_stack)])\n",
    "            self.tranistion = Transition(embedding_channels=embedding_channels, n=4)\n",
    "\n",
    "        self.linear = Linear(embedding_channels, 1)\n",
    "        self.linear_energy = Linear(embedding_channels, 1)\n",
    "        if readout_mode == 2:\n",
    "            self.gate_linear = Linear(embedding_channels, 1)\n",
    "        # self.gate_linear = Linear(embedding_channels, 1)\n",
    "        self.bias = torch.nn.Parameter(torch.ones(1))\n",
    "        self.leaky = torch.nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout2d(p=0.25)\n",
    "    def forward(self, data):\n",
    "        # Added by Enzo\n",
    "        max_dim_divisible_by_8_protein = data.max_dim_divisible_by_8_protein\n",
    "        max_dim_divisible_by_8_compound = data.max_dim_divisible_by_8_compound\n",
    "        if self.protein_embed_mode == 0:\n",
    "            x = data['protein'].x.float()\n",
    "            edge_index = data[(\"protein\", \"p2p\", \"protein\")].edge_index\n",
    "            protein_batch = data['protein'].batch\n",
    "            protein_out = self.conv_protein(x, edge_index)\n",
    "        if self.protein_embed_mode == 1:\n",
    "            nodes = (data['protein']['node_s'], data['protein']['node_v'])\n",
    "            edges = (data[(\"protein\", \"p2p\", \"protein\")][\"edge_s\"], data[(\"protein\", \"p2p\", \"protein\")][\"edge_v\"])\n",
    "            protein_batch = data['protein'].batch\n",
    "            protein_out = self.conv_protein(nodes, data[(\"protein\", \"p2p\", \"protein\")][\"edge_index\"], edges, data.seq)\n",
    "\n",
    "        if self.compound_embed_mode == 0:\n",
    "            compound_x = data['compound'].x.float()\n",
    "            compound_edge_index = data[(\"compound\", \"c2c\", \"compound\")].edge_index\n",
    "            compound_batch = data['compound'].batch\n",
    "            compound_out = self.conv_compound(compound_x, compound_edge_index)\n",
    "        elif self.compound_embed_mode == 1:\n",
    "            compound_x = data['compound'].x.float()\n",
    "            compound_edge_index = data[(\"compound\", \"c2c\", \"compound\")].edge_index.T\n",
    "            # compound_edge_index = data[(\"compound\", \"c2c\", \"compound\")].edge_index\n",
    "            compound_edge_feature = data[(\"compound\", \"c2c\", \"compound\")].edge_attr\n",
    "            edge_weight = data[(\"compound\", \"c2c\", \"compound\")].edge_weight\n",
    "            compound_batch = data['compound'].batch\n",
    "            # Enzo : print dimensions\n",
    "            #print(f\"{compound_edge_index.shape=}, {edge_weight.shape=}, {compound_edge_feature.shape=}, {compound_x.shape=}\")\n",
    "            compound_out = self.conv_compound(compound_edge_index,edge_weight,compound_edge_feature,compound_x.shape[0],compound_x)['node_feature']\n",
    "    \n",
    "        # protein_batch version could further process b matrix. better than for loop.\n",
    "        # protein_out_batched of shape b, n, c\n",
    "        protein_out_batched, protein_out_mask = to_dense_batch(protein_out, protein_batch, max_num_nodes=max_dim_divisible_by_8_protein)\n",
    "        compound_out_batched, compound_out_mask = to_dense_batch(compound_out, compound_batch, max_num_nodes=max_dim_divisible_by_8_compound)\n",
    "        batch_n = data.batch_n\n",
    "        z_mask = torch.einsum(\"bi,bj->bij\", protein_out_mask, compound_out_mask)\n",
    "        z_mask_attention = torch.einsum(\"bik, bq-> biqk\", z_mask, compound_out_mask).reshape(batch_n*protein_out_batched.shape[1], max_dim_divisible_by_8_compound, max_dim_divisible_by_8_compound).unsqueeze(1).expand(-1, self.n_heads, -1, -1).contiguous()\n",
    "        z_mask_attention = torch.where(z_mask_attention, 0.0, -10.0**6)\n",
    "        z_mask_flat = torch.arange(\n",
    "            start=0, end=z_mask.numel(), device=self.device\n",
    "        ).view(z_mask.shape)[z_mask]\n",
    "        protein_square_mask = torch.einsum(\"bi,bj->bij\", protein_out_mask, protein_out_mask)\n",
    "        node_xyz = data.node_xyz\n",
    "\n",
    "        p_coords_batched, p_coords_mask = to_dense_batch(node_xyz, protein_batch)\n",
    "        # c_coords_batched, c_coords_mask = to_dense_batch(coords, compound_batch)\n",
    "\n",
    "        protein_pair = data[\"protein\", \"p2p\", \"protein\"].pairwise_representation\n",
    "        \n",
    "        # compound_pair = get_pair_dis_one_hot(c_coords_batched, bin_size=1, bin_min=-0.5, bin_max=15)\n",
    "        compound_pair_batched, compound_pair_batched_mask = data[\"compound\", \"p2p\", \"compound\"].pairwise_representation, data[\"compound\", \"p2p\", \"compound\"].pairwise_representation_mask\n",
    "\n",
    "        batch_n = compound_pair_batched.shape[0]\n",
    "        # max_compound_size_square = compound_pair_batched.shape[1]\n",
    "        # max_compound_size = int(max_compound_size_square**0.5)\n",
    "        # assert (max_compound_size**2 - max_compound_size_square)**2 < 1e-4\n",
    "        # compound_pair = torch.zeros((batch_n, max_compound_size, max_compound_size, 16)).to(data.compound_pair.device)\n",
    "        # for i in range(batch_n):\n",
    "        #     one = compound_pair_batched[i]\n",
    "        #     compound_size_square = (data.compound_pair_batch == i).sum()\n",
    "        #     compound_size = int(compound_size_square**0.5)\n",
    "        #     compound_pair[i,:compound_size, :compound_size] = one[:compound_size_square].reshape(\n",
    "        #                                                         (compound_size, compound_size, -1))\n",
    "        protein_pair = self.protein_pair_embedding(protein_pair)\n",
    "        compound_pair = self.compound_pair_embedding(data[\"compound\", \"p2p\", \"compound\"].pairwise_representation.float())\n",
    "        # b = torch.einsum(\"bik,bjk->bij\", protein_out_batched, compound_out_batched).flatten()\n",
    "\n",
    "        protein_out_batched = self.layernorm(protein_out_batched)\n",
    "        compound_out_batched = self.layernorm(compound_out_batched)\n",
    "        # z of shape, b, protein_length, compound_length, channels.\n",
    "        z = torch.einsum(\"bik,bjk->bijk\", protein_out_batched, compound_out_batched)\n",
    "        # z_mask = torch.einsum(\"bi,bj->bij\", protein_out_mask, compound_out_mask)\n",
    "        # z = z * z_mask.unsqueeze(-1)\n",
    "        # print(protein_pair.shape, compound_pair.shape, b.shape)\n",
    "        if self.mode == 0:\n",
    "            for _ in range(1):\n",
    "                for i_module in range(self.n_trigonometry_module_stack):\n",
    "                    z = z + self.dropout(self.protein_to_compound_list[i_module](z, protein_pair, compound_pair, z_mask.unsqueeze(-1)))\n",
    "                    if self.fast_attention:\n",
    "                        z = z + self.dropout(self.triangle_self_attention_list[i_module](z, z_mask_attention, z_mask))\n",
    "                    else:\n",
    "                        z = z + self.dropout(self.triangle_self_attention_list[i_module](z, z_mask))\n",
    "                    z = self.tranistion(z)\n",
    "        # batch_dim = z.shape[0]\n",
    "\n",
    "        b = self.linear(z).squeeze(-1)\n",
    "        y_pred = b.flatten()[z_mask_flat]\n",
    "        y_pred = y_pred.sigmoid() * 10   # normalize to 0 to 10.\n",
    "        if self.readout_mode == 0:\n",
    "            pair_energy = self.linear_energy(z).squeeze(-1) * z_mask\n",
    "            affinity_pred = self.leaky(self.bias + ((pair_energy).sum(axis=(-1, -2))))\n",
    "        if self.readout_mode == 1:\n",
    "            # valid_interaction_z = (z * z_mask.unsqueeze(-1)).mean(axis=(1, 2))\n",
    "            valid_interaction_z = (z * z_mask.unsqueeze(-1)).sum(axis=(1, 2)) / z_mask.sum(axis=(1, 2)).unsqueeze(-1)\n",
    "            affinity_pred = self.linear_energy(valid_interaction_z).squeeze(-1)\n",
    "            # print(\"z shape\", z.shape, \"z_mask shape\", z_mask.shape,   \"valid_interaction_z shape\", valid_interaction_z.shape, \"affinity_pred shape\", affinity_pred.shape)\n",
    "        if self.readout_mode == 2:\n",
    "            pair_energy = (self.gate_linear(z).sigmoid() * self.linear_energy(z)).squeeze(-1) * z_mask\n",
    "            affinity_pred = self.leaky(self.bias + ((pair_energy).sum(axis=(-1, -2))))\n",
    "        return y_pred, affinity_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
