{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /fs/pool/pool-marsot/tankbind_philip/TankBind/tankbind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"/fs/pool/pool-marsot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import TankBindDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TankBindDataSet(\"/fs/pool/pool-marsot/pdbbind/pdbbind2020/dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import TankBindDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch import nn\n",
    "from torch.nn import Linear\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "from gvp import GVP, GVPConvLayer, LayerNorm, tuple_index\n",
    "from torch.distributions import Categorical\n",
    "from torch_scatter import scatter_mean\n",
    "#from GATv2 import GAT\n",
    "from GINv2 import GIN\n",
    "import xformers.ops as xops\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.conv2 = SAGEConv((-1, -1), out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class GVP_embedding(nn.Module):\n",
    "    '''\n",
    "    Modified based on https://github.com/drorlab/gvp-pytorch/blob/main/gvp/models.py\n",
    "    GVP-GNN for Model Quality Assessment as described in manuscript.\n",
    "    \n",
    "    Takes in protein structure graphs of type `torch_geometric.data.Data` \n",
    "    or `torch_geometric.data.Batch` and returns a scalar score for\n",
    "    each graph in the batch in a `torch.Tensor` of shape [n_nodes]\n",
    "    \n",
    "    Should be used with `gvp.data.ProteinGraphDataset`, or with generators\n",
    "    of `torch_geometric.data.Batch` objects with the same attributes.\n",
    "    \n",
    "    :param node_in_dim: node dimensions in input graph, should be\n",
    "                        (6, 3) if using original features\n",
    "    :param node_h_dim: node dimensions to use in GVP-GNN layers\n",
    "    :param node_in_dim: edge dimensions in input graph, should be\n",
    "                        (32, 1) if using original features\n",
    "    :param edge_h_dim: edge dimensions to embed to before use\n",
    "                       in GVP-GNN layers\n",
    "    :seq_in: if `True`, sequences will also be passed in with\n",
    "             the forward pass; otherwise, sequence information\n",
    "             is assumed to be part of input node embeddings\n",
    "    :param num_layers: number of GVP-GNN layers\n",
    "    :param drop_rate: rate to use in all dropout layers\n",
    "    '''\n",
    "    def __init__(self, node_in_dim, node_h_dim, \n",
    "                 edge_in_dim, edge_h_dim,\n",
    "                 seq_in=False, num_layers=3, drop_rate=0.1):\n",
    "\n",
    "        super(GVP_embedding, self).__init__()\n",
    "        \n",
    "        if seq_in:\n",
    "            self.W_s = nn.Embedding(20, 20)\n",
    "            node_in_dim = (node_in_dim[0] + 20, node_in_dim[1])\n",
    "        \n",
    "        self.W_v = nn.Sequential(\n",
    "            LayerNorm(node_in_dim),\n",
    "            GVP(node_in_dim, node_h_dim, activations=(None, None))\n",
    "        )\n",
    "        self.W_e = nn.Sequential(\n",
    "            LayerNorm(edge_in_dim),\n",
    "            GVP(edge_in_dim, edge_h_dim, activations=(None, None))\n",
    "        )\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "                GVPConvLayer(node_h_dim, edge_h_dim, drop_rate=drop_rate) \n",
    "            for _ in range(num_layers))\n",
    "        \n",
    "        ns, _ = node_h_dim\n",
    "        self.W_out = nn.Sequential(\n",
    "            LayerNorm(node_h_dim),\n",
    "            GVP(node_h_dim, (ns, 0)))\n",
    "\n",
    "    def forward(self, h_V, edge_index, h_E, seq):      \n",
    "        '''\n",
    "        :param h_V: tuple (s, V) of node embeddings\n",
    "        :param edge_index: `torch.Tensor` of shape [2, num_edges]\n",
    "        :param h_E: tuple (s, V) of edge embeddings\n",
    "        :param seq: if not `None`, int `torch.Tensor` of shape [num_nodes]\n",
    "                    to be embedded and appended to `h_V`\n",
    "        '''\n",
    "        seq = self.W_s(seq)\n",
    "        h_V = (torch.cat([h_V[0], seq], dim=-1), h_V[1])\n",
    "        h_V = self.W_v(h_V)\n",
    "        h_E = self.W_e(h_E)\n",
    "        for layer in self.layers:\n",
    "            h_V = layer(h_V, edge_index, h_E)\n",
    "        out = self.W_out(h_V)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def get_pair_dis_one_hot(d, bin_size=2, bin_min=-1, bin_max=30):\n",
    "    # without compute_mode='donot_use_mm_for_euclid_dist' could lead to wrong result.\n",
    "    pair_dis = torch.cdist(d, d, compute_mode='donot_use_mm_for_euclid_dist')\n",
    "    pair_dis[pair_dis>bin_max] = bin_max\n",
    "    pair_dis_bin_index = torch.div(pair_dis - bin_min, bin_size, rounding_mode='floor').long()\n",
    "    pair_dis_one_hot = torch.nn.functional.one_hot(pair_dis_bin_index, num_classes=16)\n",
    "    return pair_dis_one_hot\n",
    "\n",
    "class TriangleProteinToCompound(torch.nn.Module):\n",
    "    def __init__(self, embedding_channels=256, c=128, hasgate=True):\n",
    "        super().__init__()\n",
    "        self.layernorm = torch.nn.LayerNorm(embedding_channels)\n",
    "        self.layernorm_c = torch.nn.LayerNorm(c)\n",
    "        self.hasgate = hasgate\n",
    "        if hasgate:\n",
    "            self.gate_linear = Linear(embedding_channels, c)\n",
    "        self.linear = Linear(embedding_channels, c)\n",
    "        self.ending_gate_linear = Linear(embedding_channels, embedding_channels)\n",
    "        self.linear_after_sum = Linear(c, embedding_channels)\n",
    "    def forward(self, z, protein_pair, compound_pair, z_mask):\n",
    "        # z of shape b, i, j, embedding_channels, where i is protein dim, j is compound dim.\n",
    "        # z_mask of shape b, i, j, 1\n",
    "        z = self.layernorm(z)\n",
    "        if self.hasgate:\n",
    "            ab = self.gate_linear(z).sigmoid() * self.linear(z) * z_mask\n",
    "        else:\n",
    "            ab = self.linear(z) * z_mask\n",
    "        g = self.ending_gate_linear(z).sigmoid()\n",
    "        block1 = torch.einsum(\"bikc,bkjc->bijc\", protein_pair, ab)\n",
    "        block2 = torch.einsum(\"bikc,bjkc->bijc\", ab, compound_pair)\n",
    "        z = g * self.linear_after_sum(self.layernorm_c(block1+block2)) * z_mask\n",
    "        return z\n",
    "\n",
    "class TriangleProteinToCompound_v2(torch.nn.Module):\n",
    "    # separate left/right edges (block1/block2).\n",
    "    def __init__(self, embedding_channels=256, c=128):\n",
    "        super().__init__()\n",
    "        self.layernorm = torch.nn.LayerNorm(embedding_channels, bias=False)\n",
    "        self.layernorm_c = torch.nn.LayerNorm(c, bias=False)\n",
    "\n",
    "        # self.gate_linear1 = Linear(embedding_channels, c)\n",
    "        # self.gate_linear2 = Linear(embedding_channels, c)\n",
    "        # modification by Enzo to remove biases. (hypothesis: biases make the outputs dependent on padding)\n",
    "        self.gate_linear1 = Linear(embedding_channels, c, bias=False)\n",
    "        self.gate_linear2 = Linear(embedding_channels, c, bias=False)\n",
    "\n",
    "        self.linear1 = Linear(embedding_channels, c)\n",
    "        self.linear2 = Linear(embedding_channels, c)\n",
    "\n",
    "        self.ending_gate_linear = Linear(embedding_channels, embedding_channels)\n",
    "        self.linear_after_sum = Linear(c, embedding_channels)\n",
    "    def forward(self, z, protein_pair, compound_pair, z_mask):\n",
    "        # z of shape b, i, j, embedding_channels, where i is protein dim, j is compound dim.\n",
    "        z = self.layernorm(z)\n",
    "        protein_pair = self.layernorm(protein_pair)\n",
    "        compound_pair = self.layernorm(compound_pair)\n",
    " \n",
    "        ab1 = self.gate_linear1(z).sigmoid() * self.linear1(z) * z_mask\n",
    "        ab2 = self.gate_linear2(z).sigmoid() * self.linear2(z) * z_mask\n",
    "        protein_pair = self.gate_linear2(protein_pair).sigmoid() * self.linear2(protein_pair)\n",
    "        compound_pair = self.gate_linear1(compound_pair).sigmoid() * self.linear1(compound_pair)\n",
    "\n",
    "        g = self.ending_gate_linear(z).sigmoid()\n",
    "        block1 = torch.einsum(\"bikc,bkjc->bijc\", protein_pair, ab1)\n",
    "        block2 = torch.einsum(\"bikc,bjkc->bijc\", ab2, compound_pair)\n",
    "        # print(g.shape, block1.shape, block2.shape)\n",
    "        z = g * self.linear_after_sum(self.layernorm_c(block1+block2)) * z_mask\n",
    "        return z\n",
    "\n",
    "# class Self_Attention(nn.Module):\n",
    "#     def __init__(self, hidden_size,num_attention_heads=8,drop_rate=0.5):\n",
    "#         super().__init__()\n",
    "#         self.num_attention_heads = num_attention_heads\n",
    "#         self.attention_head_size = int(hidden_size / num_attention_heads)\n",
    "#         self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "#         self.dp = nn.Dropout(drop_rate)\n",
    "#         self.ln = nn.LayerNorm(hidden_size)\n",
    "\n",
    "#     def transpose_for_scores(self, x):\n",
    "#         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "#         x = x.view(*new_x_shape)\n",
    "#         return x.permute(0, 2, 1, 3)\n",
    "\n",
    "#     def forward(self,q,k,v,attention_mask=None,attention_weight=None):\n",
    "#         q = self.transpose_for_scores(q)\n",
    "#         k = self.transpose_for_scores(k)\n",
    "#         v = self.transpose_for_scores(v)\n",
    "#         attention_scores = torch.matmul(q, k.transpose(-1, -2))\n",
    "\n",
    "#         attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "#         # attention_probs = self.dp(attention_probs)\n",
    "#         if attention_weight is not None:\n",
    "#             attention_weight_sorted_sorted = torch.argsort(torch.argsort(-attention_weight,axis=-1),axis=-1)\n",
    "#             # if self.training:\n",
    "#             #     top_mask = (attention_weight_sorted_sorted<np.random.randint(28,45))\n",
    "#             # else:\n",
    "#             top_mask = (attention_weight_sorted_sorted<32)\n",
    "#             attention_probs = attention_probs * top_mask\n",
    "#             # attention_probs = attention_probs * attention_weight\n",
    "#             attention_probs = attention_probs / (torch.sum(attention_probs,dim=-1,keepdim=True) + 1e-5)\n",
    "#         # print(attention_probs.shape,v.shape)\n",
    "#         # attention_probs = self.dp(attention_probs)\n",
    "#         outputs = torch.matmul(attention_probs, v)\n",
    "\n",
    "#         outputs = outputs.permute(0, 2, 1, 3).contiguous()\n",
    "#         new_output_shape = outputs.size()[:-2] + (self.all_head_size,)\n",
    "#         outputs = outputs.view(*new_output_shape)\n",
    "#         outputs = self.ln(outputs)\n",
    "#         return outputs\n",
    "\n",
    "\n",
    "class FastTriangleSelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_channels, num_attention_heads):\n",
    "        super().__init__()\n",
    "        self.layernorm = nn.LayerNorm(embedding_channels, bias=False)\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.attention_head_size = embedding_channels // num_attention_heads\n",
    "        self.linear_qkv = nn.Linear(embedding_channels, 3*embedding_channels, bias=False)\n",
    "        self.output_linear = nn.Linear(embedding_channels, embedding_channels)\n",
    "        self.g = nn.Linear(embedding_channels, embedding_channels)\n",
    "    def forward(self, z, z_mask_attention_float, z_mask):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        z: torch.Tensor of shape [batch, n_protein, n_compound, embedding_channels]\n",
    "        z_mask: torch.Tensor of shape [batch*n_protein*num_attention_heads, n_compound, n_compound] saying which coefficients\n",
    "            correspond to actual data. (we take this weird shape because scaled_dot_product_attention\n",
    "            requires it). We take it to be float(\"-inf\") where we want to mask.\n",
    "        Returns\n",
    "        -------\n",
    "        \"\"\"\n",
    "        z = self.layernorm(z)\n",
    "        batch_size, n_protein, n_compound, embedding_channels = z.shape\n",
    "        z = z.reshape(batch_size*n_protein, n_compound, embedding_channels)\n",
    "        q, k, v = self.linear_qkv(z).chunk(3, dim=-1)\n",
    "        q = q.view(batch_size*n_protein, n_compound, self.num_attention_heads, self.attention_head_size).contiguous()\n",
    "        k = k.view(batch_size*n_protein, n_compound, self.num_attention_heads, self.attention_head_size).contiguous()\n",
    "        v = v.view(batch_size*n_protein, n_compound, self.num_attention_heads, self.attention_head_size).contiguous()\n",
    "        attention_coefficients = xops.memory_efficient_attention(query=q,\n",
    "                                                key=k,\n",
    "                                                value=v,\n",
    "                                                attn_bias=z_mask_attention_float.to(\"cuda:0\")) # shape [batch*protein_nodes, compound_nodes, n_heads, embedding//n_heads]        \n",
    "\n",
    "        attention_output = attention_coefficients.view(batch_size, n_protein, n_compound, embedding_channels)\n",
    "        g = self.g(z).sigmoid()\n",
    "        output = g * attention_output.view(batch_size*n_protein, n_compound, embedding_channels)\n",
    "\n",
    "        output = self.output_linear(output.view(batch_size, n_protein, n_compound, embedding_channels))*z_mask.unsqueeze(-1).to('cuda:0')\n",
    "        return output\n",
    "\n",
    "class TriangleSelfAttentionRowWise(torch.nn.Module):\n",
    "    # use the protein-compound matrix only.\n",
    "    def __init__(self, embedding_channels=128, c=32, num_attention_heads=4):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.attention_head_size = c\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        # self.dp = nn.Dropout(drop_rate)\n",
    "        # self.ln = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        self.layernorm = torch.nn.LayerNorm(embedding_channels)\n",
    "        # self.layernorm_c = torch.nn.LayerNorm(c)\n",
    "\n",
    "        self.linear_q = Linear(embedding_channels, self.all_head_size, bias=False)\n",
    "        self.linear_k = Linear(embedding_channels, self.all_head_size, bias=False)\n",
    "        self.linear_v = Linear(embedding_channels, self.all_head_size, bias=False)\n",
    "        # self.b = Linear(embedding_channels, h, bias=False)\n",
    "        self.g = Linear(embedding_channels, self.all_head_size)\n",
    "        self.final_linear = Linear(self.all_head_size, embedding_channels)\n",
    "\n",
    "    def reshape_last_dim(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x\n",
    "\n",
    "    def forward(self, z, z_mask):\n",
    "        # z of shape b, i, j, embedding_channels, where i is protein dim, j is compound dim.\n",
    "        # z_mask of shape b, i, j\n",
    "        z = self.layernorm(z)\n",
    "        p_length = z.shape[1]\n",
    "        batch_n = z.shape[0]\n",
    "        # new_z = torch.zeros(z.shape, device=z.device)\n",
    "        z_i = z\n",
    "        z_mask_i = z_mask.view((batch_n, p_length, 1, 1, -1))\n",
    "        attention_mask_i = (1e9 * (z_mask_i.float() - 1.))\n",
    "        # q, k, v of shape b, j, h, c\n",
    "        q = self.reshape_last_dim(self.linear_q(z_i)) #  * (self.attention_head_size**(-0.5))\n",
    "        k = self.reshape_last_dim(self.linear_k(z_i))\n",
    "        v = self.reshape_last_dim(self.linear_v(z_i))\n",
    "        logits = torch.einsum('biqhc,bikhc->bihqk', q, k) + attention_mask_i\n",
    "        weights = nn.Softmax(dim=-1)(logits)\n",
    "        # weights of shape b, h, j, j\n",
    "        # attention_probs = self.dp(attention_probs)\n",
    "        weighted_avg = torch.einsum('bihqk,bikhc->biqhc', weights, v)\n",
    "        g = self.reshape_last_dim(self.g(z_i)).sigmoid()\n",
    "        output = g * weighted_avg\n",
    "        new_output_shape = output.size()[:-2] + (self.all_head_size,)\n",
    "        output = output.view(*new_output_shape)\n",
    "        # output of shape b, j, embedding.\n",
    "        # z[:, i] = output\n",
    "        z = output\n",
    "        # print(g.shape, block1.shape, block2.shape)\n",
    "        z = self.final_linear(z) * z_mask.unsqueeze(-1)\n",
    "        return z\n",
    "\n",
    "\n",
    "class Transition(torch.nn.Module):\n",
    "    # separate left/right edges (block1/block2).\n",
    "    def __init__(self, embedding_channels=256, n=4):\n",
    "        super().__init__()\n",
    "        self.layernorm = torch.nn.LayerNorm(embedding_channels)\n",
    "        self.linear1 = Linear(embedding_channels, n*embedding_channels)\n",
    "        self.linear2 = Linear(n*embedding_channels, embedding_channels)\n",
    "    def forward(self, z):\n",
    "        # z of shape b, i, j, embedding_channels, where i is protein dim, j is compound dim.\n",
    "        z = self.layernorm(z)\n",
    "        z = self.linear2((self.linear1(z)).relu())\n",
    "        return z\n",
    "\n",
    "\n",
    "\n",
    "class IaBNet_with_affinity(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels=128, embedding_channels=128, c=128, fast_attention=True, mode=0, protein_embed_mode=1, compound_embed_mode=1, n_trigonometry_module_stack=5, protein_bin_max=30, readout_mode=2):\n",
    "        super().__init__()\n",
    "        self.layernorm = torch.nn.LayerNorm(embedding_channels)\n",
    "        self.protein_bin_max = protein_bin_max\n",
    "        self.mode = mode\n",
    "        self.protein_embed_mode = protein_embed_mode\n",
    "        self.compound_embed_mode = compound_embed_mode\n",
    "        self.n_trigonometry_module_stack = n_trigonometry_module_stack\n",
    "        self.readout_mode = readout_mode\n",
    "        self.n_heads=4\n",
    "        # Added by Enzo\n",
    "        self.fast_attention = fast_attention\n",
    "        if protein_embed_mode == 0:\n",
    "            self.conv_protein = GNN(hidden_channels, embedding_channels)\n",
    "            self.conv_compound = GNN(hidden_channels, embedding_channels)\n",
    "            # self.conv_protein = SAGEConv((-1, -1), embedding_channels)\n",
    "            # self.conv_compound = SAGEConv((-1, -1), embedding_channels)\n",
    "        if protein_embed_mode == 1:\n",
    "            self.conv_protein = GVP_embedding((6, 3), (embedding_channels, 16), \n",
    "                                              (32, 1), (32, 1), seq_in=True)\n",
    "            \n",
    "\n",
    "        if compound_embed_mode == 0:\n",
    "            self.conv_compound = GNN(hidden_channels, embedding_channels)\n",
    "        elif compound_embed_mode == 1:\n",
    "            self.conv_compound = GIN(input_dim = 56, hidden_dims = [128,56,embedding_channels], edge_input_dim = 19, concat_hidden = False)\n",
    "\n",
    "        if mode == 0:\n",
    "            self.protein_pair_embedding = nn.Embedding(16, c)\n",
    "            self.compound_pair_embedding = nn.Linear(16, c)\n",
    "            self.protein_to_compound_list = []\n",
    "            self.protein_to_compound_list = nn.ModuleList([TriangleProteinToCompound_v2(embedding_channels=embedding_channels, c=c) for _ in range(n_trigonometry_module_stack)])\n",
    "            if fast_attention:\n",
    "                self.triangle_self_attention_list = nn.ModuleList([FastTriangleSelfAttention(embedding_channels=embedding_channels, num_attention_heads=4) for _ in range(n_trigonometry_module_stack)])\n",
    "            else:\n",
    "                self.triangle_self_attention_list = nn.ModuleList([TriangleSelfAttentionRowWise(embedding_channels=embedding_channels) for _ in range(n_trigonometry_module_stack)])\n",
    "            self.tranistion = Transition(embedding_channels=embedding_channels, n=4)\n",
    "\n",
    "        self.linear = Linear(embedding_channels, 1)\n",
    "        self.linear_energy = Linear(embedding_channels, 1)\n",
    "        if readout_mode == 2:\n",
    "            self.gate_linear = Linear(embedding_channels, 1)\n",
    "        # self.gate_linear = Linear(embedding_channels, 1)\n",
    "        self.bias = torch.nn.Parameter(torch.ones(1))\n",
    "        self.leaky = torch.nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout2d(p=0.25)\n",
    "    def forward(self, data):\n",
    "        # Added by Enzo\n",
    "        max_dim_divisible_by_8_protein = data.max_dim_divisible_by_8_protein\n",
    "        max_dim_divisible_by_8_compound = data.max_dim_divisible_by_8_compound\n",
    "        if self.protein_embed_mode == 0:\n",
    "            x = data['protein'].x.float()\n",
    "            edge_index = data[(\"protein\", \"p2p\", \"protein\")].edge_index\n",
    "            protein_batch = data['protein'].batch\n",
    "            protein_out = self.conv_protein(x, edge_index)\n",
    "        if self.protein_embed_mode == 1:\n",
    "            nodes = (data['protein']['node_s'], data['protein']['node_v'])\n",
    "            edges = (data[(\"protein\", \"p2p\", \"protein\")][\"edge_s\"], data[(\"protein\", \"p2p\", \"protein\")][\"edge_v\"])\n",
    "            protein_batch = data['protein'].batch\n",
    "            protein_out = self.conv_protein(nodes, data[(\"protein\", \"p2p\", \"protein\")][\"edge_index\"], edges, data.seq)\n",
    "\n",
    "        if self.compound_embed_mode == 0:\n",
    "            compound_x = data['compound'].x.float()\n",
    "            compound_edge_index = data[(\"compound\", \"c2c\", \"compound\")].edge_index\n",
    "            compound_batch = data['compound'].batch\n",
    "            compound_out = self.conv_compound(compound_x, compound_edge_index)\n",
    "        elif self.compound_embed_mode == 1:\n",
    "            compound_x = data['compound'].x.float()\n",
    "            compound_edge_index = data[(\"compound\", \"c2c\", \"compound\")].edge_index.T\n",
    "            # compound_edge_index = data[(\"compound\", \"c2c\", \"compound\")].edge_index\n",
    "            compound_edge_feature = data[(\"compound\", \"c2c\", \"compound\")].edge_attr\n",
    "            edge_weight = data[(\"compound\", \"c2c\", \"compound\")].edge_weight\n",
    "            compound_batch = data['compound'].batch\n",
    "            # Enzo : print dimensions\n",
    "            #print(f\"{compound_edge_index.shape=}, {edge_weight.shape=}, {compound_edge_feature.shape=}, {compound_x.shape=}\")\n",
    "            compound_out = self.conv_compound(compound_edge_index,edge_weight,compound_edge_feature,compound_x.shape[0],compound_x)['node_feature']\n",
    "    \n",
    "        # protein_batch version could further process b matrix. better than for loop.\n",
    "        # protein_out_batched of shape b, n, c\n",
    "        protein_out_batched, protein_out_mask = to_dense_batch(protein_out, protein_batch, max_num_nodes=max_dim_divisible_by_8_protein)\n",
    "        compound_out_batched, compound_out_mask = to_dense_batch(compound_out, compound_batch, max_num_nodes=max_dim_divisible_by_8_compound)\n",
    "        batch_n = data.batch_n\n",
    "        z_mask = torch.einsum(\"bi,bj->bij\", protein_out_mask, compound_out_mask)\n",
    "        z_mask_attention = torch.einsum(\"bik, bq-> biqk\", z_mask, compound_out_mask).reshape(batch_n*protein_out_batched.shape[1], max_dim_divisible_by_8_compound, max_dim_divisible_by_8_compound).unsqueeze(1).expand(-1, self.n_heads, -1, -1).contiguous()\n",
    "        z_mask_attention = torch.where(z_mask_attention, 0.0, -10.0**6)\n",
    "        z_mask_flat = torch.arange(\n",
    "            start=0, end=z_mask.numel(), device=self.device\n",
    "        ).view(z_mask.shape)[z_mask]\n",
    "        protein_square_mask = torch.einsum(\"bi,bj->bij\", protein_out_mask, protein_out_mask)\n",
    "        node_xyz = data.node_xyz\n",
    "\n",
    "        p_coords_batched, p_coords_mask = to_dense_batch(node_xyz, protein_batch)\n",
    "        # c_coords_batched, c_coords_mask = to_dense_batch(coords, compound_batch)\n",
    "\n",
    "        protein_pair = data[\"protein\", \"p2p\", \"protein\"].pairwise_representation\n",
    "        \n",
    "        # compound_pair = get_pair_dis_one_hot(c_coords_batched, bin_size=1, bin_min=-0.5, bin_max=15)\n",
    "        compound_pair_batched, compound_pair_batched_mask = data[\"compound\", \"p2p\", \"compound\"].pairwise_representation, data[\"compound\", \"p2p\", \"compound\"].pairwise_representation_mask\n",
    "\n",
    "        batch_n = compound_pair_batched.shape[0]\n",
    "        # max_compound_size_square = compound_pair_batched.shape[1]\n",
    "        # max_compound_size = int(max_compound_size_square**0.5)\n",
    "        # assert (max_compound_size**2 - max_compound_size_square)**2 < 1e-4\n",
    "        # compound_pair = torch.zeros((batch_n, max_compound_size, max_compound_size, 16)).to(data.compound_pair.device)\n",
    "        # for i in range(batch_n):\n",
    "        #     one = compound_pair_batched[i]\n",
    "        #     compound_size_square = (data.compound_pair_batch == i).sum()\n",
    "        #     compound_size = int(compound_size_square**0.5)\n",
    "        #     compound_pair[i,:compound_size, :compound_size] = one[:compound_size_square].reshape(\n",
    "        #                                                         (compound_size, compound_size, -1))\n",
    "        protein_pair = self.protein_pair_embedding(protein_pair)\n",
    "        compound_pair = self.compound_pair_embedding(data[\"compound\", \"p2p\", \"compound\"].pairwise_representation.float())\n",
    "        # b = torch.einsum(\"bik,bjk->bij\", protein_out_batched, compound_out_batched).flatten()\n",
    "\n",
    "        protein_out_batched = self.layernorm(protein_out_batched)\n",
    "        compound_out_batched = self.layernorm(compound_out_batched)\n",
    "        # z of shape, b, protein_length, compound_length, channels.\n",
    "        z = torch.einsum(\"bik,bjk->bijk\", protein_out_batched, compound_out_batched)\n",
    "        # z_mask = torch.einsum(\"bi,bj->bij\", protein_out_mask, compound_out_mask)\n",
    "        # z = z * z_mask.unsqueeze(-1)\n",
    "        # print(protein_pair.shape, compound_pair.shape, b.shape)\n",
    "        if self.mode == 0:\n",
    "            for _ in range(1):\n",
    "                for i_module in range(self.n_trigonometry_module_stack):\n",
    "                    z = z + self.dropout(self.protein_to_compound_list[i_module](z, protein_pair, compound_pair, z_mask.unsqueeze(-1)))\n",
    "                    if self.fast_attention:\n",
    "                        z = z + self.dropout(self.triangle_self_attention_list[i_module](z, z_mask_attention, z_mask))\n",
    "                    else:\n",
    "                        z = z + self.dropout(self.triangle_self_attention_list[i_module](z, z_mask))\n",
    "                    z = self.tranistion(z)\n",
    "        # batch_dim = z.shape[0]\n",
    "\n",
    "        b = self.linear(z).squeeze(-1)\n",
    "        y_pred = b.flatten()[z_mask_flat]\n",
    "        y_pred = y_pred.sigmoid() * 10   # normalize to 0 to 10.\n",
    "        if self.readout_mode == 0:\n",
    "            pair_energy = self.linear_energy(z).squeeze(-1) * z_mask\n",
    "            affinity_pred = self.leaky(self.bias + ((pair_energy).sum(axis=(-1, -2))))\n",
    "        if self.readout_mode == 1:\n",
    "            # valid_interaction_z = (z * z_mask.unsqueeze(-1)).mean(axis=(1, 2))\n",
    "            valid_interaction_z = (z * z_mask.unsqueeze(-1)).sum(axis=(1, 2)) / z_mask.sum(axis=(1, 2)).unsqueeze(-1)\n",
    "            affinity_pred = self.linear_energy(valid_interaction_z).squeeze(-1)\n",
    "            # print(\"z shape\", z.shape, \"z_mask shape\", z_mask.shape,   \"valid_interaction_z shape\", valid_interaction_z.shape, \"affinity_pred shape\", affinity_pred.shape)\n",
    "        if self.readout_mode == 2:\n",
    "            pair_energy = (self.gate_linear(z).sigmoid() * self.linear_energy(z)).squeeze(-1) * z_mask\n",
    "            affinity_pred = self.leaky(self.bias + ((pair_energy).sum(axis=(-1, -2))))\n",
    "        return y_pred, affinity_pred\n",
    "# Added by Enzo\n",
    "from torch_geometric.loader.dataloader import Collater\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "import torch\n",
    "\n",
    "\n",
    "class TankBindDataLoader(torch.utils.data.DataLoader):\n",
    "    \"\"\"Subclass of the torch DataLoader, in order to apply the collate function TankBindCollater.\"\"\"\n",
    "    def __init__(self,\n",
    "                 dataset,\n",
    "                 batch_size=1,\n",
    "                 shuffle=False,\n",
    "                 follow_batch=None,\n",
    "                 exclude_keys=None,\n",
    "                 make_divisible_by_8=True,\n",
    "                 **kwargs):\n",
    "        self.follow_batch = follow_batch\n",
    "        self.exclude_keys = exclude_keys\n",
    "        self.make_divisible_by_8=make_divisible_by_8\n",
    "        super().__init__(dataset,\n",
    "                         batch_size,\n",
    "                         shuffle,\n",
    "                         collate_fn=TankBindCollater(dataset, follow_batch, exclude_keys, make_divisible_by_8=self.make_divisible_by_8),\n",
    "                         **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "class TankBindCollater(Collater):\n",
    "    \"\"\"Applies batching operations and computations of masks in place of the model, in order to avoid having to recompute it in the\n",
    "    forward pass on GPU.\"\"\"\n",
    "    def __init__(self, dataset,\n",
    "                 follow_batch=None,\n",
    "                 exclude_keys=None,\n",
    "                 make_divisible_by_8=True):\n",
    "        super().__init__(dataset, follow_batch, exclude_keys)\n",
    "        self.make_divisible_by_8 = make_divisible_by_8\n",
    "    def __call__(self, batch):\n",
    "        data = super().__call__(batch)\n",
    "        if self.make_divisible_by_8:\n",
    "            max_dim_divisible_by_8_protein = 8 * (torch.diff(data[\"protein\"].ptr).max() // 8 + 1)\n",
    "            max_dim_divisible_by_8_compound = 8 * (torch.diff(data[\"compound\"].ptr).max() // 8 + 1)\n",
    "        else:\n",
    "            max_dim_divisible_by_8_protein = torch.diff(data[\"protein\"].ptr).max()\n",
    "            max_dim_divisible_by_8_compound = torch.diff(data[\"compound\"].ptr).max()\n",
    "        protein_coordinates_batched, _ = to_dense_batch(\n",
    "            data.node_xyz, data[\"protein\"].batch,\n",
    "            max_num_nodes=max_dim_divisible_by_8_protein,\n",
    "            )\n",
    "        protein_pairwise_representation = get_pair_dis_index(\n",
    "            protein_coordinates_batched,\n",
    "            bin_size=2,\n",
    "            bin_min=-1,\n",
    "            bin_max=protein_bin_max,\n",
    "            ) # shape [batch_n, max_protein_size, max_protein_size, 16]\n",
    "        _compound_lengths = (data[\"compound\"].ptr[1:] - data[\"compound\"].ptr[:-1]) ** 2\n",
    "        _total = torch.cumsum(_compound_lengths, 0)\n",
    "        compound_pairwise_distance_batch = torch.zeros(\n",
    "                _total[-1], dtype=torch.long\n",
    "            )\n",
    "        for i in range(len(_total) - 1):\n",
    "            compound_pairwise_distance_batch[_total[i] : _total[i + 1]] = i + 1\n",
    "        compound_pair_batched, compound_pair_batched_mask = to_dense_batch(\n",
    "            data.compound_pair,\n",
    "            data.compound_pair_batch,\n",
    "            )\n",
    "        compound_pairwise_representation = torch.zeros(\n",
    "            (len(batch), max_dim_divisible_by_8_compound, max_dim_divisible_by_8_compound, 16),\n",
    "            dtype=torch.float32,\n",
    "            )\n",
    "        for i in range(len(batch)):\n",
    "            one = compound_pair_batched[i]\n",
    "            compound_size_square = (compound_pairwise_distance_batch == i).sum()\n",
    "            compound_size = int(compound_size_square**0.5)\n",
    "            compound_pairwise_representation[i, :compound_size, :compound_size] = one[\n",
    "                :compound_size_square\n",
    "                ].reshape((compound_size, compound_size, -1))\n",
    "        data.batch_n = len(batch)\n",
    "        data.max_dim_divisible_by_8_protein = max_dim_divisible_by_8_protein\n",
    "        data.max_dim_divisible_by_8_compound = max_dim_divisible_by_8_compound\n",
    "        data[\"protein\", \"p2p\", \"protein\"].pairwise_representation = protein_pairwise_representation\n",
    "        data[\"compound\", \"p2p\", \"compound\"].pairwise_representation = compound_pairwise_representation\n",
    "        data[\"compound\", \"p2p\", \"compound\"].pairwise_representation_mask = compound_pair_batched_mask\n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_pair_dis_index(d, bin_size=2, bin_min=-1, bin_max=30):\n",
    "    \"\"\"\n",
    "    Computing pairwise distances and binning.\n",
    "    \"\"\"\n",
    "    pair_dis = torch.cdist(d, d, compute_mode='donot_use_mm_for_euclid_dist')\n",
    "    pair_dis[pair_dis>bin_max] = bin_max\n",
    "    pair_dis_bin_index = torch.div(pair_dis - bin_min, bin_size, rounding_mode='floor').long()\n",
    "    return pair_dis_bin_index\n",
    "\n",
    "protein_bin_max = 30\n",
    "\n",
    "\n",
    "def get_model(mode, logging, device):\n",
    "    if mode == 0:\n",
    "        logging.info(\"5 stack, readout2, pred dis map add self attention and GVP embed, compound model GIN\")\n",
    "        model = IaBNet_with_affinity().to(device)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IaBNet_with_affinity(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels=128, embedding_channels=128, c=128, fast_attention=True, mode=0, protein_embed_mode=1, compound_embed_mode=1, n_trigonometry_module_stack=5, protein_bin_max=30, readout_mode=2):\n",
    "        super().__init__()\n",
    "        self.layernorm = torch.nn.LayerNorm(embedding_channels)\n",
    "        self.protein_bin_max = protein_bin_max\n",
    "        self.mode = mode\n",
    "        self.protein_embed_mode = protein_embed_mode\n",
    "        self.compound_embed_mode = compound_embed_mode\n",
    "        self.n_trigonometry_module_stack = n_trigonometry_module_stack\n",
    "        self.readout_mode = readout_mode\n",
    "        self.n_heads=4\n",
    "        # Added by Enzo\n",
    "        self.fast_attention = fast_attention\n",
    "        if protein_embed_mode == 0:\n",
    "            self.conv_protein = GNN(hidden_channels, embedding_channels)\n",
    "            self.conv_compound = GNN(hidden_channels, embedding_channels)\n",
    "            # self.conv_protein = SAGEConv((-1, -1), embedding_channels)\n",
    "            # self.conv_compound = SAGEConv((-1, -1), embedding_channels)\n",
    "        if protein_embed_mode == 1:\n",
    "            self.conv_protein = GVP_embedding((6, 3), (embedding_channels, 16), \n",
    "                                              (32, 1), (32, 1), seq_in=True)\n",
    "            \n",
    "\n",
    "        if compound_embed_mode == 0:\n",
    "            self.conv_compound = GNN(hidden_channels, embedding_channels)\n",
    "        elif compound_embed_mode == 1:\n",
    "            self.conv_compound = GIN(input_dim = 56, hidden_dims = [128,56,embedding_channels], edge_input_dim = 19, concat_hidden = False)\n",
    "\n",
    "        if mode == 0:\n",
    "            self.protein_pair_embedding = nn.Embedding(16, c)\n",
    "            self.compound_pair_embedding = Linear(16, c)\n",
    "            self.protein_to_compound_list = []\n",
    "            self.protein_to_compound_list = nn.ModuleList([TriangleProteinToCompound_v2(embedding_channels=embedding_channels, c=c) for _ in range(n_trigonometry_module_stack)])\n",
    "            if fast_attention:\n",
    "                self.triangle_self_attention_list = nn.ModuleList([FastTriangleSelfAttention(embedding_channels=embedding_channels, num_attention_heads=4) for _ in range(n_trigonometry_module_stack)])\n",
    "            else:\n",
    "                self.triangle_self_attention_list = nn.ModuleList([TriangleSelfAttentionRowWise(embedding_channels=embedding_channels) for _ in range(n_trigonometry_module_stack)])\n",
    "            self.tranistion = Transition(embedding_channels=embedding_channels, n=4)\n",
    "\n",
    "        self.linear = Linear(embedding_channels, 1)\n",
    "        self.linear_energy = Linear(embedding_channels, 1)\n",
    "        if readout_mode == 2:\n",
    "            self.gate_linear = Linear(embedding_channels, 1)\n",
    "        # self.gate_linear = Linear(embedding_channels, 1)\n",
    "        self.bias = torch.nn.Parameter(torch.ones(1))\n",
    "        self.leaky = torch.nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout2d(p=0.25)\n",
    "    def forward(self, data):\n",
    "        # Added by Enzo\n",
    "        max_dim_divisible_by_8_protein = data.max_dim_divisible_by_8_protein\n",
    "        max_dim_divisible_by_8_compound = data.max_dim_divisible_by_8_compound\n",
    "        if self.protein_embed_mode == 0:\n",
    "            x = data['protein'].x.float()\n",
    "            edge_index = data[(\"protein\", \"p2p\", \"protein\")].edge_index\n",
    "            protein_batch = data['protein'].batch\n",
    "            protein_out = self.conv_protein(x, edge_index)\n",
    "        if self.protein_embed_mode == 1:\n",
    "            nodes = (data['protein']['node_s'], data['protein']['node_v'])\n",
    "            edges = (data[(\"protein\", \"p2p\", \"protein\")][\"edge_s\"], data[(\"protein\", \"p2p\", \"protein\")][\"edge_v\"])\n",
    "            protein_batch = data['protein'].batch\n",
    "            protein_out = self.conv_protein(nodes, data[(\"protein\", \"p2p\", \"protein\")][\"edge_index\"], edges, data.seq)\n",
    "\n",
    "        if self.compound_embed_mode == 0:\n",
    "            compound_x = data['compound'].x.float()\n",
    "            compound_edge_index = data[(\"compound\", \"c2c\", \"compound\")].edge_index\n",
    "            compound_batch = data['compound'].batch\n",
    "            compound_out = self.conv_compound(compound_x, compound_edge_index)\n",
    "        elif self.compound_embed_mode == 1:\n",
    "            compound_x = data['compound'].x.float()\n",
    "            compound_edge_index = data[(\"compound\", \"c2c\", \"compound\")].edge_index.T\n",
    "            # compound_edge_index = data[(\"compound\", \"c2c\", \"compound\")].edge_index\n",
    "            compound_edge_feature = data[(\"compound\", \"c2c\", \"compound\")].edge_attr\n",
    "            edge_weight = data[(\"compound\", \"c2c\", \"compound\")].edge_weight\n",
    "            compound_batch = data['compound'].batch\n",
    "            # Enzo : print dimensions\n",
    "            #print(f\"{compound_edge_index.shape=}, {edge_weight.shape=}, {compound_edge_feature.shape=}, {compound_x.shape=}\")\n",
    "            compound_out = self.conv_compound(compound_edge_index,edge_weight,compound_edge_feature,compound_x.shape[0],compound_x)['node_feature']\n",
    "    \n",
    "        # protein_batch version could further process b matrix. better than for loop.\n",
    "        # protein_out_batched of shape b, n, c\n",
    "        protein_out_batched, protein_out_mask = to_dense_batch(protein_out, protein_batch, max_num_nodes=max_dim_divisible_by_8_protein)\n",
    "        compound_out_batched, compound_out_mask = to_dense_batch(compound_out, compound_batch, max_num_nodes=max_dim_divisible_by_8_compound)\n",
    "        batch_n = data.batch_n\n",
    "        z_mask = torch.einsum(\"bi,bj->bij\", protein_out_mask, compound_out_mask)\n",
    "        z_mask_attention = torch.einsum(\"bik, bq-> biqk\", z_mask, compound_out_mask).reshape(batch_n*protein_out_batched.shape[1], max_dim_divisible_by_8_compound, max_dim_divisible_by_8_compound).unsqueeze(1).expand(-1, self.n_heads, -1, -1).contiguous()\n",
    "        z_mask_attention = torch.where(z_mask_attention, 0.0, -10.0**6)\n",
    "        z_mask_flat = torch.arange(\n",
    "            start=0, end=z_mask.numel(), device=self.device\n",
    "        ).view(z_mask.shape)[z_mask]\n",
    "        protein_square_mask = torch.einsum(\"bi,bj->bij\", protein_out_mask, protein_out_mask)\n",
    "        node_xyz = data.node_xyz\n",
    "\n",
    "        p_coords_batched, p_coords_mask = to_dense_batch(node_xyz, protein_batch)\n",
    "        # c_coords_batched, c_coords_mask = to_dense_batch(coords, compound_batch)\n",
    "\n",
    "        protein_pair = data[\"protein\", \"p2p\", \"protein\"].pairwise_representation\n",
    "        \n",
    "        # compound_pair = get_pair_dis_one_hot(c_coords_batched, bin_size=1, bin_min=-0.5, bin_max=15)\n",
    "        compound_pair_batched, compound_pair_batched_mask = data[\"compound\", \"p2p\", \"compound\"].pairwise_representation, data[\"compound\", \"p2p\", \"compound\"].pairwise_representation_mask\n",
    "\n",
    "        batch_n = compound_pair_batched.shape[0]\n",
    "        # max_compound_size_square = compound_pair_batched.shape[1]\n",
    "        # max_compound_size = int(max_compound_size_square**0.5)\n",
    "        # assert (max_compound_size**2 - max_compound_size_square)**2 < 1e-4\n",
    "        # compound_pair = torch.zeros((batch_n, max_compound_size, max_compound_size, 16)).to(data.compound_pair.device)\n",
    "        # for i in range(batch_n):\n",
    "        #     one = compound_pair_batched[i]\n",
    "        #     compound_size_square = (data.compound_pair_batch == i).sum()\n",
    "        #     compound_size = int(compound_size_square**0.5)\n",
    "        #     compound_pair[i,:compound_size, :compound_size] = one[:compound_size_square].reshape(\n",
    "        #                                                         (compound_size, compound_size, -1))\n",
    "        protein_pair = self.protein_pair_embedding(protein_pair)\n",
    "        compound_pair = self.compound_pair_embedding(data[\"compound\", \"p2p\", \"compound\"].pairwise_representation.float())        # b = torch.einsum(\"bik,bjk->bij\", protein_out_batched, compound_out_batched).flatten()\n",
    "\n",
    "        protein_out_batched = self.layernorm(protein_out_batched)\n",
    "        compound_out_batched = self.layernorm(compound_out_batched)\n",
    "        # z of shape, b, protein_length, compound_length, channels.\n",
    "        z = torch.einsum(\"bik,bjk->bijk\", protein_out_batched, compound_out_batched)\n",
    "        # z_mask = torch.einsum(\"bi,bj->bij\", protein_out_mask, compound_out_mask)\n",
    "\n",
    "        # print(protein_pair.shape, compound_pair.shape, b.shape)\n",
    "        if self.mode == 0:\n",
    "            for _ in range(1):\n",
    "                for i_module in range(self.n_trigonometry_module_stack):\n",
    "                    z = z + self.dropout(self.protein_to_compound_list[i_module](z, protein_pair, compound_pair, z_mask.unsqueeze(-1)))\n",
    "                    if self.fast_attention:\n",
    "                        z = z + self.dropout(self.triangle_self_attention_list[i_module](z, z_mask_attention, z_mask))\n",
    "                    else:\n",
    "                        z = z + self.dropout(self.triangle_self_attention_list[i_module](z, z_mask))\n",
    "                    z = self.tranistion(z)\n",
    "        # batch_dim = z.shape[0]\n",
    "\n",
    "        b = self.linear(z).squeeze(-1)\n",
    "        y_pred = b.flatten()[z_mask_flat]\n",
    "        y_pred = y_pred.sigmoid() * 10   # normalize to 0 to 10.\n",
    "        if self.readout_mode == 0:\n",
    "            pair_energy = self.linear_energy(z).squeeze(-1) * z_mask\n",
    "            affinity_pred = self.leaky(self.bias + ((pair_energy).sum(axis=(-1, -2))))\n",
    "        if self.readout_mode == 1:\n",
    "            # valid_interaction_z = (z * z_mask.unsqueeze(-1)).mean(axis=(1, 2))\n",
    "            valid_interaction_z = (z * z_mask.unsqueeze(-1)).sum(axis=(1, 2)) / z_mask.sum(axis=(1, 2)).unsqueeze(-1)\n",
    "            affinity_pred = self.linear_energy(valid_interaction_z).squeeze(-1)\n",
    "            # print(\"z shape\", z.shape, \"z_mask shape\", z_mask.shape,   \"valid_interaction_z shape\", valid_interaction_z.shape, \"affinity_pred shape\", affinity_pred.shape)\n",
    "        if self.readout_mode == 2:\n",
    "            pair_energy = (self.gate_linear(z).sigmoid() * self.linear_energy(z)).squeeze(-1) * z_mask\n",
    "            affinity_pred = self.leaky(self.bias + ((pair_energy).sum(axis=(-1, -2))))\n",
    "        return y_pred, affinity_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = RandomSampler(dataset, replacement=True, num_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = TankBindDataLoader(dataset, batch_size=4, follow_batch=['x', 'compound_pair'], sampler = sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IaBNet_with_affinity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda:0\")\n",
    "batch_cuda = batch.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(batch_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etape importante: \n",
    "- Refaire script lightning+wandb\n",
    "- Verifier que l'evaluation du modele fonctionne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(addNoise=None):\n",
    "    pre = \"./\"\n",
    "    add_noise_to_com = float(addNoise) if addNoise else None\n",
    "\n",
    "    new_dataset = TankBindDataSet(\"/fs/pool/pool-marsot/pdbbind/pdbbind2020/dataset\", add_noise_to_com=add_noise_to_com)\n",
    "    new_dataset.data = new_dataset.data.query(\"c_length < 100 and native_num_contact > 5\").reset_index(drop=True)\n",
    "    d = new_dataset.data\n",
    "    only_native_train_index = d.query(\"use_compound_com and group =='train'\").index.values\n",
    "    train = new_dataset[only_native_train_index]\n",
    "    train_index = d.query(\"group =='train'\").index.values\n",
    "    train_after_warm_up = new_dataset[train_index]\n",
    "    valid_index = d.query(\"use_compound_com and group =='valid'\").index.values\n",
    "    valid = new_dataset[valid_index]\n",
    "    test_index = d.query(\"use_compound_com and group =='test'\").index.values\n",
    "    test = new_dataset[test_index]\n",
    "\n",
    "    all_pocket_test_fileName = \"/fs/pool/pool-marsot/tankbind_philip/TankBind/dataset/test_dataset\"\n",
    "    all_pocket_test = TankBindDataSet(all_pocket_test_fileName)\n",
    "    all_pocket_test.compound_dict = \"/fs/pool/pool-marsot/pdbbind/pdbbind2020/dataset/processed/compound.pt\"\n",
    "    info = None\n",
    "    return train, train_after_warm_up, valid, test, all_pocket_test, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /fs/pool/pool-marsot/tankbind_philip/TankBind/tankbind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"/fs/pool/pool-marsot/\")\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "timestamp = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "run = wandb.init(project=\"TankBind\", name=f\"{timestamp}\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = IaBNet_with_affinity()\n",
    "train, train_after_warm_up, valid, test, all_pocket_test, info = get_data(addNoise=5)\n",
    "sampler = RandomSampler(train, replacement=True, num_samples=20000)\n",
    "train_loader = TankBindDataLoader(train, batch_size=5, follow_batch=['x', 'compound_pair'], sampler = sampler)\n",
    "sampler_2 = RandomSampler(train_after_warm_up, replacement=True, num_samples=20000)\n",
    "train_after_warmup_loader = TankBindDataLoader(train_after_warm_up, batch_size=5, follow_batch=['x', 'compound_pair'], sampler = sampler_2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "criterion = nn.MSELoss()\n",
    "affinity_criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    y_list = []\n",
    "\n",
    "    y_pred_list = []\n",
    "    affinity_list = []\n",
    "    affinity_pred_list = []\n",
    "    batch_loss = 0.0\n",
    "    affinity_batch_loss = 0.0\n",
    "    data_it = tqdm(train_after_warmup_loader)\n",
    "\n",
    "    for data in data_it:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred, affinity_pred = model(data)\n",
    "        y = data.y\n",
    "        affinity = data.affinity\n",
    "        dis_map = data.dis_map\n",
    "        y_pred = y_pred[data.equivalent_native_y_mask]\n",
    "        y = y[data.equivalent_native_y_mask]\n",
    "        dis_map = dis_map[data.equivalent_native_y_mask]\n",
    "\n",
    "\n",
    "        contact_loss = criterion(y_pred, dis_map) if len(dis_map) > 0 else torch.tensor([0]).to(dis_map.device)\n",
    "        y_pred = y_pred.sigmoid()\n",
    "\n",
    "\n",
    "        relative_k = 0.01\n",
    "\n",
    "\n",
    "        native_pocket_mask = data.is_equivalent_native_pocket\n",
    "        affinity_loss =  relative_k * my_affinity_criterion(affinity_pred,\n",
    "                                                            affinity, \n",
    "                                                            native_pocket_mask, decoy_gap=1.0)\n",
    "\n",
    "        loss = contact_loss + affinity_loss\n",
    "        wandb.log({\"contact_loss\":contact_loss.detach(), \"affinity_loss\":affinity_loss.detach(), \"loss\":loss.detach()})\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss += len(y_pred)*contact_loss.item()\n",
    "        affinity_batch_loss += len(affinity_pred)*affinity_loss.item()\n",
    "        # print(f\"{loss.item():.3}\")\n",
    "        y_list.append(y)\n",
    "        y_pred_list.append(y_pred.detach())\n",
    "        affinity_list.append(data.affinity)\n",
    "        affinity_pred_list.append(affinity_pred.detach())\n",
    "        # torch.cuda.empty_cache()\n",
    "\n",
    "    y = torch.cat(y_list)\n",
    "    y_pred = torch.cat(y_pred_list)\n",
    "    if args.pred_dis:\n",
    "        y_pred = torch.clip(1 - (y_pred / 10.0), min=1e-6, max=0.99999)\n",
    "        contact_threshold = 0.2\n",
    "    else:\n",
    "        contact_threshold = 0.5\n",
    "\n",
    "    affinity = torch.cat(affinity_list)\n",
    "    affinity_pred = torch.cat(affinity_pred_list)\n",
    "    metrics = {\"loss\":batch_loss/len(y_pred) + affinity_batch_loss/len(affinity_pred)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /fs/pool/pool-marsot/tankbind_philip_base/tankbind_philip/tankbind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"/fs/pool/pool-marsot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~(val_dataset.data)[\"pdb\"].str.endswith('_c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset.data[\"pdb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset.data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import rdkit.Chem as Chem\n",
    "from bindbind.datasets.processing.ligand_features.tankbind_ligand_features import read_molecule, create_tankbind_ligand_features, get_LAS_distance_constraint_mask\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric\n",
    "from bindbind.datasets.processing.ligand_features.tankbind_ligand_features import read_molecule, create_tankbind_ligand_features, get_LAS_distance_constraint_mask\n",
    "from bindbind.experiments.ablations.regular.metrics.helper import compute_RMSD, write_with_new_coords, generate_sdf_from_smiles_using_rdkit, get_info_pred_distance, simple_custom_description, distribute_function\n",
    "import pandas as pd\n",
    "from tankbind_philip_base.tankbind_philip.tankbind.data import TankBindDataSet\n",
    "from tankbind_philip.TankBind.tankbind.data import TankBindDataLoader\n",
    "def evaluate_model_val(model,\n",
    "                       batch_size=8,\n",
    "                       num_workers=8,\n",
    "                       val_dataset_path=\"/fs/pool/pool-marsot/tankbind_philip/TankBind/dataset/val_dataset\",\n",
    "                       full_dataset_path=\"/fs/pool/pool-marsot/tankbind_philip/TankBind/dataset/dataset\",\n",
    "                       rdkit_folder=\"/fs/pool/pool-marsot/tankbind_philip/TankBind/data/rdkit_folder\",\n",
    "                       renumbered_ligands_folder=\"/fs/pool/pool-marsot/tankbind_philip/TankBind/data/renumber_atom_index_same_as_smiles\",\n",
    "                       recompute=False,\n",
    "                       ):\n",
    "    print(\"hi\")\n",
    "\n",
    "val_dataset_path=\"/fs/pool/pool-marsot/tankbind_philip/TankBind/dataset/val_dataset\"\n",
    "full_dataset_path=\"/fs/pool/pool-marsot/tankbind_philip/TankBind/dataset/dataset\"\n",
    "recompute=False\n",
    "rdkit_folder=\"/fs/pool/pool-marsot/tankbind_philip/TankBind/data/rdkit_folder\"\n",
    "renumbered_ligands_folder=\"/fs/pool/pool-marsot/tankbind_philip/TankBind/data/renumber_atom_index_same_as_smiles\"\n",
    "num_workers=8\n",
    "batch_size=8\n",
    "\n",
    "if not os.path.exists(f\"{val_dataset_path}/processed\"):\n",
    "    dataset = TankBindDataSet(full_dataset_path)\n",
    "    val_data = dataset.data[(dataset.data[\"group\"]==\"valid\") & (~(val_dataset.data)[\"pdb\"].str.endswith('_c'))]\n",
    "    val_names = val_data[\"protein_name\"].unique().tolist()\n",
    "    val_compound_dict = {name:item for (name, item) in dataset.compound_dict.items() if name in val_names}\n",
    "    val_protein_dict = {name:item for (name, item) in dataset.protein_dict.items() if name in val_names}\n",
    "    val_dataset = TankBindDataSet(val_dataset_path, data=val_data,\n",
    "                                    compound_dict=val_compound_dict,\n",
    "                                    protein_dict=val_protein_dict)\n",
    "else:\n",
    "    val_dataset = TankBindDataSet(val_dataset_path)\n",
    "device = model.device\n",
    "model.eval()\n",
    "val = val_dataset.data[\"protein_name\"].unique().tolist()\n",
    "if recompute or not os.path.exists(f\"{rdkit_folder}/compound_dict_based_on_rdkit.pt\"):\n",
    "    compound_dict = {}\n",
    "    print(\"generating compound dictionary\")\n",
    "    for protein_name in tqdm(val):\n",
    "        mol, _ = read_molecule(f\"{renumbered_ligands_folder}/{protein_name}.sdf\", None)\n",
    "        smiles = Chem.MolToSmiles(mol)\n",
    "        rdkit_mol_path = f\"{rdkit_folder}/{protein_name}.sdf\"\n",
    "        generate_sdf_from_smiles_using_rdkit(smiles, rdkit_mol_path, shift_dis=0.0)\n",
    "        mol, _ = read_molecule(rdkit_mol_path, None)\n",
    "        compound_dict[protein_name] = create_tankbind_ligand_features(rdkit_mol_path, None, has_LAS_mask=True)\n",
    "    torch.save(compound_dict, f\"{rdkit_folder}/compound_dict_based_on_rdkit.pt\")\n",
    "else:\n",
    "    compound_dict = torch.load(f\"{rdkit_folder}/compound_dict_based_on_rdkit.pt\")\n",
    "\n",
    "data_loader = TankBindDataLoader(val_dataset,follow_batch=[\"protein_nodes_xyz\", \"coords\", \"y_pred\", \"y\", \"LAS_distance_constraint_mask\"], batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "affinity_pred_list = []\n",
    "y_pred_list = []\n",
    "for data in tqdm(data_loader):\n",
    "    data = data.to(device)\n",
    "    previous_index_start=0\n",
    "    this_index_start=0\n",
    "    protein_sizes = torch.diff(data[\"protein\"].ptr)\n",
    "    compound_sizes = torch.diff(data[\"compound\"].ptr)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred, affinity_pred = model(data)\n",
    "    affinity_pred_list.append(affinity_pred.detach().cpu())\n",
    "    for i in range(data.batch_n):\n",
    "        this_index_start += protein_sizes[i] * compound_sizes[i]\n",
    "        y_pred_list.append((y_pred[previous_index_start:this_index_start]).detach().cpu())\n",
    "        previous_index_start = this_index_start.clone()\n",
    "affinity_pred_list = torch.cat(affinity_pred_list)\n",
    "output_info_chosen = val_dataset.data\n",
    "output_info_chosen[\"affinity\"] = affinity_pred_list\n",
    "output_info_chosen['dataset_index'] = range(len(output_info_chosen))\n",
    "\n",
    "chosen = output_info_chosen.loc[output_info_chosen.groupby(['protein_name'], sort=False)['affinity'].agg('idxmax')].reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "compound_coordinates_dict = {}\n",
    "protein_coordinates_dict = {}\n",
    "for name in val:\n",
    "    compound_coordinates_dict[name] = compound_dict[name][\"tankbind_ligand_atom_coordinates\"]\n",
    "    \n",
    "    protein_coordinates_dict[name] = val_dataset.protein_dict[name][0]\n",
    "max_compound_nodes = 0\n",
    "max_protein_nodes = 0\n",
    "list_mols = []\n",
    "list_complexes = []\n",
    "for idx, line in tqdm(chosen.iterrows(), total=chosen.shape[0]):\n",
    "    protein_name = line['protein_name']\n",
    "    dataset_index = line['dataset_index']\n",
    "\n",
    "    coords = val_dataset[dataset_index].coords\n",
    "    protein_node_coordinates = val_dataset[dataset_index].node_xyz\n",
    "    # if denormalize:\n",
    "    #    protein_node_coordinates = denormalize_feature(protein_node_coordinates, \"protein_node_coordinates\")\n",
    "    n_compound = coords.shape[0]\n",
    "    n_protein = protein_node_coordinates.shape[0]\n",
    "    y_pred = y_pred_list[dataset_index]\n",
    "    y = val_dataset[dataset_index].dis_map\n",
    "    rdkit_mol_path = f\"{rdkit_folder}/{protein_name}.sdf\"\n",
    "    mol, _ = read_molecule(rdkit_mol_path, None)\n",
    "    LAS_distance_constraint_mask = get_LAS_distance_constraint_mask(mol).bool().flatten()\n",
    "    max_compound_nodes = max(max_compound_nodes, n_compound)\n",
    "    max_protein_nodes = max(max_protein_nodes, n_protein)\n",
    "    cplx = HeteroData()\n",
    "    cplx.protein_name = protein_name\n",
    "    cplx.protein_nodes_xyz = protein_node_coordinates\n",
    "    cplx.coords = coords\n",
    "    cplx.y_pred = y_pred\n",
    "    cplx.y = y\n",
    "    cplx.LAS_distance_constraint_mask = LAS_distance_constraint_mask\n",
    "    list_complexes.append(cplx)\n",
    "    list_mols.append(mol)\n",
    "\n",
    "dataloader = DataLoader(list_complexes, batch_size=chosen.shape[0], shuffle=False,\n",
    "                        follow_batch=[\"protein_nodes_xyz\", \"coords\", \"y_pred\", \"y\", \"LAS_distance_constraint_mask\"])\n",
    "\n",
    "batch = next(iter(dataloader))\n",
    "coords_batched, coords_mask = torch_geometric.utils.to_dense_batch(batch.coords, batch.coords_batch)\n",
    "coords_pair_mask = torch.einsum(\"ij,ik->ijk\", coords_mask, coords_mask)\n",
    "compound_pair_dis_constraint = torch.cdist(coords_batched, coords_batched)[coords_pair_mask]\n",
    "batch.compound_pair_dis_constraint = compound_pair_dis_constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "def distance_loss_function(epoch, x, batch):\n",
    "    protein_nodes_xyz_batched, protein_nodes_xyz_mask = torch_geometric.utils.to_dense_batch(batch.protein_nodes_xyz, batch.protein_nodes_xyz_batch)\n",
    "    x_batched, x_mask = torch_geometric.utils.to_dense_batch(x, batch.coords_batch)\n",
    "    protein_compound_mask = torch.einsum(\"ij,ik->ijk\", protein_nodes_xyz_mask, x_mask)\n",
    "    dis = torch.cdist(protein_nodes_xyz_batched, x_batched)\n",
    "    dis_clamp = torch.clamp(dis, max=10)\n",
    "    dis_flat = dis_clamp[protein_compound_mask]\n",
    "    interaction_loss = torch_geometric.utils.segment((dis_flat - batch.y_pred).abs(), batch.y_pred_ptr, reduce=\"mean\")\n",
    "    xx_mask = torch.einsum(\"ij,ik->ijk\", x_mask, x_mask)\n",
    "    config_dis = torch.cdist(x_batched, x_batched)[xx_mask]\n",
    "\n",
    "    configuration_loss = 1 * (((config_dis-batch.compound_pair_dis_constraint).abs()))\n",
    "    configuration_loss += 2 * ((1.22 - config_dis).relu())\n",
    "    configuration_loss = torch_geometric.utils.segment(configuration_loss, batch.LAS_distance_constraint_mask_ptr, reduce=\"mean\")\n",
    "    # if epoch < 500:\n",
    "    #     loss = interaction_loss.sum()\n",
    "    # else:\n",
    "    #     loss = 1 * (interaction_loss.sum() + 5e-3 * (epoch - 500) * configuration_loss.sum())\n",
    "    # added by Enzo\n",
    "    interaction_loss_sum = interaction_loss.sum()\n",
    "    configuration_loss_sum = configuration_loss.sum() \n",
    "    # modification by Enzo: achieves 20 percent\n",
    "    loss = 1 * (interaction_loss_sum + 5e-2 * configuration_loss_sum)\n",
    "    return loss, (interaction_loss_sum.detach(), configuration_loss_sum.detach())\n",
    "\n",
    "\n",
    "\n",
    "def distance_optimize_compound_coords(batch, total_epoch=5000, loss_function=distance_loss_function, LAS_distance_constraint_mask=None, mode=0, show_progress=False):\n",
    "    # random initialization. center at the protein center.\n",
    "    # coords: shape n_compound_nodes, 3\n",
    "    # y_pred: shape n_protein_nodes, n_compound_nodes\n",
    "    # protein_nodes_xyz: shape n_protein_nodes, 3\n",
    "    # compound_pair_dis_constraint: shape n_compound_nodes, n_compound_nodes\n",
    "    # LAS_distance_constraint_mask: boolean tensor shape n_compound_nodes, n_compound_nodes\n",
    "    batch = batch.to(\"cuda:0\")\n",
    "\n",
    "    # TODO: c_pred est le centre de la protéine. On obtient la valeur avec torch_scatter\n",
    "    c_pred = torch_geometric.utils.segment(batch.protein_nodes_xyz, batch.protein_nodes_xyz_ptr, reduce=\"mean\")\n",
    "    c_pred = c_pred[batch.coords_batch]\n",
    "    x = (5 * (2 * torch.randn_like(batch.coords) - 1) + c_pred.reshape(-1, 3)).detach().clone().requires_grad_(True)\n",
    "    # modification by Enzo: achieves 20 percent\n",
    "    optimizer = torch.optim.Adam([x], lr=1)\n",
    "    # optimizer = torch.optim.Adam([x], lr=0.01)\n",
    "    # optimizer = torch.optim.SGD([x], lr=1, momentum=0.9)\n",
    "    loss_list = []\n",
    "    rmsd_list = []\n",
    "    progress_bar = tqdm(range(total_epoch))\n",
    "    for epoch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        loss, (interaction_loss, configuration_loss) = loss_function(epoch, x, batch)\n",
    "        print(f\"loss: {loss.item()} interaction loss: {interaction_loss.item()} configuration loss: {configuration_loss.item()}\")\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Append the loss to the list\n",
    "        loss_list.append(loss.item())\n",
    "        \n",
    "        # Compute RMSD\n",
    "        rmsd = compute_RMSD_batch(batch.coords, x.detach(), batch.coords_ptr)\n",
    "        rmsd_list += rmsd.detach().cpu().tolist()\n",
    "        \n",
    "        # Update the progress bar with the loss\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "        \n",
    "    return x, loss_list, rmsd_list\n",
    "\n",
    "def get_info_pred_distance(batch, n_repeat=1, total_epoch=5000, mode=0, show_progress=False):\n",
    "    info = []\n",
    "    if show_progress:\n",
    "        it = tqdm(range(n_repeat))\n",
    "    else:\n",
    "        it = range(n_repeat)\n",
    "    for repeat in it:\n",
    "        # random initialization.\n",
    "        # x = torch.rand(coords.shape, requires_grad=True)\n",
    "        x, loss_list, rmsd_list = distance_optimize_compound_coords(batch, mode=mode, total_epoch=total_epoch, show_progress=False)\n",
    "        rmsd = rmsd_list[-1]\n",
    "        for i in range(len(batch.coords_ptr)-1):\n",
    "            try:\n",
    "                info.append([repeat, rmsd_list[batch.coords_ptr[i]:batch.coords_ptr[i+1]], float(loss_list[-1]), x[batch.coords_ptr[i]:batch.coords_ptr[i+1]].detach().cpu().numpy()])\n",
    "            except:\n",
    "                info.append([repeat, rmsd_list[batch.coords_ptr[i]:batch.coords_ptr[i+1]], 0, x[batch.coords_ptr[i]:batch.coords_ptr[i+1]].detach().cpu().numpy()])\n",
    "    info = pd.DataFrame(info, columns=['repeat', 'rmsd', 'loss', 'coords'])\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_RMSD_batch(a, b, ptr):\n",
    "    # correct rmsd calculation.\n",
    "\n",
    "    distances=((a-b)**2).sum(dim=-1) # (compound_nodes_batch, 3) -> (compound_nodes_batch)\n",
    "    sum_distances = torch_geometric.utils.segment(distances, ptr, reduce=\"mean\")\n",
    "    return torch.sqrt(sum_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_dist_info = get_info_pred_distance(batch,\n",
    "                            n_repeat=1, show_progress=False)\n",
    "\n",
    "for idx, line in tqdm(chosen.iterrows(), total=chosen.shape[0]):\n",
    "    protein_name = line['protein_name']\n",
    "    toFile = f'{rdkit_folder}/{protein_name}_tankbind_chosen.sdf'\n",
    "    new_coords = pred_dist_info['coords'].iloc[idx].astype(np.double)\n",
    "    write_with_new_coords(list_mols[idx], new_coords, toFile)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(chosen[\"num_contact\"]==chosen[\"native_num_contact\"]).values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epoch = 5000\n",
    "loss_function=distance_loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ligand_metrics = []\n",
    "for idx, line in tqdm(chosen.iterrows(), total=chosen.shape[0]):\n",
    "    protein_name = line['protein_name']\n",
    "    mol, _ = read_molecule(f\"{renumbered_ligands_folder}/{protein_name}.sdf\", None)\n",
    "    mol_pred, _ = read_molecule(f\"{rdkit_folder}/{protein_name}_tankbind_chosen.sdf\", None) # tankbind_chosen is the compound with predicted coordinates assigned by write_with_new_coords\n",
    "\n",
    "    sm = Chem.MolToSmiles(mol)\n",
    "    mol_order = list(mol.GetPropsAsDict(includePrivate=True, includeComputed=True)['_smilesAtomOutputOrder'])\n",
    "    mol = Chem.RenumberAtoms(mol, mol_order)\n",
    "    mol = Chem.RemoveHs(mol)\n",
    "    true_ligand_pos = np.array(mol.GetConformer().GetPositions())\n",
    "\n",
    "    sm = Chem.MolToSmiles(mol_pred)\n",
    "    mol_order = list(mol_pred.GetPropsAsDict(includePrivate=True, includeComputed=True)['_smilesAtomOutputOrder'])\n",
    "    mol_pred = Chem.RenumberAtoms(mol_pred, mol_order)\n",
    "    mol_pred = Chem.RemoveHs(mol_pred)\n",
    "    mol_pred_pos = np.array(mol_pred.GetConformer().GetPositions())\n",
    "\n",
    "    rmsd = np.sqrt(((true_ligand_pos - mol_pred_pos) ** 2).sum(axis=1).mean(axis=0))\n",
    "    com_dist = compute_RMSD(mol_pred_pos.mean(axis=0), true_ligand_pos.mean(axis=0))\n",
    "    ligand_metrics.append([protein_name, rmsd, com_dist,])\n",
    "\n",
    "d = pd.DataFrame(ligand_metrics, columns=['pdb', 'TankBind_RMSD', 'TankBind_COM_DIST',])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_custom_description(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset.protein_dict['1zsb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset.compound_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(compound_dict, f\"{rdkit_folder}/compound_dict_based_on_rdkit.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset_path=\"/fs/pool/pool-marsot/tankbind_philip/TankBind/dataset/dataset\"\n",
    "dataset = TankBindDataSet(full_dataset_path)\n",
    "val_data = dataset.data[(dataset.data[\"group\"]==\"valid\") & (~(dataset.data[\"use_compound_com\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_val(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.protein_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.protein_dict['3zzf'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in dataset.protein_dict['3zzf']:\n",
    "    print(item.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open(\"/fs/pool/pool-marsot/bindbind/datasets/data/equibind_dataset/compound_coordinates_dict.pkl\", \"rb\") as f:\n",
    "    compound_coordinates_dict = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.compound_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_names = dataset.data[(dataset.data[\"group\"]==\"valid\")][\"protein_name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_coordinates_dict = {}\n",
    "for name in val_names:\n",
    "    compound_coordinates_dict[name] = dataset.compound_dict[name][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_coordinates_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bindbind.experiments.ablations.regular.metrics.metrics_fast import evaluate_model_val    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/fs/gpfs41/lv11/fileset01/pool/pool-marsot/tankbind_philip_base/tankbind_philip/tankbind\n"
     ]
    }
   ],
   "source": [
    "%cd /fs/pool/pool-marsot/tankbind_philip_base/tankbind_philip/tankbind/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"/fs/pool/pool-marsot/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tankbind_philip_base.tankbind_philip.tankbind.model import IaBNet_with_affinity as old_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model = old_model().to(\"cuda:0\")\n",
    "model.load_state_dict(torch.load(f\"/fs/pool/pool-marsot/tankbind_philip/TankBind/tankbind/result/2024_07_03_21_19/model_119.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IaBNet_with_affinity(\n",
       "  (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (conv_protein): GVP_embedding(\n",
       "    (W_s): Embedding(20, 20)\n",
       "    (W_v): Sequential(\n",
       "      (0): LayerNorm(\n",
       "        (scalar_norm): LayerNorm((26,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): GVP(\n",
       "        (wh): Linear(in_features=3, out_features=16, bias=False)\n",
       "        (ws): Linear(in_features=42, out_features=128, bias=True)\n",
       "        (wv): Linear(in_features=16, out_features=16, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (W_e): Sequential(\n",
       "      (0): LayerNorm(\n",
       "        (scalar_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): GVP(\n",
       "        (wh): Linear(in_features=1, out_features=1, bias=False)\n",
       "        (ws): Linear(in_features=33, out_features=32, bias=True)\n",
       "        (wv): Linear(in_features=1, out_features=1, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x GVPConvLayer(\n",
       "        (conv): GVPConv()\n",
       "        (norm): ModuleList(\n",
       "          (0-1): 2 x LayerNorm(\n",
       "            (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (dropout): ModuleList(\n",
       "          (0-1): 2 x Dropout(\n",
       "            (sdropout): Dropout(p=0.1, inplace=False)\n",
       "            (vdropout): _VDropout()\n",
       "          )\n",
       "        )\n",
       "        (ff_func): Sequential(\n",
       "          (0): GVP(\n",
       "            (wh): Linear(in_features=16, out_features=32, bias=False)\n",
       "            (ws): Linear(in_features=160, out_features=512, bias=True)\n",
       "            (wv): Linear(in_features=32, out_features=32, bias=False)\n",
       "          )\n",
       "          (1): GVP(\n",
       "            (wh): Linear(in_features=32, out_features=32, bias=False)\n",
       "            (ws): Linear(in_features=544, out_features=128, bias=True)\n",
       "            (wv): Linear(in_features=32, out_features=16, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (W_out): Sequential(\n",
       "      (0): LayerNorm(\n",
       "        (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): GVP(\n",
       "        (wh): Linear(in_features=16, out_features=16, bias=False)\n",
       "        (ws): Linear(in_features=144, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_compound): GIN(\n",
       "    (layers): ModuleList(\n",
       "      (0): GraphIsomorphismConv(\n",
       "        (mlp): MultiLayerPerceptron(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=56, out_features=128, bias=True)\n",
       "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (edge_linear): Linear(in_features=19, out_features=56, bias=True)\n",
       "      )\n",
       "      (1): GraphIsomorphismConv(\n",
       "        (mlp): MultiLayerPerceptron(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=128, out_features=56, bias=True)\n",
       "            (1): Linear(in_features=56, out_features=56, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (edge_linear): Linear(in_features=19, out_features=128, bias=True)\n",
       "      )\n",
       "      (2): GraphIsomorphismConv(\n",
       "        (mlp): MultiLayerPerceptron(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=56, out_features=128, bias=True)\n",
       "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (edge_linear): Linear(in_features=19, out_features=56, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (readout): SumReadout()\n",
       "  )\n",
       "  (protein_pair_embedding): Linear(in_features=16, out_features=128, bias=True)\n",
       "  (compound_pair_embedding): Linear(in_features=16, out_features=128, bias=True)\n",
       "  (protein_to_compound_list): ModuleList(\n",
       "    (0-4): 5 x TriangleProteinToCompound_v2(\n",
       "      (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (layernorm_c): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (gate_linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (gate_linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (ending_gate_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (linear_after_sum): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (triangle_self_attention_list): ModuleList(\n",
       "    (0-4): 5 x TriangleSelfAttentionRowWise(\n",
       "      (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "      (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "      (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "      (g): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (final_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (tranistion): Transition(\n",
       "    (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "  )\n",
       "  (linear): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (linear_energy): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (gate_linear): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (leaky): LeakyReLU(negative_slope=0.01)\n",
       "  (dropout): Dropout2d(p=0.25, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/fs/pool/pool-marsot/tankbind_philip/TankBind/dataset/val_dataset/processed/data.pt', '/fs/pool/pool-marsot/tankbind_philip/TankBind/dataset/val_dataset/processed/protein.pt', '/fs/pool/pool-marsot/tankbind_philip/TankBind/dataset/val_dataset/processed/compound.pt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 790/790 [01:42<00:00,  7.69it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 959/959 [00:16<00:00, 58.70it/s]\n",
      "144.33 466.43 167.65 16.88: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [03:13<00:00, 25.81it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 959/959 [00:01<00:00, 811.83it/s]\n",
      " 22%|███████████████████████████████████▏                                                                                                                           | 212/959 [00:00<00:02, 288.13it/s][14:58:08] Unusual charge on atom 1 number of radical electrons set to zero\n",
      "[14:58:08] Unusual charge on atom 1 number of radical electrons set to zero\n",
      " 71%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                              | 677/959 [00:02<00:01, 267.52it/s][14:58:09] Unusual charge on atom 17 number of radical electrons set to zero\n",
      "[14:58:09] Unusual charge on atom 17 number of radical electrons set to zero\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 959/959 [00:03<00:00, 279.27it/s]\n"
     ]
    }
   ],
   "source": [
    "from tankbind_philip.TankBind.tankbind.evaluation_fast import evaluate_model_val\n",
    "df=evaluate_model_val(model, batch_size=4,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tankbind_philip.TankBind.tankbind.helper import simple_custom_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>mean</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>5A</th>\n",
       "      <th>2A</th>\n",
       "      <th>median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TankBind_RMSD</td>\n",
       "      <td>16.889440</td>\n",
       "      <td>2.402756</td>\n",
       "      <td>13.864611</td>\n",
       "      <td>30.829780</td>\n",
       "      <td>36.704901</td>\n",
       "      <td>20.333681</td>\n",
       "      <td>13.864611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TankBind_COM_DIST</td>\n",
       "      <td>15.480724</td>\n",
       "      <td>1.190346</td>\n",
       "      <td>11.036407</td>\n",
       "      <td>30.165105</td>\n",
       "      <td>42.857143</td>\n",
       "      <td>34.410845</td>\n",
       "      <td>11.036407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               index       mean       25%        50%        75%         5A  \\\n",
       "0      TankBind_RMSD  16.889440  2.402756  13.864611  30.829780  36.704901   \n",
       "1  TankBind_COM_DIST  15.480724  1.190346  11.036407  30.165105  42.857143   \n",
       "\n",
       "          2A     median  \n",
       "0  20.333681  13.864611  \n",
       "1  34.410845  11.036407  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_custom_description(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tankbind_philip.TankBind.tankbind.model import IaBNet_with_affinity as old_model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = old_model_2().to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "info = torch.load(\"/fs/pool/pool-marsot/tankbind_philip/TankBind/dataset/dataset/processed/data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['3zzf', '3gww', '1w8l', ..., '1avd', '2xui', '2avi'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info[\"protein_name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "proteins = torch.load(\"/fs/pool/pool-marsot/tankbind_philip/TankBind/dataset/dataset/processed/protein.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19420"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(proteins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (tankbind_py312)",
   "language": "python",
   "name": "tankbind_py312"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
